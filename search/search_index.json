{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"gsql2rsql","text":"<p>OpenCypher to Databricks SQL Transpiler</p> <p>Transform graph queries into high-performance SQL for Delta Lake.</p> <p>Project Status: This is a hobby/research project being developed towards production quality. While it handles complex queries and includes comprehensive tests (682+), it's not yet battle-tested at enterprise scale. Contributions welcome!</p>"},{"location":"#what-is-gsql2rsql","title":"What is gsql2rsql?","text":"<p>gsql2rsql transpiles OpenCypher graph queries to Databricks SQL, enabling graph analytics on Delta Lake without a dedicated graph database.</p>"},{"location":"#inspiration-architecture","title":"Inspiration &amp; Architecture","text":"<p>This project was inspired by Microsoft's openCypherTranspiler (now unmaintained) which transpiled OpenCypher to T-SQL (SQL Server).</p> <p>Why a new transpiler? Two reasons:</p> <ol> <li>Databricks SQL is fundamentally different from T-SQL \u2014 WITH RECURSIVE, HOFs, and Delta Lake optimizations require different strategies</li> <li>Security-first architecture \u2014 gsql2rsql uses strict 4-phase separation of concerns for correctness:</li> <li>Parser: Syntax only (no schema access)</li> <li>Planner: Semantics only (builds logical operators)</li> <li>Resolver: Validation only (schema checking, column resolution)</li> <li>Renderer: Code generation only (intentionally \"dumb\" \u2014 no semantic decisions, just SQL generation)</li> </ol> <p>This separation makes the transpiler easier to audit, test, and trust</p> <p>The game-changer: Databricks recently added WITH RECURSIVE support, unlocking variable-length path traversal.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u2705 Variable-length paths via <code>WITH RECURSIVE</code></li> <li>\u2705 Comprehensive tests: 682+ test cases with PySpark validation</li> <li>\u2705 Optimized SQL: Filter pushdown, column pruning</li> <li>\u2705 Type-safe: Schema validation and column resolution</li> <li>\u2705 Developer friendly: Clear errors, CLI, TUI</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Transform this OpenCypher query:</p> Cypher<pre><code>MATCH (p:Person)-[:KNOWS*1..3]-(friend:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, COUNT(DISTINCT friend) AS tech_connections\nORDER BY tech_connections DESC\nLIMIT 10\n</code></pre> <p>Into optimized Databricks SQL with <code>WITH RECURSIVE</code> for path traversal!</p> <p>See the generated SQL \u2192</p>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>\ud83d\udd0d Fraud Detection - Detect fraud rings and anomalies using graph pattern matching</li> <li>\ud83d\udcb3 Credit Risk Analysis - Analyze relationship-based credit risk</li> <li>\ud83c\udfaf Feature Engineering - Graph features for ML models</li> </ul> <p>See the Examples Gallery for 69 complete query examples with generated SQL.</p>"},{"location":"#why-graph-queries-on-delta-lake","title":"Why Graph Queries on Delta Lake?","text":"Text Only<pre><code>Delta Lake (Source) \u2192 CDC/ETL \u2192 (Graph) \u2192 Cypher \u2192 Results\n</code></pre> <p>Problems: Data duplication, sync lag, consistency issues, operational overhead, vendor lock-in</p>"},{"location":"#our-approach-query-delta-directly","title":"Our Approach: Query Delta Directly","text":"Text Only<pre><code>Delta Lake (Single Source) \u2192 OpenCypher (gsql2rsql) \u2192 Databricks SQL \u2192 Results\n</code></pre> <p>Advantages: - No duplication: Query source data directly - Real-time: Always fresh data - Cost-effective: No second database - Unified governance: Single data platform</p>"},{"location":"#billion-scale-relationships","title":"Billion-Scale Relationships","text":""},{"location":"#the-problem-with-graph-database-at-scale","title":"The Problem with graph database at Scale","text":"<p>10 billion edges \u00d7 100 bytes/edge = DEATH</p> <p>graph databases Challenges: - Memory limits - Vertical scaling only - Enterprise licenses + large instances = $$$ - Long backup windows</p>"},{"location":"#triple-store-in-delta-lake","title":"Triple Store in Delta Lake","text":"SQL<pre><code>CREATE TABLE relationships (\n  subject_id STRING,\n  predicate STRING,\n  object_id STRING,\n  properties MAP&lt;STRING, STRING&gt;,\n  timestamp TIMESTAMP\n) PARTITIONED BY (DATE(timestamp));\n</code></pre> <p>Advantages: 1. Horizontal scale: Petabytes, billions of rows 2. Cost: S3 storage (\\(0.023/GB) vs RAM (\\)10+/GB) 3. Time travel: Delta Lake versioning = free audit 4. Z-ordering: <code>OPTIMIZE ZORDER BY (subject_id, predicate)</code> 5. Photon engine: Vectorized execution</p>"},{"location":"#llms-transpilers-enterprise-governance","title":"LLMs + Transpilers: Enterprise Governance","text":"<p>The Problem: In enterprise environments, someone must be accountable for queries before execution \u2014 even with LLM text-to-query.</p>"},{"location":"#why-transpilers-matter","title":"Why Transpilers Matter","text":"<p>1. Reviewability: Graph queries are 4-5 lines vs hundreds of SQL lines Cypher<pre><code># 5 lines in Cypher\nMATCH (c:Customer)-[:TRANSACTION*1..3]-&gt;(m:Merchant)\nWHERE m.risk_score &gt; 0.9\nRETURN c.id, COUNT(*) AS risky_tx\nORDER BY risky_tx DESC\nLIMIT 100\n</code></pre> vs 150+ lines of recursive SQL. Easier for humans to review and approve.</p>"},{"location":"#installation","title":"Installation","text":"Bash<pre><code>pip install gsql2rsql\n</code></pre> <p>Or install from source:</p> Bash<pre><code>git clone https://github.com/devmessias/gsql2rsql\ncd gsql2rsql/python\nuv sync\nuv pip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"Python<pre><code>from gsql2rsql.parser.opencypher_parser import OpenCypherParser\nfrom gsql2rsql.planner.logical_plan import LogicalPlan\nfrom gsql2rsql.renderer.sql_renderer import SQLRenderer\nfrom gsql2rsql.planner.schema import DatabricksSchemaProvider, SimpleGraphSchemaProvider\nfrom gsql2rsql.common.schema import NodeSchema, EdgeSchema, EntityProperty\n\n# 1. Define schema (map graph to Delta tables)\nschema = SimpleGraphSchemaProvider()\n\nperson = NodeSchema(\n    name=\"Person\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"age\", data_type=int),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\ncompany = NodeSchema(\n    name=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"industry\", data_type=str),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\nworks_at = EdgeSchema(\n    name=\"WORKS_AT\",\n    source_node_id=\"Person\",\n    sink_node_id=\"Company\",\n    source_id_property=EntityProperty(property_name=\"person_id\", data_type=int),\n    sink_id_property=EntityProperty(property_name=\"company_id\", data_type=int),\n    properties=[EntityProperty(property_name=\"since\", data_type=int)]\n)\n\nschema.add_node(person)\nschema.add_node(company)\nschema.add_edge(works_at)\n\n# 2. Write Cypher query\nquery = \"\"\"\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, p.age, c.name AS company\nORDER BY p.age DESC\nLIMIT 10\n\"\"\"\n\n# 3. Transpile to SQL\nparser = OpenCypherParser()\nschema_provider = DatabricksSchemaProvider(schema)\nrenderer = SQLRenderer(schema_provider)\n\nast = parser.parse(query)\nplan = LogicalPlan.from_ast(ast, schema)\nplan.resolve(query)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n\n# 4. Execute on Databricks\n# spark.sql(sql).show()\n</code></pre> <p>Output: Databricks SQL with JOINs, WHERE filters, ORDER BY, and LIMIT \u2014 ready to execute on Delta Lake.</p> <p>More Examples</p> <p>See Installation and Quick Start for detailed walkthrough, JSON schema format, and CLI usage.</p>"},{"location":"#documentation","title":"Documentation","text":"Section Description Installation and Quick Start Get started with your first query Examples Gallery Real-world fraud, credit, and feature queries API Reference Python API and CLI documentation Architecture How the transpiler works Contributing Development guidelines"},{"location":"#architecture","title":"Architecture","text":"<p>gsql2rsql uses a 4-phase pipeline for correctness and maintainability:</p> <pre><code>graph LR\n    A[OpenCypher Query] --&gt; B[Parser]\n    B --&gt; C[Planner]\n    C --&gt; D[Resolver]\n    D --&gt; E[Renderer]\n    E --&gt; F[Databricks SQL]\n\n    style B fill:#e3f2fd\n    style C fill:#fff3e0\n    style D fill:#f3e5f5\n    style E fill:#e8f5e9</code></pre> <ol> <li>Parser: Cypher \u2192 AST (syntax only)</li> <li>Planner: AST \u2192 Logical operators (semantics)</li> <li>Resolver: Validate columns &amp; types</li> <li>Renderer: Operators \u2192 SQL</li> </ol> <p>This separation ensures each phase has clear responsibilities and can be tested independently.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#supported","title":"\u2705 Supported","text":"<ul> <li>MATCH patterns with nodes and relationships</li> <li>Variable-length paths (<code>-[:REL*1..5]-&gt;</code>) via <code>WITH RECURSIVE</code></li> <li>WHERE clause with complex predicates</li> <li>WITH for query chaining and aggregation</li> <li>RETURN with projections and aliases</li> <li>Aggregations (COUNT, SUM, AVG, COLLECT, etc.)</li> <li>ORDER BY, LIMIT, SKIP</li> <li>DISTINCT results</li> <li>UNION queries</li> <li>CASE expressions</li> <li>Path functions (length, nodes, relationships)</li> <li>Undirected relationships (<code>-[:REL]-</code>)</li> <li>Filter pushdown optimizations</li> </ul>"},{"location":"#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Databricks Runtime 15+ required (for <code>WITH RECURSIVE</code>)</li> <li>Deep paths (&gt;10 hops) may be slow</li> <li>Write operations not supported (<code>CREATE</code>, <code>DELETE</code>, <code>SET</code>)</li> </ul> <p>See Limitations for details.</p>"},{"location":"#testing","title":"Testing","text":"<p>Comprehensive test suite with 682+ tests:</p> Bash<pre><code># Unit tests (no dependencies)\nmake test-no-pyspark\n\n# PySpark validation (dev only - requires PySpark)\nmake test-pyspark\n\n# Specific category\nmake test-pyspark-fraud\n</code></pre> <p>Note: PySpark is only needed for development/testing. Users don't need it to use the transpiler.</p> <p>All example queries are validated end-to-end on PySpark during development.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>This is an open hobby project \u2014 contributions are very welcome!</p> <ul> <li>Bugs: Open an issue</li> <li>Features: Discuss first</li> <li>PRs: Follow conventional commits</li> </ul> <p>See Contributing Guide for:</p> <ul> <li>Development setup</li> <li>Testing requirements</li> <li>Separation of Concerns</li> <li>Code style guidelines</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>Examples Gallery</li> <li>Decision Log</li> </ul> <p>Ready to get started? Check out the Installation and Quick Start Guide!</p>"},{"location":"00-summary/","title":"Project Summary","text":""},{"location":"00-summary/#elevator-pitch","title":"Elevator Pitch","text":"<p>gsql2rsql: A production-quality transpiler converting OpenCypher graph queries to Databricks Spark SQL with recursive CTE support for variable-length path traversal.</p>"},{"location":"00-summary/#quick-facts","title":"Quick Facts","text":"<ul> <li>Language: Python 3.12+ (fully typed with strict mypy)</li> <li>Runtime Target: Databricks Runtime 17+ (Spark SQL with <code>WITH RECURSIVE</code>)</li> <li>License: MIT (\u00a9 Microsoft Corporation)</li> <li>Status: Alpha (v0.1.0)</li> <li>Main Entrypoint: <code>gsql2rsql</code> CLI command (also importable as Python library)</li> </ul>"},{"location":"00-summary/#purpose","title":"Purpose","text":"<p>OpenCypher is a declarative graph query language inspired by Neo4j's Cypher, optimized for pattern matching across nodes and edges. Databricks Spark SQL is a powerful relational SQL dialect designed for big data analytics, now with recursive CTE support (runtime 17+). This transpiler bridges the two worlds, enabling:</p> <ul> <li>Graph analytics on tabular data: Query your Databricks tables as if they were a graph database</li> <li>BFS/DFS traversal: Use variable-length paths (<code>-[:KNOWS*1..5]-&gt;</code>) without custom Spark code</li> <li>Pattern matching: Leverage Cypher's expressive syntax for relationship queries</li> <li>Correctness: Strict 4-phase architecture (parse \u2192 plan \u2192 resolve \u2192 render) ensures semantically correct SQL generation</li> <li>Production-ready: Comprehensive test suite (61 test files), PySpark validation, rich error messages with suggestions</li> </ul> <p>The transpiler handles complex Cypher features including multi-hop traversals, aggregations, subqueries (<code>WITH</code> clauses), <code>OPTIONAL MATCH</code>, <code>UNWIND</code>, set operations (<code>UNION</code>/<code>INTERSECT</code>/<code>EXCEPT</code>), list comprehensions, and 40+ built-in functions. Generated SQL uses <code>WITH RECURSIVE</code> CTEs for efficient graph traversal and includes optimizations like predicate pushdown and conservative subquery flattening.</p>"},{"location":"00-summary/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>01-quickstart.md \u2014 Run your first query in 2 minutes</li> <li>02-architecture.md \u2014 Understand the 4-phase transpilation pipeline</li> <li>CONTRIBUTING.md \u2014 Separation of concerns and architectural boundaries</li> </ul>"},{"location":"01-quickstart/","title":"Quick Start Guide","text":""},{"location":"01-quickstart/#installation","title":"Installation","text":"Bash<pre><code># Clone repository (INFERRED - replace with actual repo URL)\ncd /path/to/cyper2dsql/python\n\n# Create virtual environment and install dependencies\nuv venv\nuv sync --extra dev\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"01-quickstart/#environment-requirements","title":"Environment Requirements","text":"<ul> <li>Python: 3.12 or 3.13</li> <li>Target Runtime: Databricks Runtime 17+ (requires <code>WITH RECURSIVE</code> CTE support)</li> <li>Spark Version: 3.5+ (for PySpark validation tests)</li> <li>Key Dependencies:</li> <li><code>antlr4-python3-runtime&gt;=4.13.0</code> (parser)</li> <li><code>click&gt;=8.1.0</code> (CLI)</li> <li><code>textual&gt;=0.47.0</code> (interactive TUI)</li> <li><code>pyspark&gt;=3.5.0</code> (dev only, for validation)</li> </ul>"},{"location":"01-quickstart/#minimal-example-simple-query","title":"Minimal Example: Simple Query","text":""},{"location":"01-quickstart/#1-create-a-schema-file","title":"1. Create a Schema File","text":"<p>OpenCypher queries need a graph schema defining nodes and edges. Create <code>my_schema.json</code>:</p> JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"graph.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"KNOWS\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Person\",\n      \"tableName\": \"graph.Knows\",\n      \"sourceIdProperty\": {\"name\": \"source_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"target_id\", \"type\": \"int\"},\n      \"properties\": []\n    }\n  ]\n}\n</code></pre>"},{"location":"01-quickstart/#2-transpile-a-query","title":"2. Transpile a Query","text":"<p>Cypher input: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person)\nWHERE p.age &gt; 30\nRETURN p.name, f.name, f.age\n</code></pre></p> <p>Command: Bash<pre><code>echo \"MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name, f.name, f.age\" | \\\n  uv run gsql2rsql transpile --schema my_schema.json\n</code></pre></p> <p>Generated SQL: SQL<pre><code>SELECT\n  _gsql2rsql_p_name AS name,\n  _gsql2rsql_f_name AS name,\n  _gsql2rsql_f_age AS age\nFROM (\n  SELECT\n    sink.id AS _gsql2rsql_f_id,\n    sink.name AS _gsql2rsql_f_name,\n    sink.age AS _gsql2rsql_f_age,\n    source.id AS _gsql2rsql_p_id,\n    source.name AS _gsql2rsql_p_name,\n    source.age AS _gsql2rsql_p_age\n  FROM\n    graph.Knows AS edge\n  INNER JOIN graph.Person AS source ON edge.source_id = source.id\n  INNER JOIN graph.Person AS sink ON edge.target_id = sink.id\n  WHERE source.age &gt; 30\n) AS _proj\n</code></pre></p>"},{"location":"01-quickstart/#3-variable-length-path-example","title":"3. Variable-Length Path Example","text":"<p>Cypher input (BFS traversal): Cypher<pre><code>MATCH (root:Person)-[:KNOWS*1..5]-&gt;(neighbor:Person)\nWHERE root.id = 1\nRETURN DISTINCT neighbor.id, neighbor.name\n</code></pre></p> <p>Command: Bash<pre><code>echo \"MATCH (root:Person)-[:KNOWS*1..5]-&gt;(neighbor:Person) WHERE root.id = 1 RETURN DISTINCT neighbor.id, neighbor.name\" | \\\n  uv run gsql2rsql transpile --schema my_schema.json\n</code></pre></p> <p>Generated SQL (uses WITH RECURSIVE): SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_id AS start_node,\n      e.target_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_id, e.target_id) AS path,\n      ARRAY(e.source_id) AS visited\n    FROM graph.Knows e\n    UNION ALL\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.source_id)) AS visited\n    FROM paths_1 p\n    JOIN graph.Knows e ON p.end_node = e.source_id\n    WHERE p.depth &lt; 5\n      AND NOT array_contains(p.visited, e.target_id)\n  )\nSELECT DISTINCT\n  _gsql2rsql_neighbor_id AS id,\n  _gsql2rsql_neighbor_name AS name\nFROM (\n  SELECT\n    sink.id AS _gsql2rsql_neighbor_id,\n    sink.name AS _gsql2rsql_neighbor_name,\n    source.id AS _gsql2rsql_root_id\n  FROM paths_1 p\n  JOIN graph.Person sink ON sink.id = p.end_node\n  JOIN graph.Person source ON source.id = p.start_node\n  WHERE p.depth &gt;= 1 AND p.depth &lt;= 5\n    AND source.id = 1\n) AS _proj\n</code></pre></p>"},{"location":"01-quickstart/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests (fast, excludes PySpark)\nmake test-no-pyspark\n\n# Run all tests including PySpark validation (slower)\nmake test\n\n# Run with coverage report\nmake test-cov\n\n# Run specific feature test\nuv run pytest tests/transpile_tests/test_01_simple_node_lookup.py -v\n</code></pre>"},{"location":"01-quickstart/#running-examples-from-yaml","title":"Running Examples from YAML","text":"<p>The transpiler includes curated examples in <code>examples/*.yaml</code>:</p> Bash<pre><code># Interactive TUI mode (browse examples, live transpilation)\nuv run gsql2rsql tui --examples examples/credit_queries.yaml\n\n# Run PySpark validation on all examples\nmake test-pyspark-examples\n\n# Run quick subset (first 5 credit queries)\nmake test-pyspark-quick\n</code></pre>"},{"location":"01-quickstart/#cli-commands-reference","title":"CLI Commands Reference","text":"Bash<pre><code># Show help\nuv run gsql2rsql --help\n\n# Transpile from stdin\necho \"MATCH (n:Person) RETURN n\" | uv run gsql2rsql transpile -s schema.json\n\n# Transpile from file\nuv run gsql2rsql transpile -s schema.json -i query.cypher\n\n# Enable optimization (conservative subquery flattening)\nuv run gsql2rsql transpile -s schema.json --optimize\n\n# Show scope debugging information\nuv run gsql2rsql transpile -s schema.json --explain-scopes\n\n# Parse only (show AST without transpilation)\nuv run gsql2rsql parse -i query.cypher\n\n# Generate schema template\nuv run gsql2rsql init-schema &gt; my_schema.json\n</code></pre>"},{"location":"01-quickstart/#databricks-runtime-constraints","title":"Databricks Runtime Constraints","text":"<ul> <li>Minimum Runtime: 17+ (requires <code>WITH RECURSIVE</code> support)</li> <li>SQL Features Used:</li> <li><code>WITH RECURSIVE</code> for variable-length paths</li> <li><code>ARRAY()</code> and <code>CONCAT()</code> for path tracking</li> <li><code>array_contains()</code> for cycle detection</li> <li><code>STRUCT()</code> for edge property collections</li> <li><code>COALESCE()</code> for <code>OPTIONAL MATCH</code> null handling</li> <li>Known Issues: Certain aggregation patterns may require runtime 18+ (INFERRED from test history)</li> </ul>"},{"location":"01-quickstart/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>examples/credit_queries.yaml \u2014 Credit analysis domain examples</li> <li>examples/fraud_queries.yaml \u2014 Fraud detection patterns</li> <li>examples/features_queries.yaml \u2014 Feature showcase</li> <li>02-architecture.md \u2014 Understand the transpilation pipeline</li> <li>05-testing-and-examples.md \u2014 Test structure and golden files</li> </ul>"},{"location":"02-architecture/","title":"Architecture Overview","text":""},{"location":"02-architecture/#high-level-dataflow","title":"High-Level Dataflow","text":"<pre><code>graph TD\n    A[OpenCypher Query String] --&gt; B[Phase 1: Parser]\n    B --&gt; C[Abstract Syntax Tree]\n    C --&gt; D[Phase 2: Planner]\n    D --&gt; E[Logical Operator Tree + SymbolTable]\n    E --&gt; F[Phase 3: Optimizer]\n    F --&gt; G[Optimized Logical Plan]\n    G --&gt; H[Phase 4: Resolver]\n    H --&gt; I[ResolutionResult]\n    I --&gt; J[Phase 5: Renderer]\n    J --&gt; K[Databricks Spark SQL]\n\n    style B fill:#e1f5ff\n    style D fill:#fff4e1\n    style F fill:#ffe1f5\n    style H fill:#e1ffe1\n    style J fill:#ffe1e1</code></pre>"},{"location":"02-architecture/#separation-of-concerns-4-phase-design","title":"Separation of Concerns (4-Phase Design)","text":"<p>The transpiler strictly separates concerns across phases to ensure correctness and maintainability. This architecture is documented in CONTRIBUTING.md.</p>"},{"location":"02-architecture/#design-principle","title":"Design Principle","text":"<p>Each phase has a single responsibility and does not perform the responsibilities of other phases:</p> <ol> <li>Parser: Syntax only (what is valid Cypher?)</li> <li>Planner: Semantics only (what does it mean logically?)</li> <li>Resolver: Validation only (do all references exist?)</li> <li>Renderer: Implementation only (how to generate SQL?)</li> </ol> <p>This prevents: - Parser from needing schema knowledge - Planner from validating column references - Renderer from making semantic decisions</p>"},{"location":"02-architecture/#phase-1-parser-lexicalsyntactic-analysis","title":"Phase 1: Parser (Lexical/Syntactic Analysis)","text":"<p>Location: src/gsql2rsql/parser/</p>"},{"location":"02-architecture/#responsibility","title":"Responsibility","text":"<p>Convert OpenCypher query string to Abstract Syntax Tree (AST). Validates syntax only \u2014 does NOT validate semantics, resolve references, or access schema.</p>"},{"location":"02-architecture/#key-components","title":"Key Components","text":"File Purpose Lines <code>opencypher_parser.py</code> Main entry point, ANTLR runtime invocation ~60 <code>ast.py</code> AST node definitions (50+ node types) ~1500 <code>visitor.py</code> ANTLR visitor pattern implementation ~800 <code>operators.py</code> Operator enums (binary, aggregation, functions) ~300 <code>grammar/</code> ANTLR-generated parser/lexer ~8000"},{"location":"02-architecture/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: OpenCypher query string</li> <li>Output: <code>QueryNode</code> (root AST node)</li> </ul>"},{"location":"02-architecture/#key-classes","title":"Key Classes","text":"Python<pre><code># Base AST node\nclass TreeNode:\n    def dump_tree(self, indent: int = 0) -&gt; str\n    def evaluate_type(self, symbol_table: SymbolTable) -&gt; DataType\n\n# Expression nodes\nclass QueryExpression(TreeNode):\n    # Base for all expressions (binary, function calls, literals, etc.)\n\nclass QueryExpressionBinary(QueryExpression):\n    operator: BinaryOperator\n    left: QueryExpression\n    right: QueryExpression\n\nclass QueryExpressionFunction(QueryExpression):\n    function: Function\n    arguments: list[QueryExpression]\n\n# Entity nodes\nclass NodeEntity(TreeNode):\n    variable: str | None\n    labels: list[str]\n\nclass RelationshipEntity(TreeNode):\n    variable: str | None\n    types: list[str]\n    direction: Direction  # LEFT, RIGHT, BOTH\n\n# Query structure\nclass MatchClause(TreeNode):\n    pattern: QueryPattern\n    optional: bool\n\nclass WithClause(TreeNode):\n    projections: list[ProjectionItem]\n    aggregation: bool\n\nclass ReturnClause(TreeNode):\n    projections: list[ProjectionItem]\n    distinct: bool\n    order_by: list[OrderByItem]\n    limit: int | None\n</code></pre>"},{"location":"02-architecture/#example-parsing","title":"Example Parsing","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name\n</code></pre></p> <p>AST Structure (simplified): Text Only<pre><code>QueryNode\n\u2514\u2500\u2500 SingleQueryNode\n    \u251c\u2500\u2500 MatchClause\n    \u2502   \u2514\u2500\u2500 QueryPattern\n    \u2502       \u251c\u2500\u2500 NodeEntity(variable=\"p\", labels=[\"Person\"])\n    \u2502       \u251c\u2500\u2500 RelationshipEntity(types=[\"KNOWS\"], direction=RIGHT)\n    \u2502       \u2514\u2500\u2500 NodeEntity(variable=\"f\", labels=[\"Person\"])\n    \u251c\u2500\u2500 WhereClause\n    \u2502   \u2514\u2500\u2500 QueryExpressionBinary(\n    \u2502         operator=GREATER_THAN,\n    \u2502         left=QueryExpressionProperty(entity=\"p\", property=\"age\"),\n    \u2502         right=QueryExpressionValue(value=30)\n    \u2502       )\n    \u2514\u2500\u2500 ReturnClause\n        \u2514\u2500\u2500 ProjectionItem(\n              expression=QueryExpressionProperty(entity=\"p\", property=\"name\"),\n              alias=\"name\"\n            )\n</code></pre></p>"},{"location":"02-architecture/#phase-2-planner-logical-operator-construction","title":"Phase 2: Planner (Logical Operator Construction)","text":"<p>Location: src/gsql2rsql/planner/</p>"},{"location":"02-architecture/#responsibility_1","title":"Responsibility","text":"<p>Convert AST to logical relational algebra. Builds symbol table tracking variable definitions and scopes. Does NOT resolve column references or validate property access.</p>"},{"location":"02-architecture/#key-components_1","title":"Key Components","text":"File Purpose Lines <code>logical_plan.py</code> Main orchestrator, AST \u2192 operator conversion ~500 <code>operators.py</code> Logical operator definitions (11 operator types) ~1200 <code>symbol_table.py</code> Variable tracking with nested scopes ~400 <code>path_analyzer.py</code> Variable-length path optimization ~300 <code>schema.py</code> Internal schema representation ~200 <code>subquery_optimizer.py</code> Conservative subquery flattening ~400"},{"location":"02-architecture/#inputoutput_1","title":"Input/Output","text":"<ul> <li>Input: AST + GraphSchema</li> <li>Output: LogicalPlan (operator tree + symbol table)</li> </ul>"},{"location":"02-architecture/#logical-operators","title":"Logical Operators","text":"Python<pre><code># Base class\nclass LogicalOperator:\n    input_operators: list[LogicalOperator]\n    output_scope: Schema\n\n    def get_output_scope(self) -&gt; Schema\n    def propagate_data_types(self) -&gt; None\n    def dump_operator(self) -&gt; str\n\n# Terminal node (data source)\nclass StartLogicalOperator(LogicalOperator):\n    pass\n\n# Table/edge scan\nclass DataSourceOperator(StartLogicalOperator):\n    table_name: str\n    entity_schema: EntitySchema\n    filters: list[QueryExpression]  # pushable filters\n\n# Join\nclass JoinOperator(LogicalOperator):\n    join_type: JoinType  # INNER, LEFT, RIGHT, FULL\n    join_keys: list[JoinKeyPair]  # (left_col, right_col)\n\n# Variable-length path (WITH RECURSIVE)\nclass RecursiveTraversalOperator(LogicalOperator):\n    edge_type: str\n    min_depth: int\n    max_depth: int\n    direction: Direction\n    predicates: list[QueryExpression]  # pushable edge filters\n\n# Filters (WHERE)\nclass SelectionOperator(LogicalOperator):\n    predicate: QueryExpression\n\n# Projections (SELECT)\nclass ProjectionOperator(LogicalOperator):\n    projections: list[ProjectionItem]\n    aliases: dict[str, str]\n\n# Aggregation boundary (GROUP BY)\nclass AggregationBoundaryOperator(LogicalOperator):\n    group_by_keys: list[str]\n    aggregations: list[AggregationExpression]\n\n# UNWIND\nclass UnwindOperator(LogicalOperator):\n    list_expression: QueryExpression\n    alias: str\n\n# Set operations\nclass SetOperator(LogicalOperator):\n    set_type: SetOperationType  # UNION, INTERSECT, EXCEPT\n    distinct: bool\n</code></pre>"},{"location":"02-architecture/#operator-graph-example","title":"Operator Graph Example","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person)\nWHERE p.age &gt; 30\nRETURN p.name, COUNT(*) AS friends\n</code></pre></p> <p>Operator Tree: Text Only<pre><code>ProjectionOperator(projections=[p.name, COUNT(*) AS friends])\n\u2514\u2500\u2500 AggregationBoundaryOperator(group_by=[p])\n    \u2514\u2500\u2500 SelectionOperator(predicate: p.age &gt; 30)\n        \u2514\u2500\u2500 JoinOperator(join_type=INNER, keys=[(source.id, edge.source_id), (sink.id, edge.target_id)])\n            \u251c\u2500\u2500 DataSourceOperator(table=\"Person\", alias=\"source\")\n            \u251c\u2500\u2500 DataSourceOperator(table=\"Knows\", alias=\"edge\")\n            \u2514\u2500\u2500 DataSourceOperator(table=\"Person\", alias=\"sink\")\n</code></pre></p>"},{"location":"02-architecture/#phase-3-optimizer-conservative-transformations","title":"Phase 3: Optimizer (Conservative Transformations)","text":"<p>Location: src/gsql2rsql/planner/subquery_optimizer.py</p>"},{"location":"02-architecture/#responsibility_2","title":"Responsibility","text":"<p>Apply conservative transformations to reduce SQL nesting. Only flattens patterns guaranteed to preserve semantics.</p>"},{"location":"02-architecture/#optimization-rules","title":"Optimization Rules","text":"<p>Safe to Flatten: - \u2705 Selection \u2192 Projection: <code>WHERE</code> can be pushed before <code>SELECT</code> - \u2705 Selection \u2192 Selection: Multiple <code>WHERE</code> clauses can be merged</p> <p>NEVER Flattened: - \u274c Projection \u2192 Projection: Aliases must stay in separate subqueries - \u274c Anything involving aggregation boundaries: Aggregation semantics are fragile</p>"},{"location":"02-architecture/#configuration","title":"Configuration","text":"<ul> <li>Enabled by default</li> <li>Disable with <code>--no-optimize</code> CLI flag</li> </ul>"},{"location":"02-architecture/#phase-4-resolver-column-reference-validation","title":"Phase 4: Resolver (Column Reference Validation)","text":"<p>Location: src/gsql2rsql/planner/column_resolver.py</p>"},{"location":"02-architecture/#responsibility_3","title":"Responsibility","text":"<p>Validate ALL column references before rendering. Query schema for entity properties. Build resolution context for SQL generation.</p>"},{"location":"02-architecture/#key-components_2","title":"Key Components","text":"File Purpose <code>column_resolver.py</code> Main resolver implementation <code>column_ref.py</code> Resolved reference objects"},{"location":"02-architecture/#resolution-process","title":"Resolution Process","text":"<ol> <li>Visit operators in topological order</li> <li>Build symbol table tracking available columns at each operator</li> <li>Resolve all column references in expressions</li> <li>Validate property accesses against entity schemas</li> <li>Create <code>ResolvedColumnRef</code> objects with SQL column names</li> </ol>"},{"location":"02-architecture/#key-classes_1","title":"Key Classes","text":"Python<pre><code>class ResolvedColumnRef:\n    \"\"\"Resolved reference to a single column.\"\"\"\n    entity_name: str\n    property_name: str | None  # None = entire entity\n    sql_column_name: str\n    data_type: DataType\n\nclass ResolvedExpression:\n    \"\"\"Resolved expression with type info.\"\"\"\n    original_expression: QueryExpression\n    resolved_columns: list[ResolvedColumnRef]\n    data_type: DataType\n\nclass ResolutionResult:\n    \"\"\"Complete resolution context.\"\"\"\n    resolved_projections: dict[str, ResolvedProjection]\n    resolved_expressions: dict[int, ResolvedExpression]\n    column_mappings: dict[str, ResolvedColumnRef]\n</code></pre>"},{"location":"02-architecture/#error-handling","title":"Error Handling","text":"<p>The resolver provides rich error messages with suggestions:</p> Text Only<pre><code>Error: Column 'p.nam' not found in scope\nDid you mean: 'p.name'?\nAvailable columns: p.id, p.name, p.age\n</code></pre> <p>Uses Levenshtein distance for typo suggestions.</p>"},{"location":"02-architecture/#phase-5-renderer-sql-generation","title":"Phase 5: Renderer (SQL Generation)","text":"<p>Location: src/gsql2rsql/renderer/sql_renderer.py</p>"},{"location":"02-architecture/#responsibility_4","title":"Responsibility","text":"<p>Generate Databricks Spark SQL from logical plan using pre-resolved column references. Handle SQL dialect specifics.</p>"},{"location":"02-architecture/#key-components_3","title":"Key Components","text":"File Purpose Lines <code>sql_renderer.py</code> Main SQL code generator ~2000 <code>schema_provider.py</code> Database schema provider interface ~200"},{"location":"02-architecture/#inputoutput_2","title":"Input/Output","text":"<ul> <li>Input: LogicalPlan + ResolutionResult + GraphSchema</li> <li>Output: Databricks Spark SQL string</li> </ul>"},{"location":"02-architecture/#rendering-by-operator-type","title":"Rendering by Operator Type","text":"Python<pre><code>class SQLRenderer:\n    def render_plan(self, plan: LogicalPlan) -&gt; str:\n        \"\"\"Main entry point.\"\"\"\n\n    def _render_data_source(self, op: DataSourceOperator) -&gt; str:\n        \"\"\"Table scan with optional filters.\"\"\"\n\n    def _render_join(self, op: JoinOperator) -&gt; str:\n        \"\"\"JOIN clause with ON conditions.\"\"\"\n\n    def _render_selection(self, op: SelectionOperator) -&gt; str:\n        \"\"\"WHERE clause.\"\"\"\n\n    def _render_projection(self, op: ProjectionOperator) -&gt; str:\n        \"\"\"SELECT with aliases.\"\"\"\n\n    def _render_aggregation(self, op: AggregationBoundaryOperator) -&gt; str:\n        \"\"\"GROUP BY with HAVING.\"\"\"\n\n    def _render_recursive(self, op: RecursiveTraversalOperator) -&gt; str:\n        \"\"\"WITH RECURSIVE CTE for variable-length paths.\"\"\"\n\n    def _render_unwind(self, op: UnwindOperator) -&gt; str:\n        \"\"\"LATERAL VIEW EXPLODE for UNWIND.\"\"\"\n\n    def _render_set_operator(self, op: SetOperator) -&gt; str:\n        \"\"\"UNION / INTERSECT / EXCEPT.\"\"\"\n</code></pre>"},{"location":"02-architecture/#special-sql-patterns","title":"Special SQL Patterns","text":"<p>Variable-Length Paths (WITH RECURSIVE): SQL<pre><code>WITH RECURSIVE paths_1 AS (\n    -- Base case: direct edges\n    SELECT e.source_id AS start_node, e.target_id AS end_node, 1 AS depth,\n           ARRAY(e.source_id, e.target_id) AS path,\n           ARRAY(e.source_id) AS visited\n    FROM graph.Knows e\n    UNION ALL\n    -- Recursive case: extend paths\n    SELECT p.start_node, e.target_id AS end_node, p.depth + 1 AS depth,\n           CONCAT(p.path, ARRAY(e.target_id)) AS path,\n           CONCAT(p.visited, ARRAY(e.source_id)) AS visited\n    FROM paths_1 p\n    JOIN graph.Knows e ON p.end_node = e.source_id\n    WHERE p.depth &lt; 5 AND NOT array_contains(p.visited, e.target_id)\n)\n</code></pre></p> <p>OPTIONAL MATCH (LEFT JOIN with COALESCE): SQL<pre><code>LEFT JOIN graph.Person AS f ON p.friend_id = f.id\n-- Later: COALESCE(f.name, 'Unknown')\n</code></pre></p> <p>Edge Collection (STRUCT for properties): SQL<pre><code>COLLECT(STRUCT(edge.amount AS amount, edge.timestamp AS timestamp)) AS edges\n</code></pre></p>"},{"location":"02-architecture/#directory-structure","title":"Directory Structure","text":"Text Only<pre><code>src/gsql2rsql/\n\u251c\u2500\u2500 __init__.py                    # Public API exports\n\u251c\u2500\u2500 cli.py                         # CLI with TUI (2200 lines)\n\u251c\u2500\u2500 pyspark_executor.py            # Runtime validation\n\u2502\n\u251c\u2500\u2500 parser/                        # Phase 1: Syntax\n\u2502   \u251c\u2500\u2500 opencypher_parser.py      # Main entry point\n\u2502   \u251c\u2500\u2500 ast.py                    # AST nodes (50+ types)\n\u2502   \u251c\u2500\u2500 visitor.py                # ANTLR visitor\n\u2502   \u251c\u2500\u2500 operators.py              # Operator enums\n\u2502   \u2514\u2500\u2500 grammar/                  # ANTLR-generated\n\u2502\n\u251c\u2500\u2500 planner/                       # Phase 2-4: Semantics\n\u2502   \u251c\u2500\u2500 logical_plan.py           # AST \u2192 operators\n\u2502   \u251c\u2500\u2500 operators.py              # 11 operator types\n\u2502   \u251c\u2500\u2500 symbol_table.py           # Variable tracking\n\u2502   \u251c\u2500\u2500 path_analyzer.py          # Path optimization\n\u2502   \u251c\u2500\u2500 schema.py                 # Internal schema\n\u2502   \u251c\u2500\u2500 column_resolver.py        # Phase 4: Resolution\n\u2502   \u251c\u2500\u2500 column_ref.py             # Resolved refs\n\u2502   \u2514\u2500\u2500 subquery_optimizer.py     # Phase 3: Optimization\n\u2502\n\u251c\u2500\u2500 renderer/                      # Phase 5: SQL generation\n\u2502   \u251c\u2500\u2500 sql_renderer.py           # Main codegen (2000 lines)\n\u2502   \u2514\u2500\u2500 schema_provider.py        # Schema interface\n\u2502\n\u2514\u2500\u2500 common/                        # Shared infrastructure\n    \u251c\u2500\u2500 schema.py                 # Graph schema definitions\n    \u251c\u2500\u2500 exceptions.py             # Rich error types\n    \u251c\u2500\u2500 logging.py                # Debug logging\n    \u2514\u2500\u2500 utils.py                  # Utilities\n</code></pre>"},{"location":"02-architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Why 4-Phase Separation?</li> <li>Correctness: Each phase has clear invariants</li> <li>Testability: Each phase can be tested independently</li> <li>Maintainability: Changes to one phase don't ripple to others</li> <li> <p>Debuggability: Can inspect intermediate representations (AST, operators, resolution)</p> </li> <li> <p>Why ANTLR?</p> </li> <li>Industry-standard parser generator</li> <li>Excellent error recovery</li> <li>Visitor pattern for clean AST construction</li> <li> <p>OpenCypher grammar already exists (adapted here)</p> </li> <li> <p>Why Logical Operators?</p> </li> <li>Relational algebra is well-understood</li> <li>Easy to reason about correctness</li> <li>Enables optimization passes</li> <li> <p>Clear mapping to SQL constructs</p> </li> <li> <p>Why Separate Resolution Phase?</p> </li> <li>Column references can't be validated during planning (schema not fully known)</li> <li>Renderer needs pre-resolved refs to avoid semantic decisions</li> <li> <p>Clear error messages require full context</p> </li> <li> <p>Why Conservative Optimizer?</p> </li> <li>Safety over performance (correctness first)</li> <li>User-transparent (can disable with <code>--no-optimize</code>)</li> <li>Only proven-safe patterns</li> </ol>"},{"location":"02-architecture/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>03-decision-log.md \u2014 Detailed design rationale</li> <li>CONTRIBUTING.md \u2014 Full architectural boundaries</li> <li>07-developer-guide.md \u2014 How to extend the transpiler</li> <li>src/gsql2rsql/ \u2014 Source code</li> </ul>"},{"location":"03-decision-log/","title":"Architectural Decision Log","text":"<p>This document captures important design decisions, their rationale, and where they're implemented in the codebase.</p>"},{"location":"03-decision-log/#decision-1-strict-4-phase-separation-of-concerns","title":"Decision 1: Strict 4-Phase Separation of Concerns","text":"<p>Status: \u2705 Adopted (documented in CONTRIBUTING.md)</p>"},{"location":"03-decision-log/#context","title":"Context","text":"<p>Transpilers often mix parsing, semantic analysis, and code generation, leading to tight coupling and hard-to-debug issues.</p>"},{"location":"03-decision-log/#decision","title":"Decision","text":"<p>Enforce strict separation across 4 phases: 1. Parser: Syntax only (no schema access) 2. Planner: Semantics only (no column validation) 3. Resolver: Validation only (no SQL generation) 4. Renderer: Code generation only (no semantic decisions)</p>"},{"location":"03-decision-log/#rationale","title":"Rationale","text":"<ul> <li>Correctness: Each phase has clear invariants and responsibilities</li> <li>Testability: Can test each phase independently with mocked inputs</li> <li>Debuggability: Can inspect intermediate representations (AST, operators, resolution results)</li> <li>Maintainability: Changes to one phase don't cascade to others</li> </ul>"},{"location":"03-decision-log/#trade-offs","title":"Trade-offs","text":"<ul> <li>More code: Separate data structures for each phase (AST, operators, resolved refs)</li> <li>More memory: Keep AST + operators + resolution in memory</li> <li>Performance: Multiple passes over the data structure (acceptable for query compilation)</li> </ul>"},{"location":"03-decision-log/#implementation","title":"Implementation","text":"<ul> <li>Parser: src/gsql2rsql/parser/</li> <li>Planner: src/gsql2rsql/planner/logical_plan.py</li> <li>Resolver: src/gsql2rsql/planner/column_resolver.py</li> <li>Renderer: src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"03-decision-log/#decision-2-antlr-for-parsing-vs-hand-written-parser","title":"Decision 2: ANTLR for Parsing (vs. Hand-Written Parser)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_1","title":"Context","text":"<p>OpenCypher grammar is complex (multiple expression types, operator precedence, pattern matching). Need robust error recovery.</p>"},{"location":"03-decision-log/#decision_1","title":"Decision","text":"<p>Use ANTLR 4.13+ parser generator with visitor pattern to build AST.</p>"},{"location":"03-decision-log/#rationale_1","title":"Rationale","text":"<ul> <li>Correctness: Proven grammar, handles precedence and associativity correctly</li> <li>Maintainability: Grammar is declarative and separate from code</li> <li>Error recovery: ANTLR has excellent error reporting and recovery</li> <li>Standards: OpenCypher community provides reference ANTLR grammars</li> </ul>"},{"location":"03-decision-log/#trade-offs_1","title":"Trade-offs","text":"<ul> <li>Dependency: Requires ANTLR runtime (<code>antlr4-python3-runtime&gt;=4.13.0</code>)</li> <li>Build step: Must regenerate parser when grammar changes (<code>make grammar</code>)</li> <li>Generated code: ~8000 lines of generated parser code in repo</li> </ul>"},{"location":"03-decision-log/#implementation_1","title":"Implementation","text":"<ul> <li>Grammar: CypherParser.g4 (INFERRED: root-level grammar file)</li> <li>Generated files: src/gsql2rsql/parser/grammar/</li> <li>Visitor: src/gsql2rsql/parser/visitor.py</li> <li>Entry point: src/gsql2rsql/parser/opencypher_parser.py</li> </ul>"},{"location":"03-decision-log/#decision-3-logical-operators-vs-direct-ast-sql","title":"Decision 3: Logical Operators (vs. Direct AST \u2192 SQL)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_2","title":"Context","text":"<p>Direct AST-to-SQL translation is fragile and hard to optimize. SQL generation logic becomes deeply nested.</p>"},{"location":"03-decision-log/#decision_2","title":"Decision","text":"<p>Introduce a logical operator layer (relational algebra) between AST and SQL.</p>"},{"location":"03-decision-log/#rationale_2","title":"Rationale","text":"<ul> <li>Optimization: Can apply transformations on operators before SQL generation</li> <li>Clarity: Operators have clear semantics (DataSource, Join, Selection, Projection, etc.)</li> <li>Correctness: Easier to reason about query semantics in relational algebra terms</li> <li>Extensibility: New SQL backends can reuse operator layer</li> </ul>"},{"location":"03-decision-log/#trade-offs_2","title":"Trade-offs","text":"<ul> <li>Complexity: Another intermediate representation</li> <li>Memory: Operator tree + AST in memory simultaneously during planning</li> </ul>"},{"location":"03-decision-log/#implementation_2","title":"Implementation","text":"<ul> <li>Operators: src/gsql2rsql/planner/operators.py</li> <li>Conversion: src/gsql2rsql/planner/logical_plan.py:LogicalPlan.process_query_tree() (~line 100+)</li> <li>Rendering: src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"03-decision-log/#decision-4-symbol-table-with-nested-scopes","title":"Decision 4: Symbol Table with Nested Scopes","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_3","title":"Context","text":"<p>Cypher <code>WITH</code> clauses create new scopes. Variables can be redefined. Need to track what's available at each point in the query.</p>"},{"location":"03-decision-log/#decision_3","title":"Decision","text":"<p>Build a symbol table during planning with support for nested scopes. Each <code>WITH</code> creates a new scope.</p>"},{"location":"03-decision-log/#rationale_3","title":"Rationale","text":"<ul> <li>Correctness: Prevents variable name collisions</li> <li>Error messages: Can show available variables when reference is invalid</li> <li>Type tracking: Track entity vs. value types for each variable</li> </ul>"},{"location":"03-decision-log/#trade-offs_3","title":"Trade-offs","text":"<ul> <li>Complexity: Scope management adds cognitive overhead</li> <li>Edge cases: Handling variable shadowing, scope boundaries</li> </ul>"},{"location":"03-decision-log/#implementation_3","title":"Implementation","text":"<ul> <li>Symbol table: src/gsql2rsql/planner/symbol_table.py</li> <li>Usage in planner: src/gsql2rsql/planner/logical_plan.py (built during AST traversal)</li> <li>Scope tracking: <code>SymbolTable.enter_scope()</code> / <code>SymbolTable.exit_scope()</code> methods</li> </ul>"},{"location":"03-decision-log/#decision-5-separate-resolution-phase-vs-resolving-during-planning","title":"Decision 5: Separate Resolution Phase (vs. Resolving During Planning)","text":"<p>Status: \u2705 Adopted (documented in CONTRIBUTING.md)</p>"},{"location":"03-decision-log/#context_4","title":"Context","text":"<p>Column references can't be validated during planning because: 1. Schema propagation happens bottom-up 2. Aggregation boundaries affect column availability 3. Need full operator graph to compute scopes</p>"},{"location":"03-decision-log/#decision_4","title":"Decision","text":"<p>Add a separate resolution phase after planning. Walk operator graph, build scopes, resolve all column references.</p>"},{"location":"03-decision-log/#rationale_4","title":"Rationale","text":"<ul> <li>Correctness: Full context available for validation</li> <li>Error messages: Can provide suggestions using Levenshtein distance</li> <li>Separation: Renderer doesn't need to make semantic decisions</li> </ul>"},{"location":"03-decision-log/#trade-offs_4","title":"Trade-offs","text":"<ul> <li>Extra pass: Must visit operator graph twice (once for planning, once for resolution)</li> <li>Memory: Store ResolutionResult alongside LogicalPlan</li> </ul>"},{"location":"03-decision-log/#implementation_4","title":"Implementation","text":"<ul> <li>Resolver: src/gsql2rsql/planner/column_resolver.py:ColumnResolver.resolve() (~line 50+)</li> <li>Invocation: src/gsql2rsql/planner/logical_plan.py:LogicalPlan.resolve()</li> <li>Resolution data: src/gsql2rsql/planner/column_ref.py</li> </ul>"},{"location":"03-decision-log/#decision-6-conservative-optimizer-vs-aggressive-flattening","title":"Decision 6: Conservative Optimizer (vs. Aggressive Flattening)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_5","title":"Context","text":"<p>SQL subquery nesting can be deep, affecting readability and performance. But aggressive flattening can change semantics.</p>"},{"location":"03-decision-log/#decision_5","title":"Decision","text":"<p>Only flatten patterns proven safe: - \u2705 Selection \u2192 Projection (WHERE before SELECT) - \u2705 Selection \u2192 Selection (merge WHERE clauses) - \u274c Projection \u2192 Projection (aliases must stay in separate subqueries) - \u274c Anything involving aggregation</p>"},{"location":"03-decision-log/#rationale_5","title":"Rationale","text":"<ul> <li>Safety first: Wrong results are worse than slower SQL</li> <li>Transparency: User can disable with <code>--no-optimize</code></li> <li>Debuggability: Easier to debug generated SQL when flattening is conservative</li> </ul>"},{"location":"03-decision-log/#trade-offs_5","title":"Trade-offs","text":"<ul> <li>Performance: May generate more subqueries than necessary</li> <li>SQL readability: Nested queries can be hard to read</li> </ul>"},{"location":"03-decision-log/#implementation_5","title":"Implementation","text":"<ul> <li>Optimizer: src/gsql2rsql/planner/subquery_optimizer.py:SubqueryFlatteningOptimizer</li> <li>Invocation: CLI flag <code>--optimize</code> (enabled by default, can disable with <code>--no-optimize</code>)</li> <li>Rules: <code>_can_flatten_selection_projection()</code>, <code>_can_flatten_selection_selection()</code> methods</li> </ul>"},{"location":"03-decision-log/#decision-7-with-recursive-for-variable-length-paths","title":"Decision 7: WITH RECURSIVE for Variable-Length Paths","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_6","title":"Context","text":"<p>Variable-length paths (<code>-[:KNOWS*1..5]-&gt;</code>) require graph traversal. Options: 1. Generate PySpark DataFrame code (requires runtime, not pure SQL) 2. Use <code>WITH RECURSIVE</code> CTE (SQL standard, Databricks 17+) 3. Expand paths at transpile time (exponential blowup)</p>"},{"location":"03-decision-log/#decision_6","title":"Decision","text":"<p>Use <code>WITH RECURSIVE</code> CTEs for variable-length paths. Generate BFS/DFS traversal in SQL.</p>"},{"location":"03-decision-log/#rationale_6","title":"Rationale","text":"<ul> <li>Pure SQL: No runtime dependencies beyond Databricks SQL</li> <li>Scalability: Recursive CTE scales to large graphs</li> <li>Correctness: Cycle detection with <code>visited</code> array</li> <li>Standard: SQL standard feature, widely supported</li> </ul>"},{"location":"03-decision-log/#trade-offs_6","title":"Trade-offs","text":"<ul> <li>Databricks 17+ only: Not supported in older runtimes</li> <li>Performance: Recursive CTEs can be slow for deep/wide graphs</li> <li>Complexity: Generated SQL is verbose</li> </ul>"},{"location":"03-decision-log/#implementation_6","title":"Implementation","text":"<ul> <li>Operator: src/gsql2rsql/planner/operators.py:RecursiveTraversalOperator</li> <li>Rendering: src/gsql2rsql/renderer/sql_renderer.py:_render_recursive() (~line 800+)</li> <li>Example: See tests/output/expected/21_variable_length_zero.sql</li> </ul>"},{"location":"03-decision-log/#decision-8-path-analyzer-for-edge-optimization","title":"Decision 8: Path Analyzer for Edge Optimization","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_7","title":"Context","text":"<p>Variable-length paths generate edge collections by default. But if the user doesn't access <code>relationships(path)</code>, we're collecting edges for nothing.</p>"},{"location":"03-decision-log/#decision_7","title":"Decision","text":"<p>Analyze usage of <code>relationships(path)</code> function. If not used, skip edge collection in recursive CTE.</p>"},{"location":"03-decision-log/#rationale_7","title":"Rationale","text":"<ul> <li>Performance: Collecting edges is expensive (ARRAY CONCAT on every recursion)</li> <li>Memory: Edge arrays grow linearly with path depth</li> <li>Optimization: Zero-cost abstraction when edges aren't needed</li> </ul>"},{"location":"03-decision-log/#trade-offs_7","title":"Trade-offs","text":"<ul> <li>Complexity: Must analyze expression trees to detect usage</li> <li>Correctness: Must handle ALL/ANY predicates that access edges</li> </ul>"},{"location":"03-decision-log/#implementation_7","title":"Implementation","text":"<ul> <li>Analyzer: src/gsql2rsql/planner/path_analyzer.py:PathAnalyzer</li> <li>Usage: Called during planning for <code>RecursiveTraversalOperator</code> construction</li> <li>Optimization: <code>needs_edge_collection</code> flag in <code>RecursiveTraversalOperator</code></li> </ul>"},{"location":"03-decision-log/#decision-9-structured-sql-column-naming","title":"Decision 9: Structured SQL Column Naming","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_8","title":"Context","text":"<p>Cypher variables (<code>p</code>, <code>f</code>) must map to SQL columns. Need to avoid collisions and track entity vs. property.</p>"},{"location":"03-decision-log/#decision_8","title":"Decision","text":"<p>Use structured naming convention: <code>_gsql2rsql_{variable}_{property}</code> - Entity projection: <code>_gsql2rsql_p_id AS p</code> - Property projection: <code>_gsql2rsql_p_name AS name</code></p>"},{"location":"03-decision-log/#rationale_8","title":"Rationale","text":"<ul> <li>Collision avoidance: Prefix prevents conflicts with user column names</li> <li>Tracking: Can parse column name to extract variable and property</li> <li>Debugging: Clear provenance in generated SQL</li> </ul>"},{"location":"03-decision-log/#trade-offs_8","title":"Trade-offs","text":"<ul> <li>Verbosity: Column names are long</li> <li>SQL readability: Generated SQL is harder to read for humans</li> </ul>"},{"location":"03-decision-log/#implementation_8","title":"Implementation","text":"<ul> <li>Naming: src/gsql2rsql/planner/column_ref.py:compute_sql_column_name()</li> <li>Usage: Throughout src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"03-decision-log/#decision-10-schema-as-json-vs-code-or-yaml","title":"Decision 10: Schema as JSON (vs. Code or YAML)","text":"<p>Status: \u2705 Adopted (JSON), \u26a0\ufe0f YAML for examples</p>"},{"location":"03-decision-log/#context_9","title":"Context","text":"<p>Graph schema defines nodes, edges, properties, and their SQL table mappings. Need a format that's: - Human-readable for editing - Machine-parseable for validation - Version-controllable</p>"},{"location":"03-decision-log/#decision_9","title":"Decision","text":"<p>Use JSON for schema definitions. YAML for example collections (with embedded schemas).</p>"},{"location":"03-decision-log/#rationale_9","title":"Rationale","text":"<ul> <li>JSON: Strict, widely supported, easy to validate with JSON Schema</li> <li>YAML: More readable for large example collections with comments</li> <li>Separation: Schema is separate from queries (vs. inline)</li> </ul>"},{"location":"03-decision-log/#trade-offs_9","title":"Trade-offs","text":"<ul> <li>JSON verbosity: More braces and quotes than YAML</li> <li>No comments: JSON doesn't support comments (use description fields)</li> <li>Two formats: Inconsistency between schema.json and examples/*.yaml</li> </ul>"},{"location":"03-decision-log/#implementation_9","title":"Implementation","text":"<ul> <li>JSON schema: examples/schema.json</li> <li>YAML examples: examples/credit_queries.yaml</li> <li>Schema loading: src/gsql2rsql/common/schema.py</li> <li>YAML loading: src/gsql2rsql/pyspark_executor.py:load_schema_from_yaml()</li> </ul>"},{"location":"03-decision-log/#decision-11-golden-file-testing","title":"Decision 11: Golden File Testing","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_10","title":"Context","text":"<p>Need to validate that transpiler generates correct SQL. Can't compare generated SQL directly (whitespace, formatting differences).</p>"},{"location":"03-decision-log/#decision_10","title":"Decision","text":"<p>Use golden file testing: store expected SQL in <code>tests/output/expected/</code>, compare normalized SQL.</p>"},{"location":"03-decision-log/#rationale_10","title":"Rationale","text":"<ul> <li>Regression detection: Any change to generated SQL is caught</li> <li>Visual diff: Can use standard diff tools to review changes</li> <li>Documentation: Expected files serve as examples</li> </ul>"},{"location":"03-decision-log/#trade-offs_10","title":"Trade-offs","text":"<ul> <li>Maintenance: Must update expected files when SQL format changes</li> <li>False positives: Formatting changes trigger test failures</li> </ul>"},{"location":"03-decision-log/#implementation_10","title":"Implementation","text":"<ul> <li>Expected SQL: tests/output/expected/</li> <li>Comparison: tests/utils/sql_test_utils.py:assert_sql_equal()</li> <li>Diff output: tests/output/diff/</li> <li>Update command: <code>make dump-sql-save ID=01 NAME=simple_node_lookup</code></li> </ul>"},{"location":"03-decision-log/#decision-12-pyspark-validation-tests","title":"Decision 12: PySpark Validation Tests","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_11","title":"Context","text":"<p>Generated SQL might be syntactically correct but semantically wrong. Need to validate against real execution.</p>"},{"location":"03-decision-log/#decision_11","title":"Decision","text":"<p>Run transpiled SQL on PySpark DataFrames, compare results against expected outcomes.</p>"},{"location":"03-decision-log/#rationale_11","title":"Rationale","text":"<ul> <li>Correctness: Catches semantic bugs that golden files miss</li> <li>Integration: Validates entire pipeline end-to-end</li> <li>Realistic: Uses actual Spark SQL engine</li> </ul>"},{"location":"03-decision-log/#trade-offs_11","title":"Trade-offs","text":"<ul> <li>Slow: PySpark tests take 10-20x longer than unit tests</li> <li>Dependency: Requires PySpark (only for dev, not for users)</li> <li>Flakiness: Spark startup/shutdown can be flaky</li> </ul>"},{"location":"03-decision-log/#implementation_11","title":"Implementation","text":"<ul> <li>Test file: tests/test_examples_with_pyspark.py</li> <li>Executor: src/gsql2rsql/pyspark_executor.py</li> <li>Run command: <code>make test-pyspark-examples</code></li> <li>Quick validation: <code>make test-pyspark-quick</code> (first 5 examples)</li> </ul>"},{"location":"03-decision-log/#decision-13-textual-tui-for-interactive-development","title":"Decision 13: Textual TUI for Interactive Development","text":"<p>Status: \u2705 Adopted</p>"},{"location":"03-decision-log/#context_12","title":"Context","text":"<p>Developers need to iterate on queries quickly. Command-line round-trip is slow. Need visual feedback for errors.</p>"},{"location":"03-decision-log/#decision_12","title":"Decision","text":"<p>Build interactive TUI using Textual framework with live transpilation, schema switching, and clipboard support.</p>"},{"location":"03-decision-log/#rationale_12","title":"Rationale","text":"<ul> <li>Productivity: Edit query, see SQL immediately</li> <li>Discoverability: Browse curated examples from YAML files</li> <li>Debugging: See AST, operators, scope info in UI</li> <li>UX: Rich formatting, syntax highlighting, copy-paste support</li> </ul>"},{"location":"03-decision-log/#trade-offs_12","title":"Trade-offs","text":"<ul> <li>Dependency: Requires <code>textual&gt;=0.47.0</code> (heavy framework)</li> <li>Maintenance: TUI code is complex (2200 lines in cli.py)</li> </ul>"},{"location":"03-decision-log/#implementation_12","title":"Implementation","text":"<ul> <li>TUI command: <code>gsql2rsql tui --examples examples/credit_queries.yaml</code></li> <li>Implementation: src/gsql2rsql/cli.py:tui() (~line 1500+)</li> <li>Dependencies: <code>textual</code>, <code>rich</code>, <code>prompt-toolkit</code></li> </ul>"},{"location":"03-decision-log/#decision-14-defensive-handling-of-pre-rendered-field-names","title":"Decision 14: Defensive Handling of Pre-Rendered Field Names","text":"<p>Status: \u2705 Adopted (2026-01-19)</p>"},{"location":"03-decision-log/#context_13","title":"Context","text":"<p>Different operators produce entity fields with different naming states: - DataSourceOperator: <code>field_name = \"id\"</code> (simple property name) - RecursiveTraversalOperator: <code>field_name = \"_gsql2rsql_peer_id\"</code> (full SQL name)</p> <p>Renderer methods assumed all <code>field_name</code> values were simple property names, causing double-prefixing for recursive traversal outputs.</p>"},{"location":"03-decision-log/#decision_13","title":"Decision","text":"<p>Add defensive checks in renderer to detect pre-rendered SQL column names: Python<pre><code>if field.field_name and field.field_name.startswith(COLUMN_PREFIX):\n    # Already a full SQL name - use as-is\n    key = field.field_name\nelse:\n    # Simple property name - construct full SQL name\n    key = self._get_field_name(entity_alias, field.field_alias)\n</code></pre></p>"},{"location":"03-decision-log/#rationale_13","title":"Rationale","text":"<ul> <li>Correctness: Prevents double-prefixing (<code>_gsql2rsql_peer_peer_id</code> \u274c)</li> <li>Backward compatible: Doesn't break existing DataSourceOperator behavior</li> <li>Minimal changes: Only affects renderer, no planner changes needed</li> <li>Preserves information: Keeps pre-rendered names for debugging</li> </ul>"},{"location":"03-decision-log/#trade-offs_13","title":"Trade-offs","text":"<ul> <li>More complex: Requires checks in 10+ locations across renderer</li> <li>Implicit contract: Relies on <code>COLUMN_PREFIX</code> to detect pre-rendered names</li> <li>Alternative rejected: Could have cleared <code>field_name</code> after recursive traversal, but loses debugging information</li> </ul>"},{"location":"03-decision-log/#implementation_13","title":"Implementation","text":"<ul> <li>Renderer: src/gsql2rsql/renderer/sql_renderer.py</li> <li><code>_collect_required_columns()</code> (~lines 447-490)</li> <li><code>_render_join_conditions()</code> (~lines 2311-2394)</li> <li><code>_render_join_projection()</code> (~lines 1941-2049)</li> <li><code>_extract_columns_from_schema()</code> (~lines 2166-2200)</li> <li>Tests: tests/test_variable_length_path_field_naming.py</li> <li>Documentation: docs/development/VARLEN_PATH_FIELD_NAMING_FIX.md</li> </ul>"},{"location":"03-decision-log/#open-questions-todos","title":"Open Questions / TODOs","text":""},{"location":"03-decision-log/#1-support-for-neo4j-specific-functions","title":"1. Support for Neo4j-Specific Functions","text":"<p>Question: Should we add support for Neo4j-specific functions (e.g., <code>apoc.*</code>)? Trade-off: Would require custom UDFs in Databricks, breaking pure SQL generation. Decision needed: User feedback on priority of Neo4j compatibility.</p>"},{"location":"03-decision-log/#2-multi-graph-support","title":"2. Multi-Graph Support","text":"<p>Question: Should schema support multiple named graphs (vs. single graph per schema)? Current: Single graph per schema file. Trade-off: More complex schema format, but enables multi-tenancy patterns.</p>"},{"location":"03-decision-log/#3-query-planner-hints","title":"3. Query Planner Hints","text":"<p>Question: Should we allow users to hint join order or index usage in Cypher comments? Current: No hint support, Databricks optimizer makes all decisions. Trade-off: Power users want control, but hints are non-portable.</p>"},{"location":"03-decision-log/#4-alternative-sql-backends","title":"4. Alternative SQL Backends","text":"<p>Question: Should we support other SQL dialects (PostgreSQL, DuckDB, SQLite)? Current: Databricks-only. Trade-off: Broader adoption vs. maintenance burden. Need to abstract SQL dialect differences. File to modify: src/gsql2rsql/renderer/sql_renderer.py (add dialect parameter)</p>"},{"location":"03-decision-log/#5-property-graph-schema-import","title":"5. Property Graph Schema Import","text":"<p>Question: Should we auto-generate schema from existing Databricks tables using <code>DESCRIBE TABLE</code>? Current: Manual JSON schema creation. Trade-off: Convenience vs. requiring database connection at transpile time. Related command: <code>gsql2rsql init-schema</code> (currently generates template, could introspect DB)</p>"},{"location":"03-decision-log/#6-incremental-compilation","title":"6. Incremental Compilation","text":"<p>Question: Should we cache AST/operators for repeated queries? Current: Full transpilation on every invocation. Trade-off: Performance vs. complexity. Only matters for hot-path query generation.</p>"},{"location":"03-decision-log/#7-aggregation-after-aggregation","title":"7. Aggregation After Aggregation","text":"<p>Question: Current architecture allows aggregation after aggregation. Is this correct? Context: Recent commits mention \"aggregation entity projection\" bugs. Investigation needed: Review src/gsql2rsql/planner/operators.py:AggregationBoundaryOperator (~line 600+) Related test: tests/test_aggregation_entity_projection.py</p>"},{"location":"03-decision-log/#8-predicate-pushdown-completeness","title":"8. Predicate Pushdown Completeness","text":"<p>Question: Are all pushable predicates being pushed to recursive CTEs? Context: PathAnalyzer handles some cases, but complex predicates might not be covered. Investigation needed: Review src/gsql2rsql/planner/path_analyzer.py Related tests: <code>test_37_source_node_filter_pushdown.py</code>, <code>test_39_recursive_sink_filter_pushdown.py</code></p>"},{"location":"03-decision-log/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>04-limitations.md \u2014 Known limitations and unsupported features</li> <li>CONTRIBUTING.md \u2014 Detailed phase boundaries</li> <li>02-architecture.md \u2014 Component breakdown</li> </ul>"},{"location":"04-limitations/","title":"Limitations and Unsupported Features","text":"<p>This document lists known limitations, unsupported OpenCypher features, runtime caveats, and recommended workarounds.</p>"},{"location":"04-limitations/#runtime-requirements","title":"Runtime Requirements","text":""},{"location":"04-limitations/#databricks-runtime-17-required","title":"Databricks Runtime 17+ Required","text":"<p>Requirement: Databricks Runtime 17 or higher</p> <p>Reason: The transpiler generates <code>WITH RECURSIVE</code> CTEs for variable-length paths. This feature was added in Databricks Runtime 17.</p> <p>Affected Features: - Variable-length paths: <code>-[:TYPE*1..N]-&gt;</code> - Any query with <code>*</code> in relationship patterns</p> <p>Workaround: Upgrade to Databricks Runtime 17+. No workaround for older runtimes (would require complete rewrite to PySpark DataFrame code).</p>"},{"location":"04-limitations/#spark-sql-limitations","title":"Spark SQL Limitations","text":"<p>ARRAY Operations: Generated SQL uses <code>ARRAY()</code>, <code>CONCAT()</code>, <code>array_contains()</code> functions. - These require Spark SQL, not standard ANSI SQL - Older Spark versions may have incomplete array support</p> <p>STRUCT for Edge Collections: Edge property collections use <code>COLLECT(STRUCT(...))</code>. - Requires Spark SQL struct support - May have performance implications on very large result sets</p>"},{"location":"04-limitations/#undirected-relationship-performance-optimized","title":"Undirected Relationship Performance (OPTIMIZED)","text":"<p>Status: \u2705 Optimized (as of 2026-01-19)</p> <p>Default Behavior: Undirected relationships (<code>-[:TYPE]-</code>) now use UNION ALL edge expansion for optimal performance.</p> <p>Example Query: Cypher<pre><code>-- Undirected relationship (fast with optimization)\nMATCH (a:Person)-[:KNOWS]-(b:Person)\nWHERE a.name = 'Alice'\nRETURN b.name\n</code></pre></p> <p>Generated SQL (Optimized - Default): SQL<pre><code>-- Edges expanded bidirectionally before joining\nJOIN (\n  SELECT source_id AS node_id, target_id AS other_id, props FROM Knows\n  UNION ALL\n  SELECT target_id AS node_id, source_id AS other_id, props FROM Knows\n) k ON person.id = k.node_id\n</code></pre></p> <p>Performance: - Small datasets (&lt; 1000 rows): Fast (hash join) - Medium datasets (1K-100K rows): Fast (hash join with indexes) - Large datasets (&gt; 100K rows): Optimized (O(n) instead of O(n\u00b2))</p> <p>Known Limitation: Self-loops (e.g., <code>(a)-[:KNOWS]-(a)</code>) may appear twice in results. - Workaround: Add <code>WHERE a.id &lt;&gt; b.id</code> or use <code>DISTINCT</code></p> <p>Disabling Optimization (for debugging or compatibility): Python<pre><code>from gsql2rsql import SQLRenderer\n\n# Use legacy OR join strategy (slower but simpler SQL)\nrenderer = SQLRenderer(\n    db_schema_provider=schema,\n    config={\"undirected_strategy\": \"or_join\"}  # Not recommended\n)\n</code></pre></p> <p>Generated SQL (Legacy - OR Join): SQL<pre><code>-- Only use for small datasets or debugging\nON (person.id = knows.source_id OR person.id = knows.target_id)\n</code></pre></p> <p>Learn More: See UNDIRECTED_OPTIMIZATION_IMPLEMENTATION.md for implementation details and trade-off analysis.</p>"},{"location":"04-limitations/#unsupported-opencypher-features","title":"Unsupported OpenCypher Features","text":""},{"location":"04-limitations/#1-merge-upsert-operations","title":"1. MERGE (Upsert Operations)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: <code>MERGE</code> requires write operations. The transpiler generates read-only SQL queries.</p> <p>Example (unsupported): Cypher<pre><code>MERGE (p:Person {id: 123})\nON CREATE SET p.created_at = timestamp()\nON MATCH SET p.last_seen = timestamp()\n</code></pre></p> <p>Workaround: Use Databricks <code>MERGE INTO</code> statement directly (not via transpiler).</p>"},{"location":"04-limitations/#2-create-delete-set-remove-write-operations","title":"2. CREATE, DELETE, SET, REMOVE (Write Operations)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: Transpiler is read-only. No support for graph mutations.</p> <p>Examples (all unsupported): Cypher<pre><code>CREATE (p:Person {name: 'Alice'})\nDELETE p\nSET p.age = 30\nREMOVE p.age\n</code></pre></p> <p>Workaround: Use standard SQL <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> statements on your Databricks tables.</p>"},{"location":"04-limitations/#3-shortest-path-shortestpath-allshortestpaths","title":"3. Shortest Path (shortestPath, allShortestPaths)","text":"<p>Status: \u26a0\ufe0f Partial Support (INFERRED)</p> <p>Supported: BFS traversal with depth tracking via <code>-[:TYPE*1..N]-&gt;</code> Not Supported: Built-in <code>shortestPath()</code> and <code>allShortestPaths()</code> functions</p> <p>Example: Cypher<pre><code>-- \u274c Not supported\nMATCH p = shortestPath((a:Person)-[:KNOWS*]-(b:Person))\nRETURN p\n\n-- \u2705 Workaround: use bounded BFS with ORDER BY depth LIMIT 1\nMATCH (a:Person)-[:KNOWS*1..10]-(b:Person)\nWHERE a.id = 1 AND b.id = 100\nRETURN a, b\nORDER BY length(relationships(path)) ASC\nLIMIT 1\n</code></pre></p> <p>Limitation: Workaround requires explicit max depth and may be inefficient for large graphs.</p>"},{"location":"04-limitations/#4-foreach-iteration-with-side-effects","title":"4. FOREACH (Iteration with Side Effects)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: <code>FOREACH</code> is for mutations (side effects). Transpiler is read-only.</p> <p>Example (unsupported): Cypher<pre><code>MATCH p = (a)-[:KNOWS*]-(b)\nFOREACH (n IN nodes(p) | SET n.visited = true)\n</code></pre></p> <p>Workaround: None (fundamentally incompatible with read-only SQL).</p>"},{"location":"04-limitations/#5-call-procedures-apoc-custom-procedures","title":"5. CALL Procedures (APOC, Custom Procedures)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: No concept of stored procedures in the transpilation model. APOC functions are Neo4j-specific.</p> <p>Example (unsupported): Cypher<pre><code>CALL apoc.periodic.iterate(...)\nCALL db.stats.retrieve(...)\n</code></pre></p> <p>Workaround: Use Databricks SQL UDFs or built-in functions where equivalent functionality exists.</p>"},{"location":"04-limitations/#6-pattern-comprehension-with-where","title":"6. Pattern Comprehension with WHERE","text":"<p>Status: \u26a0\ufe0f Partial Support (INFERRED)</p> <p>Supported: Simple pattern comprehension <code>[(n)-[:KNOWS]-&gt;(f) | f.name]</code> Not Supported: Pattern comprehension with complex <code>WHERE</code> clauses (INFERRED from lack of tests)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not be fully supported\nRETURN [(n)-[:KNOWS]-&gt;(f) WHERE f.age &gt; 30 | f.name] AS friends\n</code></pre></p> <p>Workaround: Use standard <code>MATCH</code> with <code>WITH</code> clause instead: Cypher<pre><code>MATCH (n)-[:KNOWS]-&gt;(f)\nWHERE f.age &gt; 30\nWITH n, COLLECT(f.name) AS friends\nRETURN n, friends\n</code></pre></p>"},{"location":"04-limitations/#7-multiple-match-patterns-in-single-clause","title":"7. Multiple MATCH Patterns in Single Clause","text":"<p>Status: \u26a0\ufe0f Limited Support</p> <p>Supported: Single pattern per <code>MATCH</code> clause Not Supported: Comma-separated patterns in one <code>MATCH</code> (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nMATCH (a)-[:KNOWS]-&gt;(b), (b)-[:LIKES]-&gt;(c)\nRETURN a, b, c\n\n-- \u2705 Workaround: use multiple MATCH clauses\nMATCH (a)-[:KNOWS]-&gt;(b)\nMATCH (b)-[:LIKES]-&gt;(c)\nRETURN a, b, c\n</code></pre></p>"},{"location":"04-limitations/#8-map-projections","title":"8. Map Projections","text":"<p>Status: \u26a0\ufe0f Partial Support</p> <p>Supported: Basic map literals <code>{key: value}</code> (test: test_35_map_literals.py) Not Supported: Map projections with property selectors <code>n{.id, .name}</code> (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nRETURN n{.id, .name, .age} AS person\n\n-- \u2705 Workaround: explicit map construction\nRETURN {id: n.id, name: n.name, age: n.age} AS person\n</code></pre></p>"},{"location":"04-limitations/#9-temporal-types-duration-temporal-arithmetic","title":"9. Temporal Types (Duration, Temporal Arithmetic)","text":"<p>Status: \u26a0\ufe0f Partial Support</p> <p>Supported: Basic datetime functions (test: test_34_datetime.py) Not Supported: <code>duration()</code>, temporal arithmetic (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nRETURN datetime() - duration({days: 7}) AS last_week\n\n-- \u2705 Workaround: use Databricks date functions\nRETURN date_sub(current_timestamp(), 7) AS last_week\n</code></pre></p>"},{"location":"04-limitations/#10-geospatial-functions","title":"10. Geospatial Functions","text":"<p>Status: \u274c Not Supported (INFERRED - no tests found)</p> <p>Reason: No OpenCypher geospatial support in transpiler.</p> <p>Example (unsupported): Cypher<pre><code>RETURN distance(point({x: 0, y: 0}), point({x: 3, y: 4})) AS dist\n</code></pre></p> <p>Workaround: Use Databricks Geospatial functions directly in SQL.</p>"},{"location":"04-limitations/#known-correctness-caveats","title":"Known Correctness Caveats","text":""},{"location":"04-limitations/#1-aggregation-after-aggregation","title":"1. Aggregation After Aggregation","text":"<p>Status: \u26a0\ufe0f Under Investigation</p> <p>Issue: Recent commits mention fixes for \"aggregation entity projection\" bugs (see commit: <code>155da2f</code>).</p> <p>Potentially Problematic Pattern: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nWITH p, COUNT(*) AS purchases\nWITH p.name, SUM(purchases) AS total  -- Aggregation after aggregation\nRETURN p.name, total\n</code></pre></p> <p>Related Test: tests/test_aggregation_entity_projection.py</p> <p>Workaround: If you encounter <code>UNRESOLVED_COLUMN</code> errors, try flattening the aggregation: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nWITH p.name, COUNT(*) AS total\nRETURN p.name, total\n</code></pre></p>"},{"location":"04-limitations/#2-multi-with-entity-continuation","title":"2. Multi-WITH Entity Continuation","text":"<p>Status: \u26a0\ufe0f Fixed Recently</p> <p>Issue: Recent tests added for \"multi-WITH entity continuation bug\" (commit: <code>7ec0add</code>).</p> <p>Affected Pattern: Cypher<pre><code>MATCH (n:Node)\nWITH n\nWITH n, n.property AS prop\nRETURN n, prop\n</code></pre></p> <p>Related Test: tests/test_multi_with_entity_continuation.py</p> <p>Status: Should be fixed in latest version. If you encounter issues, report with minimal repro case.</p>"},{"location":"04-limitations/#3-unresolved_column-errors-with-aggregations","title":"3. UNRESOLVED_COLUMN Errors with Aggregations","text":"<p>Status: \u26a0\ufe0f Edge Cases Remain (INFERRED)</p> <p>Context: Column resolution after aggregation boundaries is complex. Some edge cases may trigger Databricks <code>UNRESOLVED_COLUMN</code> errors.</p> <p>Symptom: Transpilation succeeds, but Databricks SQL execution fails with \"column not found\".</p> <p>Workaround: 1. Simplify <code>WITH</code> clauses (fewer intermediate steps) 2. Explicitly alias all aggregated columns 3. Avoid referencing entity properties after aggregation (use aliases instead)</p> <p>Related Commits: - <code>155da2f</code>: \"preserve full column names for entity projections after aggregation\" - <code>6388679</code>: \"use the AggregationBoundaryOperator itself\"</p>"},{"location":"04-limitations/#performance-caveats","title":"Performance Caveats","text":""},{"location":"04-limitations/#1-deep-variable-length-paths","title":"1. Deep Variable-Length Paths","text":"<p>Issue: <code>WITH RECURSIVE</code> CTEs for deep paths (e.g., <code>-[:TYPE*1..20]-&gt;</code>) can be slow or hit Spark recursion limits.</p> <p>Recommendation: - Keep max depth \u2264 10 for most queries - Use <code>LIMIT</code> to reduce result set size - Consider pre-computing transitive closures for frequent deep traversals</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May be slow\nMATCH (a)-[:FOLLOWS*1..20]-&gt;(b)\nRETURN DISTINCT b\n\n-- \u2705 Better: bounded depth with LIMIT\nMATCH (a)-[:FOLLOWS*1..5]-&gt;(b)\nRETURN DISTINCT b\nLIMIT 100\n</code></pre></p>"},{"location":"04-limitations/#2-cartesian-products","title":"2. Cartesian Products","text":"<p>Issue: Certain query patterns generate cartesian products (cross joins without predicates).</p> <p>Detection: Transpiler attempts to avoid cartesian products (test: test_10_relationship_join_no_cartesian.py)</p> <p>Recommendation: Always connect patterns with shared variables or predicates.</p> <p>Example: Cypher<pre><code>-- \u274c Cartesian product (no connection between patterns)\nMATCH (p:Person), (c:Company)\nRETURN p, c\n\n-- \u2705 Better: connected patterns\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nRETURN p, c\n</code></pre></p>"},{"location":"04-limitations/#3-large-collect-aggregations","title":"3. Large COLLECT Aggregations","text":"<p>Issue: <code>COLLECT()</code> aggregations create arrays in memory. Very large collections can cause OOM.</p> <p>Affected Pattern: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nRETURN p.name, COLLECT(product) AS all_products  -- Could be millions\n</code></pre></p> <p>Recommendation: - Use <code>LIMIT</code> within aggregation: <code>COLLECT(product)[0..100]</code> (if supported) - Filter before aggregation: <code>WHERE product.category = 'Electronics'</code> - Use <code>COUNT()</code> instead of <code>COLLECT()</code> when possible</p>"},{"location":"04-limitations/#4-edge-collection-in-recursive-ctes","title":"4. Edge Collection in Recursive CTEs","text":"<p>Issue: Path analyzer optimizes away edge collection when not needed (see Decision 8 in decision log).</p> <p>Impact: If you access <code>relationships(path)</code>, the transpiler collects edges in recursive CTE (expensive).</p> <p>Recommendation: Only use <code>relationships(path)</code> when necessary. Use <code>length(path)</code> for simple path length checks.</p>"},{"location":"04-limitations/#sql-dialect-constraints","title":"SQL Dialect Constraints","text":""},{"location":"04-limitations/#1-databricks-specific-functions","title":"1. Databricks-Specific Functions","text":"<p>The transpiler generates SQL using Databricks-specific functions:</p> Function Purpose Databricks-Specific? <code>array_contains()</code> Cycle detection Yes (some DBs use <code>ANY()</code>) <code>CONCAT(array1, array2)</code> Array concatenation Yes (PostgreSQL uses <code>||</code>) <code>COLLECT()</code> Aggregation to array No (SQL standard <code>ARRAY_AGG()</code>) <code>STRUCT()</code> Named tuple Yes (PostgreSQL uses <code>ROW()</code>) <p>Implication: Generated SQL is not portable to other databases without modification.</p> <p>Related Decision: Decision 4 in decision log</p>"},{"location":"04-limitations/#2-recursive-cte-limitations","title":"2. Recursive CTE Limitations","text":"<p>Databricks Specifics: - No <code>CYCLE</code> clause (must manually track visited nodes with array) - No <code>SEARCH</code> clause (must manually compute depth) - Performance: Recursive CTEs may not be optimized as well as native graph traversal</p>"},{"location":"04-limitations/#open-questions-todos","title":"Open Questions / TODOs","text":""},{"location":"04-limitations/#1-full-list-of-unsupported-functions","title":"1. Full List of Unsupported Functions","text":"<p>Status: Documentation incomplete</p> <p>TODO: Audit all OpenCypher functions and document which are unsupported.</p> <p>Known gaps: - \u274c Geospatial: <code>distance()</code>, <code>point()</code>, <code>withinBBox()</code> - \u274c Graph algorithms: <code>pageRank()</code>, <code>betweenness()</code>, etc. (Neo4j GDS) - \u274c APOC: All <code>apoc.*</code> functions - \u26a0\ufe0f Date/time: Partial support (needs audit)</p> <p>Where to add: This document (section above)</p>"},{"location":"04-limitations/#2-neo4j-compatibility-matrix","title":"2. Neo4j Compatibility Matrix","text":"<p>Status: No formal compatibility documentation</p> <p>TODO: Create a compatibility matrix showing which Neo4j features work vs. don't work.</p> <p>Format: Text Only<pre><code>| Feature              | Neo4j | gsql2rsql | Notes |\n|----------------------|-------|-----------|-------|\n| MATCH pattern        | \u2705    | \u2705        | Full  |\n| Variable-length path | \u2705    | \u2705        | Max depth 10 recommended |\n| shortestPath()       | \u2705    | \u274c        | Use workaround |\n| ...                  | ...   | ...       | ...   |\n</code></pre></p> <p>Where to add: New doc <code>docs/09-neo4j-compatibility.md</code></p>"},{"location":"04-limitations/#3-pyspark-test-coverage","title":"3. PySpark Test Coverage","text":"<p>Status: Not all tests have PySpark validation</p> <p>Context: Golden file tests (44 tests) validate SQL output, but not all are executed on PySpark.</p> <p>TODO: Identify which patterns are not covered by PySpark tests.</p> <p>Command: Compare test counts: Bash<pre><code># Golden file tests\nls tests/transpile_tests/*.py | wc -l  # 44+\n\n# PySpark examples\nls examples/*.yaml  # 3 files (credit, fraud, features)\n</code></pre></p> <p>Related Files: - tests/test_examples_with_pyspark.py - examples/</p>"},{"location":"04-limitations/#4-aggregation-semantics-edge-cases","title":"4. Aggregation Semantics Edge Cases","text":"<p>Status: Recent bug fixes suggest edge cases remain</p> <p>Context: Commits <code>155da2f</code>, <code>6388679</code>, <code>7ec0add</code> all relate to aggregation bugs.</p> <p>TODO: Document known patterns that may still have issues.</p> <p>Investigation needed: 1. Review all recent aggregation-related commits 2. Document specific patterns that were buggy 3. Add regression tests for each pattern</p> <p>Related Tests: - tests/test_aggregation_entity_projection.py - tests/test_multi_with_entity_continuation.py - tests/transpile_tests/test_41_column_projection_through_aggregation.py</p>"},{"location":"04-limitations/#5-subquery-optimizer-completeness","title":"5. Subquery Optimizer Completeness","text":"<p>Status: Conservative optimizer only handles simple cases</p> <p>TODO: Document all patterns that could be flattened but aren't (yet).</p> <p>Examples: - Multiple consecutive <code>SelectionOperator</code> (already handled) - <code>SelectionOperator</code> \u2192 <code>ProjectionOperator</code> (already handled) - <code>ProjectionOperator</code> \u2192 <code>SelectionOperator</code> (NOT handled - could be safe)</p> <p>Related File: src/gsql2rsql/planner/subquery_optimizer.py</p>"},{"location":"04-limitations/#6-error-message-quality","title":"6. Error Message Quality","text":"<p>Status: Resolver provides good suggestions, but other phases may not</p> <p>TODO: Audit error messages across all phases: - Parser: ANTLR errors (often cryptic) - Planner: Schema binding errors - Resolver: Column resolution errors (\u2705 good) - Renderer: SQL generation errors</p> <p>Where to improve: src/gsql2rsql/common/exceptions.py</p>"},{"location":"04-limitations/#recommended-workarounds-summary","title":"Recommended Workarounds Summary","text":"Limitation Workaround MERGE / CREATE / DELETE Use native Databricks SQL statements shortestPath() Use bounded BFS with <code>ORDER BY depth LIMIT 1</code> Deep paths (&gt;10 hops) Pre-compute transitive closure or use external graph DB Large COLLECT() Filter before aggregation, use LIMIT APOC functions Find equivalent Databricks SQL function or UDF Geospatial Use Databricks Geospatial functions directly Pattern comprehension with WHERE Use explicit <code>MATCH</code> + <code>WITH</code> + <code>COLLECT()</code> Cartesian products Always connect patterns with shared variables UNRESOLVED_COLUMN errors Simplify <code>WITH</code> clauses, use explicit aliases"},{"location":"04-limitations/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>05-testing-and-examples.md \u2014 How to add tests for new patterns</li> <li>07-developer-guide.md \u2014 How to extend support for new features</li> <li>03-decision-log.md \u2014 Why certain features are not supported</li> </ul>"},{"location":"05-testing-and-examples/","title":"Testing and Examples Guide","text":"<p>This document explains the test suite organization, how tests map to features, and how to add new tests or examples.</p>"},{"location":"05-testing-and-examples/#test-suite-overview","title":"Test Suite Overview","text":"<p>The test suite has 61 test files organized into categories:</p> Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                           # Pytest fixtures (shared test data)\n\u2502\n\u251c\u2500\u2500 transpile_tests/                      # Feature tests (44 golden file tests)\n\u2502   \u251c\u2500\u2500 test_01_simple_node_lookup.py\n\u2502   \u251c\u2500\u2500 test_06_single_hop_relationship.py\n\u2502   \u251c\u2500\u2500 test_11_aggregation_group_by.py\n\u2502   \u251c\u2500\u2500 test_16_variable_length_path.py  # (INFERRED - not found in listing)\n\u2502   \u2514\u2500\u2500 ... (44 total)\n\u2502\n\u251c\u2500\u2500 test_parser.py                        # Unit: AST construction\n\u251c\u2500\u2500 test_planner.py                       # Unit: Logical plan generation\n\u251c\u2500\u2500 test_symbol_table.py                  # Unit: Variable tracking\n\u251c\u2500\u2500 test_column_resolver.py               # Unit: Column resolution\n\u251c\u2500\u2500 test_renderer.py                      # Unit: SQL rendering\n\u2502\n\u251c\u2500\u2500 test_column_resolution_pipeline.py    # Integration: Full resolution flow\n\u251c\u2500\u2500 test_renderer_resolution_integration.py  # Integration: Resolution + rendering\n\u251c\u2500\u2500 test_schema_propagation.py            # Integration: Scope propagation\n\u2502\n\u251c\u2500\u2500 test_pyspark_basic.py                 # PySpark infrastructure tests\n\u251c\u2500\u2500 test_examples_with_pyspark.py         # PySpark validation on examples\n\u2502\n\u251c\u2500\u2500 test_aggregation_entity_projection.py # Regression: Aggregation bugs\n\u251c\u2500\u2500 test_multi_with_entity_continuation.py  # Regression: Multi-WITH bugs\n\u2514\u2500\u2500 ... (other regression tests)\n</code></pre>"},{"location":"05-testing-and-examples/#test-categories","title":"Test Categories","text":""},{"location":"05-testing-and-examples/#1-golden-file-tests-feature-validation","title":"1. Golden File Tests (Feature Validation)","text":"<p>Location: tests/transpile_tests/</p> <p>Count: 44 tests</p> <p>Purpose: Validate that specific Cypher patterns transpile to expected SQL.</p> <p>Pattern: 1. Define schema (nodes, edges) 2. Transpile Cypher query 3. Compare generated SQL to golden file in tests/output/expected/ 4. Structural assertions (has SELECT, has JOIN, etc.)</p> <p>Example: tests/transpile_tests/test_01_simple_node_lookup.py</p> Python<pre><code>class TestSimpleNodeLookup:\n    TEST_ID = \"01\"\n    TEST_NAME = \"simple_node_lookup\"\n\n    def test_golden_file_match(self) -&gt; None:\n        cypher = \"MATCH (p:Person) RETURN p\"\n        actual_sql = self._transpile(cypher)\n\n        expected_sql = load_expected_sql(self.TEST_ID, self.TEST_NAME)\n        assert_sql_equal(expected_sql, actual_sql, self.TEST_ID, self.TEST_NAME)\n</code></pre> <p>Golden File: tests/output/expected/01_simple_node_lookup.sql</p>"},{"location":"05-testing-and-examples/#2-unit-tests-component-isolation","title":"2. Unit Tests (Component Isolation)","text":"<p>Purpose: Test individual components (parser, planner, resolver, renderer) in isolation.</p> <p>Examples:</p> File Component Tested test_parser.py AST construction from Cypher test_planner.py Logical operator tree generation test_symbol_table.py Variable scoping and tracking test_column_resolver.py Column reference resolution test_renderer.py SQL string generation <p>Pattern: Python<pre><code>def test_parser_simple_match():\n    parser = OpenCypherParser()\n    ast = parser.parse(\"MATCH (n:Node) RETURN n\")\n\n    assert isinstance(ast, QueryNode)\n    assert len(ast.single_queries) == 1\n    # ... more assertions\n</code></pre></p>"},{"location":"05-testing-and-examples/#3-integration-tests-multi-phase","title":"3. Integration Tests (Multi-Phase)","text":"<p>Purpose: Test interactions between phases (e.g., resolution + rendering).</p> <p>Examples:</p> File Phases Tested test_column_resolution_pipeline.py Parser \u2192 Planner \u2192 Resolver test_renderer_resolution_integration.py Resolver \u2192 Renderer test_schema_propagation.py Planner scope propagation"},{"location":"05-testing-and-examples/#4-pyspark-validation-tests-end-to-end","title":"4. PySpark Validation Tests (End-to-End)","text":"<p>Purpose: Execute transpiled SQL on actual PySpark DataFrames to validate correctness.</p> <p>Files: - tests/test_pyspark_basic.py \u2014 Infrastructure tests (Spark session, data generation) - tests/test_examples_with_pyspark.py \u2014 Run examples from YAML files</p> <p>Pattern: 1. Load example YAML (schema + queries) 2. Generate sample data as PySpark DataFrames 3. Transpile Cypher query 4. Execute SQL on DataFrames 5. Validate results (non-empty, correct structure)</p> <p>Example Command: Bash<pre><code># Run all PySpark tests (slow)\nmake test-pyspark\n\n# Run quick subset (first 5 credit queries)\nmake test-pyspark-quick\n\n# Run specific domain\nmake test-pyspark-credit\n</code></pre></p>"},{"location":"05-testing-and-examples/#5-regression-tests-bug-prevention","title":"5. Regression Tests (Bug Prevention)","text":"<p>Purpose: Ensure fixed bugs don't reoccur.</p> <p>Examples: - test_aggregation_entity_projection.py \u2014 Fix for aggregation column name bug (commit <code>155da2f</code>) - test_multi_with_entity_continuation.py \u2014 Fix for multi-WITH entity bug (commit <code>7ec0add</code>) - test_symbol_duplication_fix.py \u2014 Fix for symbol table duplication (INFERRED) - test_error_position_tracking.py \u2014 Error message line/column accuracy (INFERRED)</p>"},{"location":"05-testing-and-examples/#test-file-naming-convention","title":"Test File Naming Convention","text":"<p>Golden File Tests: <code>test_{ID}_{feature_name}.py</code> - ID: 2-digit number (01-99) indicating test order/complexity - feature_name: Descriptive snake_case name</p> <p>Examples: - <code>test_01_simple_node_lookup.py</code> \u2014 Basic node matching - <code>test_06_single_hop_relationship.py</code> \u2014 Single-edge relationship - <code>test_11_aggregation_group_by.py</code> \u2014 GROUP BY aggregation - <code>test_25_unwind.py</code> \u2014 UNWIND clause - <code>test_36_path_functions.py</code> \u2014 Path manipulation functions</p> <p>Unit/Integration Tests: <code>test_{component}.py</code> - <code>test_parser.py</code> - <code>test_renderer.py</code> - <code>test_column_resolution_pipeline.py</code></p> <p>Regression Tests: <code>test_{bug_description}.py</code> - <code>test_aggregation_entity_projection.py</code> - <code>test_multi_with_entity_continuation.py</code></p>"},{"location":"05-testing-and-examples/#golden-file-structure","title":"Golden File Structure","text":""},{"location":"05-testing-and-examples/#directory-layout","title":"Directory Layout","text":"Text Only<pre><code>tests/output/\n\u251c\u2500\u2500 expected/                  # Golden reference SQL files\n\u2502   \u251c\u2500\u2500 01_simple_node_lookup.sql\n\u2502   \u251c\u2500\u2500 11_aggregation_group_by.sql\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 actual/                    # Generated SQL from latest test run\n\u2502   \u251c\u2500\u2500 01_simple_node_lookup.sql\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 diff/                      # Diff files when actual != expected\n\u2502   \u251c\u2500\u2500 01_simple_node_lookup.diff\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 pyspark/                   # PySpark execution results (INFERRED)\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"05-testing-and-examples/#updating-golden-files","title":"Updating Golden Files","text":"<p>When to Update: - Intentional change to SQL generation logic - Improved SQL output (more efficient, more readable) - Bug fix that changes correct output</p> <p>Command: Bash<pre><code># Generate and save new golden file for test 01\nmake dump-sql-save ID=01 NAME=simple_node_lookup\n\n# View diff between actual and expected\nmake dump-sql ID=01 NAME=simple_node_lookup --diff\n\n# View all diffs\nmake diff-all\n</code></pre></p> <p>Workflow: 1. Make code change 2. Run test: <code>pytest tests/transpile_tests/test_01_simple_node_lookup.py</code> 3. Test fails (SQL doesn't match golden file) 4. Review diff: <code>make dump-sql ID=01 NAME=simple_node_lookup --diff</code> 5. If correct, update golden file: <code>make dump-sql-save ID=01 NAME=simple_node_lookup</code> 6. Re-run test to confirm</p>"},{"location":"05-testing-and-examples/#example-collections-yaml","title":"Example Collections (YAML)","text":""},{"location":"05-testing-and-examples/#overview","title":"Overview","text":"<p>Example collections are YAML files containing: - Schema definition (nodes, edges, properties) - Multiple queries with descriptions - Domain-specific patterns (credit, fraud, features)</p> <p>Location: examples/</p> <p>Files: - credit_queries.yaml \u2014 Credit risk analysis (customer, loans, transactions) - fraud_queries.yaml \u2014 Fraud detection patterns - features_queries.yaml \u2014 Feature showcase (all supported Cypher constructs)</p>"},{"location":"05-testing-and-examples/#yaml-format","title":"YAML Format","text":"YAML<pre><code># Schema definition\nschema:\n  nodes:\n    - name: Customer\n      tableName: catalog.credit.Customer\n      idProperty: { name: id, type: int }\n      properties:\n        - { name: name, type: string }\n        - { name: status, type: string }\n\n  edges:\n    - name: HAS_ACCOUNT\n      sourceNode: Customer\n      sinkNode: Account\n      tableName: catalog.credit.CustomerAccount\n      sourceIdProperty: { name: customer_id, type: int }\n      sinkIdProperty: { name: account_id, type: int }\n\n# Example queries\nexamples:\n  - description: \"Find high-risk customers with multiple late payments\"\n    application: \"Credit Risk Assessment\"\n    query: |\n      MATCH (c:Customer)-[:HAS_LOAN]-&gt;(loan:Loan)-[:PAYMENT]-&gt;(p:Payment)\n      WHERE p.on_time = false\n      WITH c, COUNT(p) AS late_payments\n      WHERE late_payments &gt;= 3\n      RETURN c.name, late_payments\n      ORDER BY late_payments DESC\n    notes: \"Identifies customers with 3+ late payments for risk modeling\"\n\n  - description: \"Calculate customer credit utilization\"\n    application: \"Credit Monitoring\"\n    query: |\n      MATCH (c:Customer)-[:HAS_CARD]-&gt;(card:CreditCard)\n      WITH c, SUM(card.balance) AS total_balance, SUM(card.credit_limit) AS total_limit\n      RETURN c.name, total_balance, total_limit,\n             (total_balance / total_limit) AS utilization_ratio\n      WHERE utilization_ratio &gt; 0.8\n    notes: \"High utilization (&gt;80%) indicates financial stress\"\n</code></pre>"},{"location":"05-testing-and-examples/#using-examples","title":"Using Examples","text":"<p>Interactive TUI: Bash<pre><code># Browse examples with live transpilation\nuv run gsql2rsql tui --examples examples/credit_queries.yaml\n</code></pre></p> <p>PySpark Validation: Bash<pre><code># Run all examples on PySpark\nmake test-pyspark-examples\n\n# Run specific domain\nmake test-pyspark-credit\nmake test-pyspark-fraud\nmake test-pyspark-features\n</code></pre></p> <p>Command-Line: Bash<pre><code># Manually transpile an example (extract query from YAML)\ncat examples/credit_queries.yaml | grep \"query:\" | head -1 | \\\n  uv run gsql2rsql transpile --schema examples/credit_queries.yaml\n</code></pre></p>"},{"location":"05-testing-and-examples/#adding-a-new-golden-file-test","title":"Adding a New Golden File Test","text":""},{"location":"05-testing-and-examples/#checklist","title":"Checklist","text":"<ul> <li> 1. Determine test ID (next available number, e.g., 45)</li> <li> 2. Choose descriptive name (e.g., <code>optional_match_chained</code>)</li> <li> 3. Create test file: <code>tests/transpile_tests/test_45_optional_match_chained.py</code></li> <li> 4. Define schema (reuse fixtures from <code>conftest.py</code> if possible)</li> <li> 5. Write Cypher query</li> <li> 6. Implement <code>_transpile()</code> helper</li> <li> 7. Add <code>test_golden_file_match()</code> method</li> <li> 8. Add structural assertion tests (optional but recommended)</li> <li> 9. Run test (will fail - no golden file yet)</li> <li> 10. Generate golden file: <code>make dump-sql-save ID=45 NAME=optional_match_chained</code></li> <li> 11. Review generated SQL for correctness</li> <li> 12. Re-run test (should pass)</li> <li> 13. Add documentation (optional): <code>tests/docs/45_optional_match_chained.md</code></li> </ul>"},{"location":"05-testing-and-examples/#template","title":"Template","text":"Python<pre><code>\"\"\"Test 45: Chained OPTIONAL MATCH patterns.\n\nValidates that multiple OPTIONAL MATCH clauses correctly generate LEFT JOINs\nwith proper null handling.\n\"\"\"\n\nfrom gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom gsql2rsql.common.schema import (\n    SimpleGraphSchemaProvider,\n    NodeSchema,\n    EdgeSchema,\n    EntityProperty,\n)\nfrom gsql2rsql.renderer.schema_provider import (\n    SimpleSQLSchemaProvider,\n    SQLTableDescriptor,\n)\n\nfrom tests.utils.sql_test_utils import (\n    assert_sql_equal,\n    load_expected_sql,\n)\nfrom tests.utils.sql_assertions import (\n    assert_has_left_join,\n    assert_has_coalesce,\n)\n\n\nclass TestOptionalMatchChained:\n    \"\"\"Test chained OPTIONAL MATCH with multiple LEFT JOINs.\"\"\"\n\n    TEST_ID = \"45\"\n    TEST_NAME = \"optional_match_chained\"\n\n    def setup_method(self) -&gt; None:\n        \"\"\"Set up test fixtures.\"\"\"\n        # Define schema (reuse fixture or create inline)\n        self.graph_schema = SimpleGraphSchemaProvider()\n        # ... add nodes and edges\n\n        self.sql_schema = SimpleSQLSchemaProvider()\n        # ... add table descriptors\n\n    def _transpile(self, cypher: str) -&gt; str:\n        \"\"\"Helper to transpile a Cypher query.\"\"\"\n        parser = OpenCypherParser()\n        ast = parser.parse(cypher)\n        plan = LogicalPlan.process_query_tree(ast, self.graph_schema)\n        plan.resolve(original_query=cypher)\n        renderer = SQLRenderer(db_schema_provider=self.sql_schema)\n        return renderer.render_plan(plan)\n\n    def test_golden_file_match(self) -&gt; None:\n        \"\"\"Test that transpiled SQL matches golden file.\"\"\"\n        cypher = \"\"\"\n            MATCH (p:Person)\n            OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f:Person)\n            OPTIONAL MATCH (f)-[:LIKES]-&gt;(h:Hobby)\n            RETURN p.name, f.name, h.name\n        \"\"\"\n        actual_sql = self._transpile(cypher)\n\n        expected_sql = load_expected_sql(self.TEST_ID, self.TEST_NAME)\n        if expected_sql is None:\n            from tests.utils.sql_test_utils import EXPECTED_DIR\n            raise AssertionError(\n                f\"No golden file found. Create: \"\n                f\"{EXPECTED_DIR}/{self.TEST_ID}_{self.TEST_NAME}.sql\"\n            )\n\n        assert_sql_equal(\n            expected_sql,\n            actual_sql,\n            self.TEST_ID,\n            self.TEST_NAME,\n        )\n\n    def test_structural_has_left_joins(self) -&gt; None:\n        \"\"\"Test that SQL uses LEFT JOIN for OPTIONAL MATCH.\"\"\"\n        cypher = \"\"\"\n            MATCH (p:Person)\n            OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f:Person)\n            RETURN p.name, f.name\n        \"\"\"\n        sql = self._transpile(cypher)\n\n        assert_has_left_join(sql)\n\n    def test_structural_has_coalesce(self) -&gt; None:\n        \"\"\"Test that optional properties use COALESCE for null handling.\"\"\"\n        cypher = \"\"\"\n            MATCH (p:Person)\n            OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f:Person)\n            RETURN p.name, COALESCE(f.name, 'Unknown') AS friend_name\n        \"\"\"\n        sql = self._transpile(cypher)\n\n        assert_has_coalesce(sql)\n</code></pre>"},{"location":"05-testing-and-examples/#running-your-new-test","title":"Running Your New Test","text":"Bash<pre><code># Run test (will fail initially)\npytest tests/transpile_tests/test_45_optional_match_chained.py -v\n\n# Generate golden file\nmake dump-sql-save ID=45 NAME=optional_match_chained\n\n# Re-run test (should pass)\npytest tests/transpile_tests/test_45_optional_match_chained.py -v\n\n# Run all golden file tests\nmake test-transpile\n</code></pre>"},{"location":"05-testing-and-examples/#adding-a-new-pyspark-example","title":"Adding a New PySpark Example","text":""},{"location":"05-testing-and-examples/#checklist_1","title":"Checklist","text":"<ul> <li> 1. Choose domain (credit, fraud, features, or create new)</li> <li> 2. Add query to appropriate YAML file (e.g., <code>examples/credit_queries.yaml</code>)</li> <li> 3. Ensure schema is defined in YAML (or reuse existing)</li> <li> 4. Add <code>description</code>, <code>application</code>, <code>query</code>, and <code>notes</code> fields</li> <li> 5. Run PySpark validation: <code>make test-pyspark-credit</code></li> <li> 6. Fix any errors (schema issues, unsupported features, etc.)</li> <li> 7. Document in YAML <code>notes</code> field</li> </ul>"},{"location":"05-testing-and-examples/#yaml-template","title":"YAML Template","text":"YAML<pre><code>examples:\n  - description: \"Short one-line description of what query does\"\n    application: \"Use case category (e.g., Fraud Detection, Risk Assessment)\"\n    query: |\n      MATCH (n:Node)-[:EDGE]-&gt;(m:Node)\n      WHERE n.property &gt; 100\n      RETURN n.name, COUNT(m) AS count\n      ORDER BY count DESC\n      LIMIT 10\n    notes: \"Additional context: why this pattern is useful, edge cases, performance notes\"\n</code></pre>"},{"location":"05-testing-and-examples/#example-adding-a-fraud-detection-query","title":"Example: Adding a Fraud Detection Query","text":"<p>File: examples/fraud_queries.yaml</p> YAML<pre><code>examples:\n  - description: \"Detect circular money transfers (potential laundering)\"\n    application: \"Anti-Money Laundering (AML)\"\n    query: |\n      MATCH path = (a:Account)-[:TRANSFER*3..5]-&gt;(a)\n      WHERE ALL(t IN relationships(path) WHERE t.amount &gt; 10000)\n      RETURN a.id, length(path) AS cycle_length,\n             [t IN relationships(path) | t.amount] AS transfer_amounts\n    notes: |\n      Finds cycles of 3-5 transfers that return to the same account, all with\n      amounts &gt;$10k. Suspicious pattern for money laundering detection.\n</code></pre>"},{"location":"05-testing-and-examples/#running-pyspark-examples","title":"Running PySpark Examples","text":"Bash<pre><code># Run all fraud queries\nmake test-pyspark-fraud\n\n# Run with verbose output (see DataFrames)\nmake test-pyspark-verbose\n\n# Run with timeout (prevent hanging)\nmake test-pyspark-timeout\n</code></pre>"},{"location":"05-testing-and-examples/#test-utilities-reference","title":"Test Utilities Reference","text":""},{"location":"05-testing-and-examples/#sql-assertion-helpers","title":"SQL Assertion Helpers","text":"<p>Location: tests/utils/sql_assertions.py</p> Python<pre><code>from tests.utils.sql_assertions import (\n    assert_has_select,        # Check for SELECT clause\n    assert_has_from_table,    # Check for specific table reference\n    assert_has_join,          # Check for any JOIN\n    assert_has_left_join,     # Check for LEFT JOIN (OPTIONAL MATCH)\n    assert_has_where,         # Check for WHERE clause\n    assert_has_group_by,      # Check for GROUP BY\n    assert_has_recursive_cte, # Check for WITH RECURSIVE\n    assert_has_coalesce,      # Check for COALESCE (null handling)\n    assert_no_join,           # Verify no JOIN present\n    SQLStructure,             # Parse SQL structure\n)\n</code></pre>"},{"location":"05-testing-and-examples/#sql-test-utilities","title":"SQL Test Utilities","text":"<p>Location: tests/utils/sql_test_utils.py</p> Python<pre><code>from tests.utils.sql_test_utils import (\n    load_expected_sql,        # Load golden file\n    assert_sql_equal,         # Compare with normalization\n    EXPECTED_DIR,             # Path to expected/ directory\n    ACTUAL_DIR,               # Path to actual/ directory\n    DIFF_DIR,                 # Path to diff/ directory\n)\n</code></pre>"},{"location":"05-testing-and-examples/#fixtures-conftestpy","title":"Fixtures (conftest.py)","text":"<p>Location: tests/conftest.py</p> Python<pre><code>@pytest.fixture\ndef movie_graph_schema():\n    \"\"\"Standard test schema: Person, Movie, ACTED_IN, DIRECTED.\"\"\"\n    # Returns SimpleGraphSchemaProvider with movie domain\n\n@pytest.fixture\ndef simple_sql_schema():\n    \"\"\"SQL schema provider for movie graph.\"\"\"\n    # Returns SimpleSQLSchemaProvider\n\n# ... more fixtures\n</code></pre>"},{"location":"05-testing-and-examples/#running-tests","title":"Running Tests","text":""},{"location":"05-testing-and-examples/#fast-tests-no-pyspark","title":"Fast Tests (No PySpark)","text":"Bash<pre><code># All tests except PySpark (recommended for development)\nmake test-no-pyspark\n\n# Specific golden file test\npytest tests/transpile_tests/test_01_simple_node_lookup.py -v\n\n# All golden file tests\nmake test-transpile\n\n# Unit tests only\npytest tests/test_parser.py tests/test_planner.py -v\n</code></pre>"},{"location":"05-testing-and-examples/#pyspark-tests-slow","title":"PySpark Tests (Slow)","text":"Bash<pre><code># All PySpark tests\nmake test-pyspark\n\n# Quick subset (first 5 examples)\nmake test-pyspark-quick\n\n# Specific domain\nmake test-pyspark-credit\nmake test-pyspark-fraud\nmake test-pyspark-features\n\n# Basic infrastructure tests only\nmake test-pyspark-basic\n</code></pre>"},{"location":"05-testing-and-examples/#coverage","title":"Coverage","text":"Bash<pre><code># Run with coverage report\nmake test-cov\n\n# View HTML report (generates htmlcov/)\nopen htmlcov/index.html\n</code></pre>"},{"location":"05-testing-and-examples/#debugging","title":"Debugging","text":"Bash<pre><code># Verbose output with full tracebacks\nmake test-verbose\n\n# Run single test with pdb\npytest tests/transpile_tests/test_01_simple_node_lookup.py::TestSimpleNodeLookup::test_golden_file_match -v --pdb\n\n# Show SQL diffs\nmake diff-all\n</code></pre>"},{"location":"05-testing-and-examples/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>07-developer-guide.md \u2014 How to extend the transpiler</li> <li>tests/ \u2014 Browse existing tests for patterns</li> <li>Makefile \u2014 All test commands</li> <li>tests/utils/ \u2014 Test utilities and helpers</li> </ul>"},{"location":"06-contributing/","title":"Contributing Guide","text":"<p>This document combines the architectural boundaries from CONTRIBUTING.md with practical developer workflow guidance.</p>"},{"location":"06-contributing/#quick-start-for-contributors","title":"Quick Start for Contributors","text":""},{"location":"06-contributing/#1-set-up-development-environment","title":"1. Set Up Development Environment","text":"Bash<pre><code># Clone repository (INFERRED - update with actual URL)\ngit clone https://github.com/your-org/cyper2dsql.git\ncd cyper2dsql/python\n\n# Create virtual environment with uv\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies (dev mode)\nuv sync --extra dev\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"06-contributing/#2-verify-installation","title":"2. Verify Installation","text":"Bash<pre><code># Run fast test suite\nmake test-no-pyspark\n\n# Run linter and type checker\nmake check\n\n# Transpile example query\necho \"MATCH (n:Person) RETURN n\" | uv run gsql2rsql transpile -s examples/schema.json\n</code></pre>"},{"location":"06-contributing/#3-make-changes","title":"3. Make Changes","text":"<p>Before coding: 1. Read architectural boundaries (critical!) 2. Identify which phase your change affects (parser, planner, resolver, or renderer) 3. Add tests first (TDD recommended)</p> <p>Development loop: 1. Make changes to <code>src/</code> 2. Run <code>make test-no-pyspark</code> (fast feedback) 3. Run <code>make check</code> (lint + typecheck) 4. Add/update golden files if SQL output changes 5. Run full test suite: <code>make test</code></p>"},{"location":"06-contributing/#4-submit-changes","title":"4. Submit Changes","text":"Bash<pre><code># Format code\nmake format\n\n# Run all checks\nmake check\n\n# Run full test suite (including PySpark)\nmake test\n\n# Commit with descriptive message\ngit add .\ngit commit -m \"feat: add support for map projection in RETURN clause\"\n\n# Push and create PR\ngit push origin feature/map-projection\n</code></pre>"},{"location":"06-contributing/#architectural-separation-of-concerns","title":"Architectural Separation of Concerns","text":"<p>\u26a0\ufe0f CRITICAL: Read this before making any changes!</p> <p>The transpiler enforces strict separation of concerns across 4 phases. Violating these boundaries will cause architectural degradation and hard-to-debug issues.</p> <p>This section is derived from CONTRIBUTING.md.</p>"},{"location":"06-contributing/#phase-1-parser-opencypherparser","title":"Phase 1: Parser (OpenCypherParser)","text":"<p>Location: src/gsql2rsql/parser/</p> <p>Input: Cypher query string Output: Abstract Syntax Tree (AST)</p> <p>Responsibility: Lexical/syntactic analysis only</p> <p>Does NOT: - \u274c Validate semantics - \u274c Resolve references - \u274c Access schema - \u274c Perform type checking - \u274c Validate property names</p> <p>Rules: - Parser MUST NOT import from <code>planner/</code>, <code>renderer/</code>, or <code>common/schema.py</code> - Parser MUST NOT call graph schema provider - Parser MUST only validate syntax (grammar rules)</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding a new AST node type for a new Cypher construct\nclass QueryExpressionPatternComprehension(QueryExpression):\n    pattern: QueryPattern\n    where_clause: Optional[WhereClause]\n    projection: QueryExpression\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Parser accessing schema\nclass CypherVisitor:\n    def visitPropertyExpression(self, ctx):\n        entity_name = self._get_entity_name(ctx)\n        # \u274c WRONG: Don't validate property existence here\n        if not self.schema.has_property(entity_name, property_name):\n            raise Exception(\"Property not found\")\n</code></pre></p>"},{"location":"06-contributing/#phase-2-planning-logicalplan","title":"Phase 2: Planning (LogicalPlan)","text":"<p>Location: src/gsql2rsql/planner/logical_plan.py</p> <p>Input: AST + GraphSchema Output: Logical operator tree + SymbolTable</p> <p>Responsibility: - \u2705 Convert AST to logical operators - \u2705 Build symbol table (variable definitions, scopes) - \u2705 Track entity/value types - \u2705 Handle WITH boundaries, MATCH patterns, aggregations</p> <p>Does NOT: - \u274c Resolve column references - \u274c Validate property access - \u274c Generate SQL - \u274c Query database schema</p> <p>Rules: - Planner CAN import from <code>parser/</code> (uses AST) - Planner CAN import from <code>common/schema.py</code> (uses GraphSchema) - Planner MUST NOT import from <code>renderer/</code> - Planner MUST NOT perform column resolution (that's Phase 4)</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding a new logical operator\nclass WindowOperator(LogicalOperator):\n    \"\"\"Represents a window function (OVER clause).\"\"\"\n    partition_by: list[str]\n    order_by: list[OrderByItem]\n    window_function: WindowFunction\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Planner resolving column references\nclass LogicalPlan:\n    def _process_projection(self, projection: ProjectionItem):\n        # \u274c WRONG: Don't resolve column refs during planning\n        resolved_ref = self._resolve_column_reference(projection.expression)\n        # Column resolution belongs in Phase 4 (Resolver)\n</code></pre></p>"},{"location":"06-contributing/#phase-3-optimization-subqueryflatteningoptimizer","title":"Phase 3: Optimization (SubqueryFlatteningOptimizer)","text":"<p>Location: src/gsql2rsql/planner/subquery_optimizer.py</p> <p>Input: LogicalPlan Output: Optimized LogicalPlan (modified in-place)</p> <p>Responsibility: - \u2705 Apply conservative transformations - \u2705 Only flatten proven-safe patterns</p> <p>Does NOT: - \u274c Change query semantics - \u274c Resolve columns - \u274c Generate SQL</p> <p>Rules: - Optimizer MUST be conservative (safety first) - Optimizer MUST NOT flatten patterns that could change semantics - Optimizer CAN be disabled by user (<code>--no-optimize</code>)</p>"},{"location":"06-contributing/#phase-4-resolution-columnresolver","title":"Phase 4: Resolution (ColumnResolver)","text":"<p>Location: src/gsql2rsql/planner/column_resolver.py</p> <p>Input: LogicalPlan + AST + GraphSchema Output: ResolutionResult (resolved column refs, expressions, projections)</p> <p>Responsibility: - \u2705 Validate ALL column references against symbol table - \u2705 Query schema for entity properties - \u2705 Detect entity returns vs property returns - \u2705 Track property availability across boundaries - \u2705 Build ResolvedColumnRef/ResolvedExpression structures</p> <p>Does NOT: - \u274c Generate SQL - \u274c Modify logical plan structure - \u274c Perform optimizations</p> <p>Rules: - Resolver CAN import from <code>parser/</code>, <code>planner/</code>, <code>common/</code> - Resolver MUST NOT import from <code>renderer/</code> - Resolver MUST validate ALL column refs before SQL generation - Resolver MUST provide rich error messages with suggestions</p> <p>Example Valid Change: Python<pre><code># \u2705 Improving error messages with better suggestions\nclass ColumnResolver:\n    def _suggest_similar_columns(self, invalid_name: str, available: list[str]) -&gt; list[str]:\n        # Use Levenshtein distance to suggest typo fixes\n        distances = [(name, levenshtein(invalid_name, name)) for name in available]\n        return [name for name, dist in sorted(distances, key=lambda x: x[1]) if dist &lt;= 2]\n</code></pre></p>"},{"location":"06-contributing/#phase-5-rendering-sqlrenderer","title":"Phase 5: Rendering (SQLRenderer)","text":"<p>Location: src/gsql2rsql/renderer/sql_renderer.py</p> <p>Input: LogicalPlan + ResolutionResult + GraphSchema Output: SQL string</p> <p>Responsibility: - \u2705 Generate SQL from logical plan - \u2705 Use pre-resolved column references - \u2705 Handle SQL dialect specifics</p> <p>Does NOT: - \u274c Resolve columns - \u274c Validate references - \u274c Make semantic decisions</p> <p>Rules: - Renderer CAN import from all phases (uses everything) - Renderer MUST use <code>ResolutionResult</code> for all column refs - Renderer MUST NOT resolve columns itself - Renderer MUST NOT perform semantic validation</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding support for new SQL function\nclass SQLRenderer:\n    def _render_function(self, func: Function, args: list[str]) -&gt; str:\n        if func == Function.RTRIM:\n            # New function mapping\n            return f\"RTRIM({', '.join(args)})\"\n        # ... existing functions\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Renderer resolving columns\nclass SQLRenderer:\n    def _render_property_access(self, entity: str, property: str) -&gt; str:\n        # \u274c WRONG: Don't resolve property here\n        if not self.schema.has_property(entity, property):\n            raise Exception(\"Property not found\")\n        # Resolution should already be done in Phase 4\n</code></pre></p>"},{"location":"06-contributing/#branch-and-pr-workflow","title":"Branch and PR Workflow","text":""},{"location":"06-contributing/#branch-naming","title":"Branch Naming","text":"<p>Use descriptive branch names with prefixes:</p> <ul> <li><code>feat/</code> \u2014 New feature (e.g., <code>feat/add-shortest-path</code>)</li> <li><code>fix/</code> \u2014 Bug fix (e.g., <code>fix/aggregation-column-names</code>)</li> <li><code>refactor/</code> \u2014 Code refactoring (e.g., <code>refactor/simplify-resolver</code>)</li> <li><code>test/</code> \u2014 Test additions (e.g., <code>test/add-optional-match-tests</code>)</li> <li><code>docs/</code> \u2014 Documentation (e.g., <code>docs/update-quickstart</code>)</li> <li><code>chore/</code> \u2014 Maintenance (e.g., <code>chore/update-dependencies</code>)</li> </ul>"},{"location":"06-contributing/#commit-message-format","title":"Commit Message Format","text":"<p>Use conventional commits format:</p> Text Only<pre><code>&lt;type&gt;: &lt;short description&gt;\n\n&lt;optional longer description&gt;\n\n&lt;optional footer&gt;\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>refactor</code>, <code>test</code>, <code>docs</code>, <code>chore</code>, <code>perf</code>, <code>style</code></p> <p>Examples: Text Only<pre><code>feat: add support for CASE expression in RETURN clause\n\nImplements CASE WHEN ... THEN ... ELSE ... END syntax in projections.\nAdds new AST node QueryExpressionCaseExpression and rendering logic.\n\nCloses #42\n</code></pre></p> Text Only<pre><code>fix: preserve column names after aggregation boundaries\n\nColumn names were being lost when entity properties were projected after\nGROUP BY. Now uses full qualified names (_gsql2rsql_entity_property).\n\nFixes #51\n</code></pre>"},{"location":"06-contributing/#pr-checklist","title":"PR Checklist","text":"<p>Before submitting a PR:</p> <ul> <li> All tests pass: <code>make test</code></li> <li> Code is formatted: <code>make format</code></li> <li> No lint errors: <code>make lint</code></li> <li> Type checking passes: <code>make typecheck</code></li> <li> Added tests for new features</li> <li> Updated golden files if SQL output changed</li> <li> Added/updated documentation</li> <li> Commit messages follow conventional format</li> <li> PR description explains the change and motivation</li> </ul>"},{"location":"06-contributing/#code-style","title":"Code Style","text":""},{"location":"06-contributing/#python-style-guide","title":"Python Style Guide","text":"<p>Formatter: Ruff (configured in pyproject.toml)</p> <p>Line length: 100 characters</p> <p>Imports: Sorted with isort (part of Ruff)</p> <p>Type hints: Required for all functions (strict mypy)</p> <p>Docstrings: Google style (recommended, not enforced)</p>"},{"location":"06-contributing/#running-formatters-and-linters","title":"Running Formatters and Linters","text":"Bash<pre><code># Auto-format code\nmake format\n\n# Check formatting (CI mode)\nmake format-check\n\n# Run linter\nmake lint\n\n# Auto-fix linting issues\nmake lint-fix\n\n# Run type checker\nmake typecheck\n\n# Run all checks\nmake check\n</code></pre>"},{"location":"06-contributing/#type-hints","title":"Type Hints","text":"<p>Required: All function signatures must have type hints</p> <p>Example: Python<pre><code># \u2705 Good\ndef render_expression(\n    self,\n    expr: QueryExpression,\n    context: RenderContext\n) -&gt; str:\n    \"\"\"Render an expression to SQL string.\"\"\"\n    ...\n\n# \u274c Bad (missing type hints)\ndef render_expression(self, expr, context):\n    ...\n</code></pre></p> <p>Mypy Configuration: See pyproject.toml <code>[tool.mypy]</code> section</p> <ul> <li><code>strict = true</code> (all strict checks enabled)</li> <li><code>disallow_untyped_defs = true</code> (no untyped functions)</li> <li><code>warn_return_any = true</code> (warn on <code>Any</code> returns)</li> </ul>"},{"location":"06-contributing/#naming-conventions","title":"Naming Conventions","text":"Item Convention Example Classes PascalCase <code>LogicalOperator</code>, <code>ColumnResolver</code> Functions snake_case <code>render_plan()</code>, <code>resolve_column()</code> Constants UPPER_SNAKE_CASE <code>MAX_DEPTH</code>, <code>DEFAULT_TIMEOUT</code> Private methods <code>_snake_case</code> <code>_render_helper()</code> Type aliases PascalCase <code>EntityMap</code>, <code>ColumnMapping</code>"},{"location":"06-contributing/#file-organization","title":"File Organization","text":"<p>Within each module file: 1. Module docstring 2. Imports (stdlib, third-party, local) 3. Constants 4. Type aliases 5. Helper functions 6. Main classes 7. Module-level functions (if any)</p> <p>Example: Python<pre><code>\"\"\"Module for SQL rendering logic.\n\nThis module contains the SQLRenderer class which converts logical plans\nto Databricks Spark SQL.\n\"\"\"\n\n# Standard library\nfrom typing import Any, Optional\n\n# Third-party\nfrom antlr4 import InputStream\n\n# Local\nfrom gsql2rsql.planner.operators import LogicalOperator\nfrom gsql2rsql.planner.column_ref import ResolvedColumnRef\n\n# Constants\nMAX_RECURSION_DEPTH = 100\nDEFAULT_INDENT = \"  \"\n\n# Type aliases\nOperatorMap = dict[str, LogicalOperator]\n\n# Classes\nclass SQLRenderer:\n    ...\n</code></pre></p>"},{"location":"06-contributing/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"06-contributing/#adding-a-new-cypher-feature","title":"Adding a New Cypher Feature","text":"<p>Example: Add support for <code>range()</code> function</p> <ol> <li> <p>Phase 1 (Parser): Add to grammar or AST if needed    Python<pre><code># If new function, add to Function enum in operators.py\nclass Function(Enum):\n    RANGE = \"range\"  # Generate sequence of integers\n</code></pre></p> </li> <li> <p>Phase 2 (Planner): Handle in operator construction (if needed)    Python<pre><code># Usually functions are just expressions, no special operator needed\n</code></pre></p> </li> <li> <p>Phase 4 (Resolver): Type checking (if needed)    Python<pre><code># Add type evaluation rule\ndef _evaluate_function_type(self, func: Function, args: list[DataType]) -&gt; DataType:\n    if func == Function.RANGE:\n        return DataType.LIST_INT\n</code></pre></p> </li> <li> <p>Phase 5 (Renderer): Add SQL generation    Python<pre><code>class SQLRenderer:\n    def _render_function(self, func: Function, args: list[str]) -&gt; str:\n        if func == Function.RANGE:\n            # Databricks: sequence(start, stop, step)\n            return f\"sequence({args[0]}, {args[1]})\"\n</code></pre></p> </li> <li> <p>Add Tests: Golden file test + unit tests    Bash<pre><code># Create test_46_range_function.py\n# Generate golden file\nmake dump-sql-save ID=46 NAME=range_function\n</code></pre></p> </li> </ol>"},{"location":"06-contributing/#fixing-a-bug","title":"Fixing a Bug","text":"<p>Example: Fix incorrect null handling in OPTIONAL MATCH</p> <ol> <li> <p>Write failing test first (TDD)    Python<pre><code># tests/test_optional_match_null_bug.py\ndef test_optional_match_null_handling():\n    cypher = \"MATCH (p:Person) OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f) RETURN p.name, f.name\"\n    sql = transpile(cypher)\n    # Should use COALESCE for f.name\n    assert \"COALESCE\" in sql\n</code></pre></p> </li> <li> <p>Run test (should fail)    Bash<pre><code>pytest tests/test_optional_match_null_bug.py -v\n</code></pre></p> </li> <li> <p>Identify the phase where the bug is (use <code>--explain-scopes</code> for debugging)    Bash<pre><code>echo \"MATCH (p) OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f) RETURN p.name, f.name\" | \\\n  uv run gsql2rsql transpile -s examples/schema.json --explain-scopes\n</code></pre></p> </li> <li> <p>Fix the bug in the appropriate phase    Python<pre><code># src/gsql2rsql/renderer/sql_renderer.py\ndef _render_optional_property(self, ref: ResolvedColumnRef) -&gt; str:\n    if ref.is_from_optional_match:\n        # \u2705 Add COALESCE for null handling\n        return f\"COALESCE({ref.sql_column_name}, NULL)\"\n    return ref.sql_column_name\n</code></pre></p> </li> <li> <p>Run tests (should pass now)    Bash<pre><code>pytest tests/test_optional_match_null_bug.py -v\nmake test-no-pyspark\n</code></pre></p> </li> <li> <p>Update golden files if SQL output changed    Bash<pre><code>make diff-all  # Review changes\nmake dump-sql-save ID=09 NAME=optional_match  # Update if correct\n</code></pre></p> </li> </ol>"},{"location":"06-contributing/#debugging-transpilation-issues","title":"Debugging Transpilation Issues","text":"<p>Step 1: Isolate the query Bash<pre><code># Save problematic query to file\necho \"MATCH (n:Node) WHERE n.prop &gt; 10 RETURN n\" &gt; debug_query.cypher\n</code></pre></p> <p>Step 2: Inspect AST Bash<pre><code>uv run gsql2rsql parse -i debug_query.cypher\n</code></pre></p> <p>Step 3: Inspect logical plan Python<pre><code># In Python REPL or script\nfrom gsql2rsql import OpenCypherParser, LogicalPlan\nfrom gsql2rsql.common.schema import SimpleGraphSchemaProvider\n\nparser = OpenCypherParser()\nast = parser.parse(open(\"debug_query.cypher\").read())\n\nschema = SimpleGraphSchemaProvider()\n# ... add schema\n\nplan = LogicalPlan.process_query_tree(ast, schema)\nprint(plan.dump_graph())  # Visualize operator tree\n</code></pre></p> <p>Step 4: Check scopes Bash<pre><code>uv run gsql2rsql transpile -s examples/schema.json -i debug_query.cypher --explain-scopes\n</code></pre></p> <p>Step 5: Enable verbose logging (INFERRED - check if implemented) Python<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p>"},{"location":"06-contributing/#grammar-changes","title":"Grammar Changes","text":""},{"location":"06-contributing/#when-to-modify-grammar","title":"When to Modify Grammar","text":"<p>Rare: Grammar changes are needed only for: - New Cypher syntax not currently supported - Parser error recovery improvements - Performance optimizations</p> <p>Not needed for: - New functions (add to <code>Function</code> enum in <code>operators.py</code>) - New operators (add to <code>BinaryOperator</code> enum) - Semantic changes (those belong in planner/renderer)</p>"},{"location":"06-contributing/#modifying-the-grammar","title":"Modifying the Grammar","text":"<p>File: CypherParser.g4 (INFERRED - root level)</p> <p>After changes: Bash<pre><code># Regenerate parser\nmake grammar\n\n# Verify grammar compiles\njavac -version  # Ensure Java is installed\n\n# Run parser tests\nmake test-parser  # (INFERRED command)\n</code></pre></p> <p>Note: Generated files in <code>src/gsql2rsql/parser/grammar/</code> will change. Commit them.</p>"},{"location":"06-contributing/#release-process","title":"Release Process","text":"<p>INFERRED - Update with actual process</p>"},{"location":"06-contributing/#versioning","title":"Versioning","text":"<p>Follow Semantic Versioning (SemVer): <code>MAJOR.MINOR.PATCH</code></p> <ul> <li>MAJOR: Breaking changes (API, CLI, SQL output incompatibility)</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"06-contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li> All tests pass on main branch</li> <li> CHANGELOG.md updated (INFERRED - if exists)</li> <li> Version bumped in pyproject.toml</li> <li> Documentation updated</li> <li> Tag created: <code>git tag -a v0.2.0 -m \"Release v0.2.0\"</code></li> <li> Build package: <code>make build</code></li> <li> Publish to PyPI: <code>make publish</code> (or <code>make publish-test</code> for TestPyPI)</li> </ul>"},{"location":"06-contributing/#getting-help","title":"Getting Help","text":""},{"location":"06-contributing/#resources","title":"Resources","text":"<ul> <li>Architecture: 02-architecture.md</li> <li>Decisions: 03-decision-log.md</li> <li>Limitations: 04-limitations.md</li> <li>Developer Guide: 07-developer-guide.md</li> </ul>"},{"location":"06-contributing/#communication","title":"Communication","text":"<p>INFERRED - Update with actual channels</p> <ul> <li>Issues: GitHub Issues (bug reports, feature requests)</li> <li>Discussions: GitHub Discussions (questions, ideas)</li> <li>Slack/Discord: (if available)</li> </ul>"},{"location":"06-contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Include: 1. Cypher query that causes the issue 2. Schema definition (JSON) 3. Expected SQL output (or behavior) 4. Actual SQL output (or error message) 5. Transpiler version: <code>uv run gsql2rsql --version</code> 6. Python version: <code>python --version</code></p> <p>Template: Markdown<pre><code>## Bug Report\n\n**Cypher Query:**\n```cypher\nMATCH (n:Node) WHERE n.prop &gt; 10 RETURN n\n</code></pre></p> <p>Schema: JSON<pre><code>{ \"nodes\": [ ... ] }\n</code></pre></p> <p>Expected SQL: SQL<pre><code>SELECT * FROM Node WHERE prop &gt; 10\n</code></pre></p> <p>Actual SQL: SQL<pre><code>SELECT * FROM Node  -- WHERE clause missing!\n</code></pre></p> <p>Environment: - Transpiler version: 0.1.0 - Python version: 3.12.1 - OS: Ubuntu 22.04 ```</p>"},{"location":"06-contributing/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>07-developer-guide.md \u2014 Detailed extension guide</li> <li>05-testing-and-examples.md \u2014 Testing patterns</li> <li>02-architecture.md \u2014 Component details</li> </ul>"},{"location":"07-developer-guide/","title":"Developer Guide: Extending the Transpiler","text":"<p>This guide provides detailed instructions for extending the transpiler with new features, operators, and SQL targets.</p>"},{"location":"07-developer-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Adding New Functions</li> <li>Adding New Operators</li> <li>Adding New AST Transforms</li> <li>Adding New Codegen Rules</li> <li>Supporting New SQL Targets</li> <li>Debugging Checklist</li> <li>Common Pitfalls</li> </ol>"},{"location":"07-developer-guide/#adding-new-functions","title":"Adding New Functions","text":"<p>Use Case: Add support for a new Cypher function (e.g., <code>toUpper()</code>, <code>sqrt()</code>, <code>split()</code>)</p>"},{"location":"07-developer-guide/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"07-developer-guide/#1-add-function-to-enum","title":"1. Add Function to Enum","text":"<p>File: src/gsql2rsql/parser/operators.py</p> <p>Location: <code>Function</code> enum (~line 50+)</p> Python<pre><code>class Function(Enum):\n    \"\"\"Built-in Cypher functions.\"\"\"\n    # ... existing functions\n\n    # String functions\n    TOUPPER = \"toUpper\"\n    TOLOWER = \"toLower\"\n    SPLIT = \"split\"  # \u2705 Add new function here\n\n    # Math functions\n    SQRT = \"sqrt\"\n    POW = \"pow\"\n\n    # ... more functions\n</code></pre>"},{"location":"07-developer-guide/#2-add-type-evaluation-optional","title":"2. Add Type Evaluation (Optional)","text":"<p>File: src/gsql2rsql/parser/ast.py</p> <p>Location: <code>QueryExpressionFunction.evaluate_type()</code> method (~line 800+)</p> Python<pre><code>class QueryExpressionFunction(QueryExpression):\n    def evaluate_type(self, symbol_table: SymbolTable) -&gt; DataType:\n        # Evaluate argument types\n        arg_types = [arg.evaluate_type(symbol_table) for arg in self.arguments]\n\n        # \u2705 Add type rule for new function\n        if self.function == Function.SPLIT:\n            # split(string, delimiter) -&gt; list[string]\n            return DataType.LIST_STRING\n\n        if self.function == Function.SQRT:\n            # sqrt(number) -&gt; float\n            return DataType.FLOAT\n\n        # ... existing type rules\n</code></pre>"},{"location":"07-developer-guide/#3-add-sql-rendering","title":"3. Add SQL Rendering","text":"<p>File: src/gsql2rsql/renderer/sql_renderer.py</p> <p>Location: <code>SQLRenderer._render_function()</code> method (~line 1500+)</p> Python<pre><code>class SQLRenderer:\n    def _render_function(\n        self,\n        func: Function,\n        args: list[str],\n        context: RenderContext\n    ) -&gt; str:\n        \"\"\"Render Cypher function to SQL.\"\"\"\n\n        # \u2705 Add SQL mapping for new function\n        if func == Function.SPLIT:\n            # Cypher: split(string, delimiter)\n            # Databricks SQL: split(string, delimiter)\n            return f\"split({args[0]}, {args[1]})\"\n\n        if func == Function.SQRT:\n            # Cypher: sqrt(x)\n            # Databricks SQL: sqrt(x)\n            return f\"sqrt({args[0]})\"\n\n        if func == Function.TOUPPER:\n            # Cypher: toUpper(string)\n            # Databricks SQL: UPPER(string)\n            return f\"UPPER({args[0]})\"\n\n        # ... existing function mappings\n</code></pre>"},{"location":"07-developer-guide/#4-add-tests","title":"4. Add Tests","text":"<p>Create Golden File Test: tests/transpile_tests/test_47_string_functions.py</p> Python<pre><code>\"\"\"Test 47: String manipulation functions.\"\"\"\n\nfrom gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom tests.utils.sql_test_utils import assert_sql_equal, load_expected_sql\n\n\nclass TestStringFunctions:\n    TEST_ID = \"47\"\n    TEST_NAME = \"string_functions\"\n\n    def test_split_function(self) -&gt; None:\n        \"\"\"Test split() function transpilation.\"\"\"\n        cypher = \"\"\"\n            MATCH (p:Person)\n            RETURN split(p.name, ' ') AS name_parts\n        \"\"\"\n        actual_sql = self._transpile(cypher)\n\n        expected_sql = load_expected_sql(self.TEST_ID, self.TEST_NAME)\n        assert_sql_equal(expected_sql, actual_sql, self.TEST_ID, self.TEST_NAME)\n\n    def test_split_function_has_split_call(self) -&gt; None:\n        \"\"\"Test that SQL contains split() function call.\"\"\"\n        cypher = \"MATCH (p:Person) RETURN split(p.name, ' ') AS parts\"\n        sql = self._transpile(cypher)\n\n        assert \"split(\" in sql.lower()\n        assert \"p.name\" in sql or \"_gsql2rsql_p_name\" in sql\n</code></pre> <p>Generate Golden File: Bash<pre><code>make dump-sql-save ID=47 NAME=string_functions\n</code></pre></p> <p>Run Tests: Bash<pre><code>pytest tests/transpile_tests/test_47_string_functions.py -v\nmake test-no-pyspark\n</code></pre></p>"},{"location":"07-developer-guide/#function-categories","title":"Function Categories","text":"Category Examples SQL Mapping Notes String <code>toUpper()</code>, <code>toLower()</code>, <code>trim()</code>, <code>split()</code> Usually 1:1 with SQL functions Math <code>sqrt()</code>, <code>pow()</code>, <code>round()</code>, <code>abs()</code> Usually 1:1 with SQL functions List <code>size()</code>, <code>reverse()</code>, <code>head()</code>, <code>tail()</code> May need SQL array functions Aggregation <code>collect()</code>, <code>count()</code>, <code>sum()</code>, <code>avg()</code> Handled separately (see below) Date/Time <code>datetime()</code>, <code>date()</code>, <code>timestamp()</code> Databricks-specific functions"},{"location":"07-developer-guide/#adding-new-operators","title":"Adding New Operators","text":"<p>Use Case: Add support for a new binary operator (e.g., <code>=~</code> for regex matching)</p>"},{"location":"07-developer-guide/#step-by-step-guide_1","title":"Step-by-Step Guide","text":""},{"location":"07-developer-guide/#1-add-operator-to-enum","title":"1. Add Operator to Enum","text":"<p>File: src/gsql2rsql/parser/operators.py</p> <p>Location: <code>BinaryOperator</code> enum (~line 20+)</p> Python<pre><code>class BinaryOperator(Enum):\n    \"\"\"Binary operators in Cypher.\"\"\"\n    # ... existing operators\n\n    # Comparison operators\n    REGEX_MATCH = \"=~\"  # \u2705 Add new operator\n    NOT_REGEX_MATCH = \"!=~\"  # INFERRED - may not exist\n\n    # ... more operators\n</code></pre>"},{"location":"07-developer-guide/#2-update-grammar-if-needed","title":"2. Update Grammar (if needed)","text":"<p>File: CypherParser.g4 (INFERRED)</p> <p>Location: Expression rules</p> <p>If the operator is not already in the grammar, add it: ANTLR<pre><code>comparisonExpression\n    : additiveExpression (\n        ('=' | '&lt;&gt;' | '!=' | '&lt;' | '&lt;=' | '&gt;' | '&gt;=' | '=~' | '!=~') additiveExpression\n      )*\n    ;\n</code></pre></p> <p>Regenerate Parser: Bash<pre><code>make grammar\n</code></pre></p>"},{"location":"07-developer-guide/#3-update-visitor-if-needed","title":"3. Update Visitor (if needed)","text":"<p>File: src/gsql2rsql/parser/visitor.py</p> <p>Location: <code>visitComparisonExpression()</code> method</p> Python<pre><code>class CypherVisitor:\n    def visitComparisonExpression(self, ctx):\n        # ... existing logic\n\n        # \u2705 Map operator string to enum\n        operator_map = {\n            \"=\": BinaryOperator.EQUALS,\n            \"&lt;&gt;\": BinaryOperator.NOT_EQUALS,\n            \"=~\": BinaryOperator.REGEX_MATCH,  # Add mapping\n            # ... more mappings\n        }\n</code></pre>"},{"location":"07-developer-guide/#4-add-sql-rendering","title":"4. Add SQL Rendering","text":"<p>File: src/gsql2rsql/renderer/sql_renderer.py</p> <p>Location: <code>_render_binary_operator()</code> method (~line 1200+)</p> Python<pre><code>class SQLRenderer:\n    def _render_binary_operator(\n        self,\n        operator: BinaryOperator,\n        left: str,\n        right: str\n    ) -&gt; str:\n        \"\"\"Render binary operator to SQL.\"\"\"\n\n        # \u2705 Add SQL mapping\n        if operator == BinaryOperator.REGEX_MATCH:\n            # Databricks SQL: RLIKE or REGEXP\n            return f\"({left} RLIKE {right})\"\n\n        if operator == BinaryOperator.NOT_REGEX_MATCH:\n            return f\"(NOT {left} RLIKE {right})\"\n\n        # ... existing operators\n        if operator == BinaryOperator.EQUALS:\n            return f\"({left} = {right})\"\n</code></pre>"},{"location":"07-developer-guide/#5-add-tests","title":"5. Add Tests","text":"Python<pre><code>def test_regex_match_operator(self) -&gt; None:\n    \"\"\"Test =~ operator transpilation.\"\"\"\n    cypher = \"\"\"\n        MATCH (p:Person)\n        WHERE p.name =~ 'J.*'\n        RETURN p.name\n    \"\"\"\n    sql = self._transpile(cypher)\n\n    assert \"RLIKE\" in sql or \"REGEXP\" in sql\n</code></pre>"},{"location":"07-developer-guide/#adding-new-ast-transforms","title":"Adding New AST Transforms","text":"<p>Use Case: Add a new optimization or transformation that modifies the AST or logical plan</p>"},{"location":"07-developer-guide/#example-constant-folding-optimization","title":"Example: Constant Folding Optimization","text":"<p>Goal: Evaluate constant expressions at transpile time (e.g., <code>2 + 3</code> \u2192 <code>5</code>)</p>"},{"location":"07-developer-guide/#1-create-transform-class","title":"1. Create Transform Class","text":"<p>File: src/gsql2rsql/planner/constant_folder.py (new file)</p> Python<pre><code>\"\"\"Constant folding optimization for expressions.\"\"\"\n\nfrom gsql2rsql.parser.ast import (\n    QueryExpression,\n    QueryExpressionBinary,\n    QueryExpressionValue,\n)\nfrom gsql2rsql.parser.operators import BinaryOperator\n\n\nclass ConstantFolder:\n    \"\"\"Fold constant expressions into single values.\"\"\"\n\n    def fold_expression(self, expr: QueryExpression) -&gt; QueryExpression:\n        \"\"\"Recursively fold constants in expression tree.\"\"\"\n\n        if isinstance(expr, QueryExpressionBinary):\n            # Fold children first\n            left = self.fold_expression(expr.left)\n            right = self.fold_expression(expr.right)\n\n            # Check if both sides are constants\n            if isinstance(left, QueryExpressionValue) and isinstance(right, QueryExpressionValue):\n                # Evaluate constant operation\n                result = self._evaluate_constant_binary(expr.operator, left.value, right.value)\n                return QueryExpressionValue(value=result)\n\n            # Return with folded children\n            return QueryExpressionBinary(operator=expr.operator, left=left, right=right)\n\n        return expr\n\n    def _evaluate_constant_binary(self, op: BinaryOperator, left: Any, right: Any) -&gt; Any:\n        \"\"\"Evaluate binary operation on constants.\"\"\"\n        if op == BinaryOperator.PLUS:\n            return left + right\n        if op == BinaryOperator.MINUS:\n            return left - right\n        if op == BinaryOperator.MULTIPLY:\n            return left * right\n        if op == BinaryOperator.DIVIDE:\n            return left / right\n        # ... more operators\n\n        # Can't fold, return None (caller will keep original)\n        return None\n</code></pre>"},{"location":"07-developer-guide/#2-integrate-into-pipeline","title":"2. Integrate into Pipeline","text":"<p>File: src/gsql2rsql/planner/logical_plan.py</p> <p>Location: <code>process_query_tree()</code> method</p> Python<pre><code>from gsql2rsql.planner.constant_folder import ConstantFolder\n\nclass LogicalPlan:\n    @staticmethod\n    def process_query_tree(ast: QueryNode, schema: IGraphSchemaProvider) -&gt; \"LogicalPlan\":\n        # ... existing logic\n\n        # \u2705 Apply constant folding (optional optimization)\n        if enable_constant_folding:\n            folder = ConstantFolder()\n            ast = folder.fold_query(ast)  # Transform AST\n\n        # Continue with operator construction\n        # ...\n</code></pre>"},{"location":"07-developer-guide/#3-add-cli-flag-optional","title":"3. Add CLI Flag (Optional)","text":"<p>File: src/gsql2rsql/cli.py</p> Python<pre><code>@click.option(\n    \"--fold-constants\",\n    is_flag=True,\n    default=False,\n    help=\"Enable constant folding optimization\"\n)\ndef transpile(schema, optimize, fold_constants, ...):\n    # ... pass flag to planner\n</code></pre>"},{"location":"07-developer-guide/#4-add-tests_1","title":"4. Add Tests","text":"Python<pre><code>def test_constant_folding_addition(self) -&gt; None:\n    \"\"\"Test that 2 + 3 is folded to 5.\"\"\"\n    cypher = \"MATCH (n:Node) WHERE n.age &gt; 2 + 3 RETURN n\"\n    sql = self._transpile(cypher, fold_constants=True)\n\n    # Should contain 5, not 2 + 3\n    assert \"&gt; 5\" in sql\n    assert \"+ 3\" not in sql\n</code></pre>"},{"location":"07-developer-guide/#adding-new-codegen-rules","title":"Adding New Codegen Rules","text":"<p>Use Case: Generate more efficient SQL for specific patterns</p>"},{"location":"07-developer-guide/#example-in-clause-optimization","title":"Example: IN Clause Optimization","text":"<p>Goal: Convert <code>x = 1 OR x = 2 OR x = 3</code> to <code>x IN (1, 2, 3)</code></p>"},{"location":"07-developer-guide/#1-add-pattern-detection","title":"1. Add Pattern Detection","text":"<p>File: src/gsql2rsql/renderer/sql_renderer.py</p> <p>Location: New method</p> Python<pre><code>class SQLRenderer:\n    def _detect_in_pattern(self, expr: QueryExpressionBinary) -&gt; Optional[tuple[str, list[Any]]]:\n        \"\"\"Detect x = A OR x = B OR x = C pattern.\"\"\"\n\n        if expr.operator != BinaryOperator.OR:\n            return None\n\n        # Collect all OR-connected EQUALS comparisons\n        equals_exprs = self._collect_or_equals(expr)\n\n        if not equals_exprs:\n            return None\n\n        # Check if all comparisons reference same column\n        columns = [e.left for e in equals_exprs]\n        if not all(self._same_column(columns[0], col) for col in columns):\n            return None\n\n        # Extract values\n        values = [e.right for e in equals_exprs]\n        return (columns[0], values)\n\n    def _render_expression(self, expr: QueryExpression, context: RenderContext) -&gt; str:\n        \"\"\"Render expression with IN optimization.\"\"\"\n\n        if isinstance(expr, QueryExpressionBinary):\n            # \u2705 Try to optimize to IN clause\n            in_pattern = self._detect_in_pattern(expr)\n            if in_pattern:\n                column, values = in_pattern\n                column_sql = self._render_expression(column, context)\n                values_sql = \", \".join(self._render_expression(v, context) for v in values)\n                return f\"{column_sql} IN ({values_sql})\"\n\n        # ... existing rendering logic\n</code></pre>"},{"location":"07-developer-guide/#2-add-tests","title":"2. Add Tests","text":"Python<pre><code>def test_or_optimized_to_in_clause(self) -&gt; None:\n    \"\"\"Test that x = 1 OR x = 2 OR x = 3 becomes x IN (1, 2, 3).\"\"\"\n    cypher = \"\"\"\n        MATCH (n:Node)\n        WHERE n.age = 10 OR n.age = 20 OR n.age = 30\n        RETURN n\n    \"\"\"\n    sql = self._transpile(cypher)\n\n    assert \"IN (10, 20, 30)\" in sql\n    assert \"OR\" not in sql  # Should be optimized away\n</code></pre>"},{"location":"07-developer-guide/#supporting-new-sql-targets","title":"Supporting New SQL Targets","text":"<p>Use Case: Generate SQL for PostgreSQL, DuckDB, or other databases instead of Databricks</p>"},{"location":"07-developer-guide/#architecture-for-multi-target-support","title":"Architecture for Multi-Target Support","text":""},{"location":"07-developer-guide/#1-create-sql-dialect-interface","title":"1. Create SQL Dialect Interface","text":"<p>File: src/gsql2rsql/renderer/sql_dialect.py (new file)</p> Python<pre><code>\"\"\"SQL dialect abstraction for multi-target support.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\n\n\nclass SQLDialect(Enum):\n    \"\"\"Supported SQL dialects.\"\"\"\n    DATABRICKS = \"databricks\"\n    POSTGRESQL = \"postgresql\"\n    DUCKDB = \"duckdb\"\n    SQLITE = \"sqlite\"\n\n\nclass ISQLDialectProvider(ABC):\n    \"\"\"Interface for SQL dialect-specific code generation.\"\"\"\n\n    @abstractmethod\n    def render_recursive_cte(self, ...params...) -&gt; str:\n        \"\"\"Generate WITH RECURSIVE CTE (syntax varies by dialect).\"\"\"\n\n    @abstractmethod\n    def render_array_concat(self, left: str, right: str) -&gt; str:\n        \"\"\"Generate array concatenation (CONCAT vs ||).\"\"\"\n\n    @abstractmethod\n    def render_array_contains(self, array: str, element: str) -&gt; str:\n        \"\"\"Generate array containment check (array_contains vs ANY).\"\"\"\n\n    @abstractmethod\n    def render_struct(self, fields: dict[str, str]) -&gt; str:\n        \"\"\"Generate struct/row literal (STRUCT vs ROW).\"\"\"\n\n    @abstractmethod\n    def supports_recursive_cte(self) -&gt; bool:\n        \"\"\"Check if dialect supports WITH RECURSIVE.\"\"\"\n\n\nclass DatabricksDialect(ISQLDialectProvider):\n    \"\"\"Databricks Spark SQL dialect.\"\"\"\n\n    def render_recursive_cte(self, ...params...) -&gt; str:\n        # Current implementation\n        return f\"WITH RECURSIVE {name} AS (...)\"\n\n    def render_array_concat(self, left: str, right: str) -&gt; str:\n        return f\"CONCAT({left}, {right})\"\n\n    def render_array_contains(self, array: str, element: str) -&gt; str:\n        return f\"array_contains({array}, {element})\"\n\n    def render_struct(self, fields: dict[str, str]) -&gt; str:\n        field_list = \", \".join(f\"{v} AS {k}\" for k, v in fields.items())\n        return f\"STRUCT({field_list})\"\n\n    def supports_recursive_cte(self) -&gt; bool:\n        return True\n\n\nclass PostgreSQLDialect(ISQLDialectProvider):\n    \"\"\"PostgreSQL dialect.\"\"\"\n\n    def render_array_concat(self, left: str, right: str) -&gt; str:\n        return f\"({left} || {right})\"  # PostgreSQL uses || for arrays\n\n    def render_array_contains(self, array: str, element: str) -&gt; str:\n        return f\"({element} = ANY({array}))\"  # PostgreSQL uses ANY()\n\n    def render_struct(self, fields: dict[str, str]) -&gt; str:\n        field_list = \", \".join(f\"{v}\" for v in fields.values())\n        return f\"ROW({field_list})\"  # PostgreSQL uses ROW()\n\n    def supports_recursive_cte(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"07-developer-guide/#2-refactor-renderer-to-use-dialect","title":"2. Refactor Renderer to Use Dialect","text":"<p>File: src/gsql2rsql/renderer/sql_renderer.py</p> Python<pre><code>from gsql2rsql.renderer.sql_dialect import ISQLDialectProvider, DatabricksDialect\n\n\nclass SQLRenderer:\n    def __init__(\n        self,\n        db_schema_provider: ISQLDBSchemaProvider,\n        dialect: ISQLDialectProvider = None  # \u2705 Add dialect parameter\n    ):\n        self.db_schema = db_schema_provider\n        self.dialect = dialect or DatabricksDialect()  # Default to Databricks\n\n    def _render_recursive(self, op: RecursiveTraversalOperator) -&gt; str:\n        \"\"\"Render WITH RECURSIVE using dialect-specific syntax.\"\"\"\n        # \u2705 Delegate to dialect\n        return self.dialect.render_recursive_cte(...)\n\n    def _render_array_concat(self, left: str, right: str) -&gt; str:\n        \"\"\"Render array concatenation using dialect-specific syntax.\"\"\"\n        return self.dialect.render_array_concat(left, right)\n</code></pre>"},{"location":"07-developer-guide/#3-add-cli-flag-for-dialect-selection","title":"3. Add CLI Flag for Dialect Selection","text":"<p>File: src/gsql2rsql/cli.py</p> Python<pre><code>@click.option(\n    \"--dialect\",\n    type=click.Choice([\"databricks\", \"postgresql\", \"duckdb\"]),\n    default=\"databricks\",\n    help=\"Target SQL dialect\"\n)\ndef transpile(schema, dialect, ...):\n    # ... load schema\n\n    # \u2705 Create dialect provider\n    if dialect == \"postgresql\":\n        from gsql2rsql.renderer.sql_dialect import PostgreSQLDialect\n        dialect_provider = PostgreSQLDialect()\n    elif dialect == \"duckdb\":\n        dialect_provider = DuckDBDialect()\n    else:\n        dialect_provider = DatabricksDialect()\n\n    # Pass to renderer\n    renderer = SQLRenderer(db_schema_provider=sql_schema, dialect=dialect_provider)\n</code></pre>"},{"location":"07-developer-guide/#4-add-tests-for-each-dialect","title":"4. Add Tests for Each Dialect","text":"Python<pre><code>class TestPostgreSQLDialect:\n    \"\"\"Test PostgreSQL-specific SQL generation.\"\"\"\n\n    def test_array_concat_uses_pipe_operator(self):\n        cypher = \"MATCH (n)-[:REL*1..3]-&gt;(m) RETURN n\"\n        sql = self._transpile(cypher, dialect=\"postgresql\")\n\n        # PostgreSQL uses || for array concat\n        assert \"||\" in sql\n        assert \"CONCAT(\" not in sql\n\n    def test_array_contains_uses_any(self):\n        cypher = \"MATCH (n)-[r*]-(m) WHERE 'foo' IN relationships(path) RETURN n\"\n        sql = self._transpile(cypher, dialect=\"postgresql\")\n\n        assert \"ANY(\" in sql\n        assert \"array_contains(\" not in sql\n</code></pre>"},{"location":"07-developer-guide/#debugging-checklist","title":"Debugging Checklist","text":""},{"location":"07-developer-guide/#when-sql-generation-is-wrong","title":"When SQL Generation is Wrong","text":"<p>Step 1: Verify AST is Correct Bash<pre><code># Parse only, inspect AST\nuv run gsql2rsql parse -i query.cypher\n\n# Look for:\n# - Correct node types\n# - Correct operator precedence\n# - All expressions parsed\n</code></pre></p> <p>Step 2: Verify Logical Plan is Correct Python<pre><code># In Python REPL\nfrom gsql2rsql import OpenCypherParser, LogicalPlan\n\nast = parser.parse(query)\nplan = LogicalPlan.process_query_tree(ast, schema)\n\nprint(plan.dump_graph())  # Visualize operator tree\n\n# Look for:\n# - Correct operator types (Join, Selection, Projection, etc.)\n# - Correct operator ordering\n# - All predicates captured\n</code></pre></p> <p>Step 3: Verify Symbol Table is Correct Bash<pre><code># Show scope information\nuv run gsql2rsql transpile -s schema.json -i query.cypher --explain-scopes\n\n# Look for:\n# - All variables defined\n# - Correct scopes (WITH boundaries)\n# - No missing definitions\n</code></pre></p> <p>Step 4: Verify Column Resolution is Correct Python<pre><code># Check ResolutionResult\nplan.resolve(original_query=query)\n\nif not plan.is_resolved:\n    print(\"Resolution failed!\")\n\n# Inspect resolved columns\nfor proj_name, resolved_proj in plan.resolution_result.resolved_projections.items():\n    print(f\"{proj_name}: {resolved_proj.column_refs}\")\n</code></pre></p> <p>Step 5: Inspect Generated SQL Bash<pre><code># Generate SQL\nuv run gsql2rsql transpile -s schema.json -i query.cypher\n\n# Compare with golden file\nmake dump-sql ID=XX NAME=test_name --diff\n</code></pre></p>"},{"location":"07-developer-guide/#when-tests-are-failing","title":"When Tests are Failing","text":"<p>Checklist: - [ ] Did you run <code>make format</code> and <code>make check</code>? - [ ] Are all imports correct (no circular dependencies)? - [ ] Did you update golden files if SQL output changed? - [ ] Are type hints correct (run <code>make typecheck</code>)? - [ ] Did you add unit tests and integration tests? - [ ] Does the change respect phase boundaries (see 06-contributing.md)?</p>"},{"location":"07-developer-guide/#when-pyspark-tests-are-failing","title":"When PySpark Tests are Failing","text":"<p>Common Issues:</p> <ol> <li>Schema mismatch: Generated data doesn't match schema definition</li> <li>Check: src/gsql2rsql/pyspark_executor.py data generation</li> <li> <p>Fix: Update sample data generator</p> </li> <li> <p>SQL syntax error: Generated SQL is invalid for Spark</p> </li> <li>Check: Run SQL directly in Databricks SQL editor</li> <li> <p>Fix: Update renderer to use correct Spark SQL syntax</p> </li> <li> <p>Result mismatch: Query returns wrong results</p> </li> <li>Check: Add <code>print(df.show())</code> to see actual results</li> <li> <p>Fix: Likely a semantic bug in planner or renderer</p> </li> <li> <p>Timeout: Query takes too long</p> </li> <li>Check: Is the query generating cartesian product?</li> <li>Fix: Add proper join conditions</li> </ol>"},{"location":"07-developer-guide/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"07-developer-guide/#1-violating-phase-boundaries","title":"1. Violating Phase Boundaries","text":"<p>\u274c Wrong: Python<pre><code># Parser accessing schema\nclass CypherVisitor:\n    def visitPropertyExpression(self, ctx):\n        if not self.schema.has_property(...):  # \u274c Parser shouldn't access schema\n            raise Exception()\n</code></pre></p> <p>\u2705 Correct: Python<pre><code># Resolver validating properties\nclass ColumnResolver:\n    def resolve_property(self, entity, property):\n        if not self.schema.has_property(...):  # \u2705 Resolver's responsibility\n            raise ColumnResolutionError()\n</code></pre></p>"},{"location":"07-developer-guide/#2-forgetting-to-update-golden-files","title":"2. Forgetting to Update Golden Files","text":"<p>Problem: Test fails after legitimate SQL change</p> <p>Solution: Bash<pre><code># Review change\nmake dump-sql ID=XX NAME=test_name --diff\n\n# If correct, update golden file\nmake dump-sql-save ID=XX NAME=test_name\n</code></pre></p>"},{"location":"07-developer-guide/#3-not-handling-null-values","title":"3. Not Handling NULL Values","text":"<p>Problem: OPTIONAL MATCH generates incorrect SQL (no null handling)</p> <p>Solution: Always use <code>COALESCE()</code> for optional properties Python<pre><code>if column_ref.is_from_optional_match:\n    return f\"COALESCE({sql_column}, NULL)\"\n</code></pre></p>"},{"location":"07-developer-guide/#4-incorrect-operator-precedence","title":"4. Incorrect Operator Precedence","text":"<p>Problem: <code>a + b * c</code> rendered as <code>(a + b) * c</code> instead of <code>a + (b * c)</code></p> <p>Solution: Ensure parentheses in renderer respect precedence Python<pre><code>def _render_binary(self, expr):\n    left = self._render_with_parens_if_needed(expr.left, expr.operator)\n    right = self._render_with_parens_if_needed(expr.right, expr.operator)\n    return f\"{left} {op} {right}\"\n</code></pre></p>"},{"location":"07-developer-guide/#5-missing-type-annotations","title":"5. Missing Type Annotations","text":"<p>Problem: Mypy errors in CI</p> <p>Solution: Add type hints to all functions Python<pre><code># \u2705 All parameters and return types annotated\ndef render_expression(self, expr: QueryExpression, ctx: RenderContext) -&gt; str:\n    ...\n</code></pre></p>"},{"location":"07-developer-guide/#6-circular-imports","title":"6. Circular Imports","text":"<p>Problem: <code>ImportError: cannot import name 'X' from partially initialized module</code></p> <p>Solution: Use forward references and import inside functions Python<pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from gsql2rsql.planner.operators import LogicalOperator\n\ndef process(operator: \"LogicalOperator\") -&gt; None:\n    ...\n</code></pre></p>"},{"location":"07-developer-guide/#7-mutable-default-arguments","title":"7. Mutable Default Arguments","text":"<p>Problem: Default list/dict arguments are shared across calls</p> <p>\u274c Wrong: Python<pre><code>def process(items: list[str] = []) -&gt; None:  # \u274c Mutable default\n    items.append(\"new\")\n</code></pre></p> <p>\u2705 Correct: Python<pre><code>def process(items: list[str] | None = None) -&gt; None:  # \u2705 Use None\n    if items is None:\n        items = []\n    items.append(\"new\")\n</code></pre></p>"},{"location":"07-developer-guide/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>06-contributing.md \u2014 Code style and PR workflow</li> <li>02-architecture.md \u2014 Phase details and boundaries</li> <li>05-testing-and-examples.md \u2014 Testing patterns</li> <li>08-api-reference.md \u2014 Public API reference</li> </ul>"},{"location":"08-api-reference/","title":"API Reference","text":"<p>This document provides a reference for the public API, CLI commands, and configuration options.</p>"},{"location":"08-api-reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Python API</li> <li>CLI Reference</li> <li>Schema Format</li> <li>Configuration Options</li> <li>Exception Types</li> </ol>"},{"location":"08-api-reference/#python-api","title":"Python API","text":""},{"location":"08-api-reference/#public-modules","title":"Public Modules","text":"<p>The transpiler can be used as a Python library. Import from the top-level package:</p> Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\n</code></pre>"},{"location":"08-api-reference/#core-classes","title":"Core Classes","text":""},{"location":"08-api-reference/#opencypherparser","title":"<code>OpenCypherParser</code>","text":"<p>Module: <code>gsql2rsql.parser.opencypher_parser</code></p> <p>Purpose: Parse OpenCypher query strings to AST.</p> Python<pre><code>class OpenCypherParser:\n    \"\"\"Parser for OpenCypher queries.\"\"\"\n\n    def parse(self, query: str) -&gt; QueryNode:\n        \"\"\"\n        Parse OpenCypher query string to AST.\n\n        Args:\n            query: OpenCypher query string\n\n        Returns:\n            QueryNode: Root AST node\n\n        Raises:\n            TranspilerSyntaxErrorException: If query has syntax errors\n        \"\"\"\n</code></pre> <p>Example Usage: Python<pre><code>from gsql2rsql import OpenCypherParser\n\nparser = OpenCypherParser()\nast = parser.parse(\"MATCH (n:Person) RETURN n\")\n\nprint(ast.dump_tree())  # Visualize AST\n</code></pre></p>"},{"location":"08-api-reference/#logicalplan","title":"<code>LogicalPlan</code>","text":"<p>Module: <code>gsql2rsql.planner.logical_plan</code></p> <p>Purpose: Convert AST to logical operator tree.</p> Python<pre><code>class LogicalPlan:\n    \"\"\"Logical query plan with operator tree.\"\"\"\n\n    @staticmethod\n    def process_query_tree(\n        ast: QueryNode,\n        graph_schema: IGraphSchemaProvider\n    ) -&gt; \"LogicalPlan\":\n        \"\"\"\n        Convert AST to logical plan.\n\n        Args:\n            ast: Abstract syntax tree from parser\n            graph_schema: Graph schema provider\n\n        Returns:\n            LogicalPlan: Logical plan with operators\n\n        Raises:\n            TranspilerBindingException: If schema binding fails\n        \"\"\"\n\n    def resolve(self, original_query: str) -&gt; None:\n        \"\"\"\n        Resolve column references and validate schema.\n\n        Args:\n            original_query: Original Cypher query (for error messages)\n\n        Raises:\n            ColumnResolutionError: If column resolution fails\n        \"\"\"\n\n    def is_resolved(self) -&gt; bool:\n        \"\"\"Check if plan has been resolved.\"\"\"\n\n    def dump_graph(self) -&gt; str:\n        \"\"\"Generate text visualization of operator tree.\"\"\"\n\n    @property\n    def starting_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get starting operators (data sources).\"\"\"\n\n    @property\n    def terminal_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get terminal operators (outputs).\"\"\"\n\n    def all_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get all operators in topological order.\"\"\"\n</code></pre> <p>Example Usage: Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan\nfrom gsql2rsql.common.schema import SimpleGraphSchemaProvider\n\n# Parse\nparser = OpenCypherParser()\nast = parser.parse(\"MATCH (n:Person) RETURN n\")\n\n# Build schema\nschema = SimpleGraphSchemaProvider()\n# ... add nodes and edges\n\n# Create logical plan\nplan = LogicalPlan.process_query_tree(ast, schema)\n\n# Resolve column references\nplan.resolve(original_query=\"MATCH (n:Person) RETURN n\")\n\n# Inspect plan\nprint(plan.dump_graph())\n</code></pre></p>"},{"location":"08-api-reference/#sqlrenderer","title":"<code>SQLRenderer</code>","text":"<p>Module: <code>gsql2rsql.renderer.sql_renderer</code></p> <p>Purpose: Generate Databricks SQL from logical plan.</p> Python<pre><code>class SQLRenderer:\n    \"\"\"SQL code generator for Databricks Spark SQL.\"\"\"\n\n    def __init__(\n        self,\n        db_schema_provider: ISQLDBSchemaProvider,\n        enable_column_pruning: bool = True,\n        config: dict[str, Any] | None = None\n    ):\n        \"\"\"\n        Initialize renderer.\n\n        Args:\n            db_schema_provider: SQL database schema provider\n            enable_column_pruning: Enable column pruning optimization (default: True)\n            config: Optional configuration dictionary for renderer behavior.\n                Supported keys:\n                - 'undirected_strategy': Strategy for undirected relationships.\n                  Values: 'union_edges' (default) or 'or_join'\n        \"\"\"\n\n    def render_plan(self, plan: LogicalPlan) -&gt; str:\n        \"\"\"\n        Generate SQL from logical plan.\n\n        Args:\n            plan: Logical plan (must be resolved)\n\n        Returns:\n            str: Databricks Spark SQL query\n\n        Raises:\n            RuntimeError: If plan is not resolved\n            TranspilerNotSupportedException: If unsupported pattern\n        \"\"\"\n</code></pre> <p>Example Usage (Default): Python<pre><code>from gsql2rsql import SQLRenderer\nfrom gsql2rsql.renderer.schema_provider import SimpleSQLSchemaProvider, SQLTableDescriptor\n\n# Create SQL schema provider\nsql_schema = SimpleSQLSchemaProvider()\nsql_schema.add_node(\n    node_schema,\n    SQLTableDescriptor(table_name=\"dbo.Person\", node_id_columns=[\"id\"])\n)\n\n# Render SQL with default optimizations\nrenderer = SQLRenderer(db_schema_provider=sql_schema)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n</code></pre></p> <p>Example Usage (Custom Configuration): Python<pre><code># Disable undirected relationship optimization (use legacy OR joins)\n# Only recommended for debugging or very small datasets\nrenderer = SQLRenderer(\n    db_schema_provider=sql_schema,\n    config={\"undirected_strategy\": \"or_join\"}  # Default: \"union_edges\"\n)\nsql = renderer.render_plan(plan)\n</code></pre></p> <p>Configuration Options:</p> Key Values Default Description <code>undirected_strategy</code> <code>\"union_edges\"</code> or <code>\"or_join\"</code> <code>\"union_edges\"</code> Strategy for undirected relationships (<code>-[:TYPE]-</code>). <code>\"union_edges\"</code> uses UNION ALL for O(n) performance. <code>\"or_join\"</code> uses OR conditions (legacy, slower). See limitations.md for details."},{"location":"08-api-reference/#schema-classes","title":"Schema Classes","text":""},{"location":"08-api-reference/#simplegraphschemaprovider","title":"<code>SimpleGraphSchemaProvider</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> <p>Purpose: In-memory graph schema provider.</p> Python<pre><code>class SimpleGraphSchemaProvider(IGraphSchemaProvider):\n    \"\"\"Simple in-memory graph schema provider.\"\"\"\n\n    def add_node(self, node: NodeSchema) -&gt; None:\n        \"\"\"Add node type to schema.\"\"\"\n\n    def add_edge(self, edge: EdgeSchema) -&gt; None:\n        \"\"\"Add edge type to schema.\"\"\"\n\n    def get_node_schema(self, label: str) -&gt; NodeSchema | None:\n        \"\"\"Get node schema by label.\"\"\"\n\n    def get_edge_schema(self, type_name: str) -&gt; EdgeSchema | None:\n        \"\"\"Get edge schema by type name.\"\"\"\n\n    def all_nodes(self) -&gt; list[NodeSchema]:\n        \"\"\"Get all node schemas.\"\"\"\n\n    def all_edges(self) -&gt; list[EdgeSchema]:\n        \"\"\"Get all edge schemas.\"\"\"\n</code></pre>"},{"location":"08-api-reference/#nodeschema","title":"<code>NodeSchema</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass NodeSchema(EntitySchema):\n    \"\"\"Schema for a node type.\"\"\"\n\n    name: str\n    properties: list[EntityProperty]\n    node_id_property: EntityProperty\n</code></pre>"},{"location":"08-api-reference/#edgeschema","title":"<code>EdgeSchema</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass EdgeSchema(EntitySchema):\n    \"\"\"Schema for an edge type.\"\"\"\n\n    name: str\n    source_node: str\n    sink_node: str\n    properties: list[EntityProperty]\n    source_id_property: EntityProperty\n    sink_id_property: EntityProperty\n</code></pre>"},{"location":"08-api-reference/#entityproperty","title":"<code>EntityProperty</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass EntityProperty:\n    \"\"\"Property definition for node or edge.\"\"\"\n\n    name: str\n    python_type: type  # int, str, float, bool, etc.\n</code></pre>"},{"location":"08-api-reference/#full-example-transpilation-pipeline","title":"Full Example: Transpilation Pipeline","text":"Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom gsql2rsql.common.schema import (\n    SimpleGraphSchemaProvider,\n    NodeSchema,\n    EdgeSchema,\n    EntityProperty,\n)\nfrom gsql2rsql.renderer.schema_provider import (\n    SimpleSQLSchemaProvider,\n    SQLTableDescriptor,\n)\n\n# 1. Define graph schema\ngraph_schema = SimpleGraphSchemaProvider()\n\ngraph_schema.add_node(\n    NodeSchema(\n        name=\"Person\",\n        node_id_property=EntityProperty(\"id\", int),\n        properties=[\n            EntityProperty(\"id\", int),\n            EntityProperty(\"name\", str),\n            EntityProperty(\"age\", int),\n        ],\n    )\n)\n\ngraph_schema.add_edge(\n    EdgeSchema(\n        name=\"KNOWS\",\n        source_node=\"Person\",\n        sink_node=\"Person\",\n        source_id_property=EntityProperty(\"source_id\", int),\n        sink_id_property=EntityProperty(\"target_id\", int),\n        properties=[],\n    )\n)\n\n# 2. Define SQL schema\nsql_schema = SimpleSQLSchemaProvider()\nsql_schema.add_node(\n    NodeSchema(name=\"Person\", node_id_property=EntityProperty(\"id\", int)),\n    SQLTableDescriptor(table_name=\"graph.Person\", node_id_columns=[\"id\"]),\n)\nsql_schema.add_edge(\n    EdgeSchema(\n        name=\"KNOWS\",\n        source_node=\"Person\",\n        sink_node=\"Person\",\n        source_id_property=EntityProperty(\"source_id\", int),\n        sink_id_property=EntityProperty(\"target_id\", int),\n    ),\n    SQLTableDescriptor(\n        table_name=\"graph.Knows\",\n        source_id_columns=[\"source_id\"],\n        sink_id_columns=[\"target_id\"],\n    ),\n)\n\n# 3. Parse query\nquery = \"MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name, f.name\"\nparser = OpenCypherParser()\nast = parser.parse(query)\n\n# 4. Create logical plan\nplan = LogicalPlan.process_query_tree(ast, graph_schema)\n\n# 5. Optimize (optional)\nfrom gsql2rsql.planner.subquery_optimizer import SubqueryFlatteningOptimizer\noptimizer = SubqueryFlatteningOptimizer(enabled=True)\noptimizer.optimize(plan)\n\n# 6. Resolve columns\nplan.resolve(original_query=query)\n\n# 7. Render SQL\nrenderer = SQLRenderer(db_schema_provider=sql_schema)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n</code></pre>"},{"location":"08-api-reference/#schema-definition-via-python-api","title":"Schema Definition via Python API","text":"<p>You can define schemas programmatically using Python dataclasses instead of JSON files.</p>"},{"location":"08-api-reference/#using-dataclasses-recommended","title":"Using Dataclasses (Recommended)","text":"Python<pre><code>from gsql2rsql.common.schema import NodeSchema, EdgeSchema, EntityProperty\nfrom gsql2rsql.planner.schema import SimpleGraphSchemaProvider\n\n# Create schema provider\nschema = SimpleGraphSchemaProvider()\n\n# Define nodes\nperson = NodeSchema(\n    name=\"Person\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"age\", data_type=int),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\ncompany = NodeSchema(\n    name=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"industry\", data_type=str),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\n# Define edges\nworks_at = EdgeSchema(\n    name=\"WORKS_AT\",\n    source_node_id=\"Person\",\n    sink_node_id=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"since\", data_type=int),\n    ],\n    source_id_property=EntityProperty(property_name=\"person_id\", data_type=int),\n    sink_id_property=EntityProperty(property_name=\"company_id\", data_type=int)\n)\n\n# Add to schema\nschema.add_node(person)\nschema.add_node(company)\nschema.add_edge(works_at)\n\n# Use with transpiler\nfrom gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom gsql2rsql.renderer.schema_provider import DatabricksSchemaProvider\n\n# Map entities to SQL tables\nsql_schema = DatabricksSchemaProvider()\nsql_schema.add_table_mapping(\n    entity_id=\"Person\",\n    table_name=\"catalog.mydb.Person\",\n    node_id_columns=[\"id\"],\n    column_mappings={\"id\": \"id\", \"name\": \"name\", \"age\": \"age\"}\n)\nsql_schema.add_table_mapping(\n    entity_id=\"Company\",\n    table_name=\"catalog.mydb.Company\",\n    node_id_columns=[\"id\"]\n)\nsql_schema.add_table_mapping(\n    entity_id=\"Person@WORKS_AT@Company\",\n    table_name=\"catalog.mydb.PersonWorksAt\",\n    column_mappings={\n        \"person_id\": \"person_id\",\n        \"company_id\": \"company_id\",\n        \"since\": \"since\"\n    }\n)\n\n# Transpile\nquery = \"\"\"\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, c.name\n\"\"\"\n\nparser = OpenCypherParser()\nast = parser.parse(query)\n\nplan = LogicalPlan(schema)\nplan.process_query_tree(ast, query)\nplan.resolve(query)\n\nrenderer = SQLRenderer(sql_schema)\nsql = renderer.render_plan(plan)\nprint(sql)\n</code></pre>"},{"location":"08-api-reference/#using-dictionary-alternative","title":"Using Dictionary (Alternative)","text":"<p>You can also pass dictionaries directly:</p> Python<pre><code>from gsql2rsql.planner.schema import SimpleGraphSchemaProvider\n\nschema_dict = {\n    \"nodes\": [\n        {\n            \"name\": \"Person\",\n            \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n            \"properties\": [\n                {\"name\": \"name\", \"type\": \"string\"},\n                {\"name\": \"age\", \"type\": \"int\"}\n            ]\n        }\n    ],\n    \"edges\": [\n        {\n            \"name\": \"KNOWS\",\n            \"sourceNode\": \"Person\",\n            \"sinkNode\": \"Person\",\n            \"sourceIdProperty\": {\"name\": \"person_id\", \"type\": \"int\"},\n            \"sinkIdProperty\": {\"name\": \"friend_id\", \"type\": \"int\"},\n            \"properties\": []\n        }\n    ]\n}\n\n# Convert to schema objects\n# (Implementation depends on your schema loader)\n</code></pre>"},{"location":"08-api-reference/#benefits-of-python-api","title":"Benefits of Python API","text":"<ul> <li>Type Safety: Catch errors at development time</li> <li>IDE Support: Autocompletion and type hints</li> <li>Dynamic Generation: Generate schemas from database introspection</li> <li>Validation: Built-in validation in dataclass <code>__post_init__</code></li> <li>Composability: Reuse schema components across projects</li> </ul>"},{"location":"08-api-reference/#pydantic-support-future","title":"Pydantic Support (Future)","text":"<p>While we currently use Python's built-in <code>dataclasses</code>, we're considering adding Pydantic models for enhanced validation and serialization in future versions. The API would be similar:</p> Python<pre><code># Future Pydantic API (not yet implemented)\nfrom gsql2rsql.schema import NodeModel, EdgeModel\n\nperson = NodeModel(\n    name=\"Person\",\n    properties={\"id\": int, \"name\": str, \"age\": int},\n    id_property=\"id\"\n)\n</code></pre>"},{"location":"08-api-reference/#cli-reference","title":"CLI Reference","text":""},{"location":"08-api-reference/#installation","title":"Installation","text":"Bash<pre><code># Install package\npip install gsql2rsql  # (INFERRED - when published)\n\n# Or install from source\ncd cyper2dsql/python\npip install -e .\n\n# Verify installation\ngsql2rsql --version\n</code></pre>"},{"location":"08-api-reference/#main-command","title":"Main Command","text":"Bash<pre><code>gsql2rsql [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Global Options: - <code>--help</code> \u2014 Show help message - <code>--version</code> \u2014 Show version information</p>"},{"location":"08-api-reference/#command-transpile","title":"Command: <code>transpile</code>","text":"<p>Purpose: Transpile OpenCypher query to Databricks SQL</p> <p>Usage: Bash<pre><code>gsql2rsql transpile [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-s, --schema PATH</code> File path Required JSON schema file <code>-i, --input PATH</code> File path stdin Input Cypher query file <code>-o, --output PATH</code> File path stdout Output SQL file <code>--optimize / --no-optimize</code> Flag <code>--optimize</code> Enable/disable subquery optimization <code>--resolve / --no-resolve</code> Flag <code>--resolve</code> Enable/disable column resolution <code>--explain-scopes</code> Flag Off Show scope information in output <code>--format</code> Choice <code>sql</code> Output format: <code>sql</code>, <code>ast</code>, <code>plan</code> <p>Examples:</p> Bash<pre><code># Transpile from stdin to stdout\necho \"MATCH (n:Person) RETURN n\" | gsql2rsql transpile -s schema.json\n\n# Transpile from file\ngsql2rsql transpile -s schema.json -i query.cypher -o output.sql\n\n# Disable optimization\ngsql2rsql transpile -s schema.json --no-optimize &lt; query.cypher\n\n# Show AST instead of SQL\ngsql2rsql transpile -s schema.json --format ast &lt; query.cypher\n\n# Show scope debugging info\ngsql2rsql transpile -s schema.json --explain-scopes &lt; query.cypher\n</code></pre>"},{"location":"08-api-reference/#command-parse","title":"Command: <code>parse</code>","text":"<p>Purpose: Parse OpenCypher query to AST (no transpilation)</p> <p>Usage: Bash<pre><code>gsql2rsql parse [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-i, --input PATH</code> File path stdin Input Cypher query file <code>-o, --output PATH</code> File path stdout Output AST file <code>--format</code> Choice <code>tree</code> Output format: <code>tree</code>, <code>json</code> <p>Examples:</p> Bash<pre><code># Parse and show AST tree\necho \"MATCH (n) RETURN n\" | gsql2rsql parse\n\n# Parse from file\ngsql2rsql parse -i query.cypher\n\n# Output as JSON\ngsql2rsql parse --format json &lt; query.cypher\n</code></pre>"},{"location":"08-api-reference/#command-init-schema","title":"Command: <code>init-schema</code>","text":"<p>Purpose: Generate schema template</p> <p>Usage: Bash<pre><code>gsql2rsql init-schema [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-o, --output PATH</code> File path stdout Output schema file <code>--example</code> Choice <code>simple</code> Example type: <code>simple</code>, <code>movie</code>, <code>social</code> <p>Examples:</p> Bash<pre><code># Generate simple schema template\ngsql2rsql init-schema &gt; my_schema.json\n\n# Generate movie graph schema\ngsql2rsql init-schema --example movie -o movie_schema.json\n</code></pre>"},{"location":"08-api-reference/#command-tui","title":"Command: <code>tui</code>","text":"<p>Purpose: Launch interactive TUI (Text User Interface)</p> <p>Usage: Bash<pre><code>gsql2rsql tui [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>--examples PATH</code> File path None YAML examples file <code>--schema PATH</code> File path None Default JSON schema file <p>Examples:</p> Bash<pre><code># Launch TUI with examples\ngsql2rsql tui --examples examples/credit_queries.yaml\n\n# Launch TUI with custom schema\ngsql2rsql tui --schema my_schema.json --examples examples/my_queries.yaml\n</code></pre> <p>TUI Features: - Browse curated examples from YAML files - Live transpilation as you type - Syntax highlighting for Cypher and SQL - Copy SQL to clipboard - Switch between schemas - View AST, logical plan, and scope info - Error highlighting with suggestions</p>"},{"location":"08-api-reference/#schema-format","title":"Schema Format","text":""},{"location":"08-api-reference/#json-schema-structure","title":"JSON Schema Structure","text":"JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"NodeLabel\",\n      \"tableName\": \"catalog.schema.Table\",\n      \"idProperty\": {\n        \"name\": \"id_column\",\n        \"type\": \"int\"\n      },\n      \"properties\": [\n        {\"name\": \"property1\", \"type\": \"string\"},\n        {\"name\": \"property2\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"RELATIONSHIP_TYPE\",\n      \"sourceNode\": \"SourceLabel\",\n      \"sinkNode\": \"SinkLabel\",\n      \"tableName\": \"catalog.schema.EdgeTable\",\n      \"sourceIdProperty\": {\n        \"name\": \"source_id_column\",\n        \"type\": \"int\"\n      },\n      \"sinkIdProperty\": {\n        \"name\": \"sink_id_column\",\n        \"type\": \"int\"\n      },\n      \"properties\": [\n        {\"name\": \"edge_property\", \"type\": \"float\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"08-api-reference/#supported-property-types","title":"Supported Property Types","text":"Type Python Type Databricks SQL Type <code>\"int\"</code> <code>int</code> <code>BIGINT</code> <code>\"string\"</code> <code>str</code> <code>STRING</code> <code>\"float\"</code> <code>float</code> <code>DOUBLE</code> <code>\"boolean\"</code> <code>bool</code> <code>BOOLEAN</code> <code>\"date\"</code> (INFERRED) Date <code>DATE</code> <code>\"timestamp\"</code> (INFERRED) Datetime <code>TIMESTAMP</code>"},{"location":"08-api-reference/#example-schemas","title":"Example Schemas","text":"<p>Simple Social Graph: JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"social.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"KNOWS\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Person\",\n      \"tableName\": \"social.Knows\",\n      \"sourceIdProperty\": {\"name\": \"person1_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"person2_id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"since\", \"type\": \"int\"}\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>Multi-Node Graph (Credit Risk): JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Customer\",\n      \"tableName\": \"credit.Customer\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"credit_score\", \"type\": \"int\"}\n      ]\n    },\n    {\n      \"name\": \"Account\",\n      \"tableName\": \"credit.Account\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"balance\", \"type\": \"float\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"HAS_ACCOUNT\",\n      \"sourceNode\": \"Customer\",\n      \"sinkNode\": \"Account\",\n      \"tableName\": \"credit.CustomerAccount\",\n      \"sourceIdProperty\": {\"name\": \"customer_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"account_id\", \"type\": \"int\"},\n      \"properties\": []\n    }\n  ]\n}\n</code></pre></p>"},{"location":"08-api-reference/#configuration-options","title":"Configuration Options","text":""},{"location":"08-api-reference/#pyprojecttoml","title":"pyproject.toml","text":"<p>Package Metadata: pyproject.toml</p> TOML<pre><code>[project]\nname = \"gsql2rsql\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\n\n[project.dependencies]\nantlr4-python3-runtime = \"&gt;=4.13.0\"\nclick = \"&gt;=8.1.0\"\n# ... more dependencies\n\n[project.scripts]\ngsql2rsql = \"gsql2rsql.cli:main\"\n</code></pre>"},{"location":"08-api-reference/#environment-variables","title":"Environment Variables","text":"<p>INFERRED - Update if implemented</p> Variable Purpose Default <code>GSQL2RSQL_SCHEMA</code> Default schema path None <code>GSQL2RSQL_LOG_LEVEL</code> Logging level <code>INFO</code> <code>GSQL2RSQL_CACHE_DIR</code> Cache directory <code>~/.gsql2rsql/cache</code>"},{"location":"08-api-reference/#exception-types","title":"Exception Types","text":""},{"location":"08-api-reference/#transpilersyntaxerrorexception","title":"<code>TranspilerSyntaxErrorException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Parser encounters syntax error in Cypher query</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>line: int | None</code> \u2014 Line number in query - <code>column: int | None</code> \u2014 Column number in query - <code>query: str | None</code> \u2014 Original query</p> <p>Example: Python<pre><code>from gsql2rsql import OpenCypherParser\nfrom gsql2rsql.common.exceptions import TranspilerSyntaxErrorException\n\nparser = OpenCypherParser()\ntry:\n    ast = parser.parse(\"MATCH (n RETURN n\")  # Missing closing paren\nexcept TranspilerSyntaxErrorException as e:\n    print(f\"Syntax error at line {e.line}, column {e.column}: {e.message}\")\n</code></pre></p>"},{"location":"08-api-reference/#transpilerbindingexception","title":"<code>TranspilerBindingException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Schema binding fails (e.g., undefined node label)</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>entity_name: str | None</code> \u2014 Entity that failed to bind</p> <p>Example: Python<pre><code>from gsql2rsql import LogicalPlan\nfrom gsql2rsql.common.exceptions import TranspilerBindingException\n\ntry:\n    plan = LogicalPlan.process_query_tree(ast, schema)\nexcept TranspilerBindingException as e:\n    print(f\"Schema binding failed: {e.message}\")\n    print(f\"Entity: {e.entity_name}\")\n</code></pre></p>"},{"location":"08-api-reference/#columnresolutionerror","title":"<code>ColumnResolutionError</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Column reference cannot be resolved</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>column_name: str | None</code> \u2014 Column that failed to resolve - <code>suggestions: list[str]</code> \u2014 Suggested column names (Levenshtein distance)</p> <p>Example: Python<pre><code>from gsql2rsql.common.exceptions import ColumnResolutionError\n\ntry:\n    plan.resolve(original_query=query)\nexcept ColumnResolutionError as e:\n    print(f\"Column resolution failed: {e.message}\")\n    if e.suggestions:\n        print(f\"Did you mean: {', '.join(e.suggestions)}?\")\n</code></pre></p>"},{"location":"08-api-reference/#transpilernotsupportedexception","title":"<code>TranspilerNotSupportedException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Unsupported Cypher feature is used</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>feature: str | None</code> \u2014 Unsupported feature name</p> <p>Example: Python<pre><code>from gsql2rsql.common.exceptions import TranspilerNotSupportedException\n\ntry:\n    sql = renderer.render_plan(plan)\nexcept TranspilerNotSupportedException as e:\n    print(f\"Unsupported feature: {e.feature}\")\n    print(f\"Details: {e.message}\")\n</code></pre></p>"},{"location":"08-api-reference/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>01-quickstart.md \u2014 Usage examples</li> <li>02-architecture.md \u2014 Component details</li> <li>07-developer-guide.md \u2014 Extending the API</li> <li>examples/ \u2014 Example schemas and queries</li> </ul>"},{"location":"about/","title":"About","text":""},{"location":"about/#gsql2rsql","title":"gsql2rsql","text":"<p>gsql2rsql is an OpenCypher to Databricks SQL transpiler that enables graph analytics at scale using distributed SQL.</p> <p>This project transforms graph queries into high-performance SQL, allowing you to leverage the power of Databricks while expressing complex graph patterns in the intuitive Cypher query language.</p>"},{"location":"about/#author","title":"Author","text":"<p>Bruno Messias</p> <ul> <li>LinkedIn: bruno-messias-510553193</li> </ul>"},{"location":"about/#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files, to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the conditions of the MIT License.</p> <p>See the LICENSE file for full details.</p>"},{"location":"about/#contributing","title":"Contributing","text":"<p>Contributions are welcome! See the Contributing Guide for details on how to get started.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This document provides a reference for the public API, CLI commands, and configuration options.</p>"},{"location":"api-reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Python API</li> <li>CLI Reference</li> <li>Schema Format</li> <li>Configuration Options</li> <li>Exception Types</li> </ol>"},{"location":"api-reference/#python-api","title":"Python API","text":""},{"location":"api-reference/#public-modules","title":"Public Modules","text":"<p>The transpiler can be used as a Python library. Import from the top-level package:</p> Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\n</code></pre>"},{"location":"api-reference/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/#opencypherparser","title":"<code>OpenCypherParser</code>","text":"<p>Module: <code>gsql2rsql.parser.opencypher_parser</code></p> <p>Purpose: Parse OpenCypher query strings to AST.</p> Python<pre><code>class OpenCypherParser:\n    \"\"\"Parser for OpenCypher queries.\"\"\"\n\n    def parse(self, query: str) -&gt; QueryNode:\n        \"\"\"\n        Parse OpenCypher query string to AST.\n\n        Args:\n            query: OpenCypher query string\n\n        Returns:\n            QueryNode: Root AST node\n\n        Raises:\n            TranspilerSyntaxErrorException: If query has syntax errors\n        \"\"\"\n</code></pre> <p>Example Usage: Python<pre><code>from gsql2rsql import OpenCypherParser\n\nparser = OpenCypherParser()\nast = parser.parse(\"MATCH (n:Person) RETURN n\")\n\nprint(ast.dump_tree())  # Visualize AST\n</code></pre></p>"},{"location":"api-reference/#logicalplan","title":"<code>LogicalPlan</code>","text":"<p>Module: <code>gsql2rsql.planner.logical_plan</code></p> <p>Purpose: Convert AST to logical operator tree.</p> Python<pre><code>class LogicalPlan:\n    \"\"\"Logical query plan with operator tree.\"\"\"\n\n    @staticmethod\n    def process_query_tree(\n        ast: QueryNode,\n        graph_schema: IGraphSchemaProvider\n    ) -&gt; \"LogicalPlan\":\n        \"\"\"\n        Convert AST to logical plan.\n\n        Args:\n            ast: Abstract syntax tree from parser\n            graph_schema: Graph schema provider\n\n        Returns:\n            LogicalPlan: Logical plan with operators\n\n        Raises:\n            TranspilerBindingException: If schema binding fails\n        \"\"\"\n\n    def resolve(self, original_query: str) -&gt; None:\n        \"\"\"\n        Resolve column references and validate schema.\n\n        Args:\n            original_query: Original Cypher query (for error messages)\n\n        Raises:\n            ColumnResolutionError: If column resolution fails\n        \"\"\"\n\n    def is_resolved(self) -&gt; bool:\n        \"\"\"Check if plan has been resolved.\"\"\"\n\n    def dump_graph(self) -&gt; str:\n        \"\"\"Generate text visualization of operator tree.\"\"\"\n\n    @property\n    def starting_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get starting operators (data sources).\"\"\"\n\n    @property\n    def terminal_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get terminal operators (outputs).\"\"\"\n\n    def all_operators(self) -&gt; list[LogicalOperator]:\n        \"\"\"Get all operators in topological order.\"\"\"\n</code></pre> <p>Example Usage: Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan\nfrom gsql2rsql.common.schema import SimpleGraphSchemaProvider\n\n# Parse\nparser = OpenCypherParser()\nast = parser.parse(\"MATCH (n:Person) RETURN n\")\n\n# Build schema\nschema = SimpleGraphSchemaProvider()\n# ... add nodes and edges\n\n# Create logical plan\nplan = LogicalPlan.process_query_tree(ast, schema)\n\n# Resolve column references\nplan.resolve(original_query=\"MATCH (n:Person) RETURN n\")\n\n# Inspect plan\nprint(plan.dump_graph())\n</code></pre></p>"},{"location":"api-reference/#sqlrenderer","title":"<code>SQLRenderer</code>","text":"<p>Module: <code>gsql2rsql.renderer.sql_renderer</code></p> <p>Purpose: Generate Databricks SQL from logical plan.</p> Python<pre><code>class SQLRenderer:\n    \"\"\"SQL code generator for Databricks Spark SQL.\"\"\"\n\n    def __init__(\n        self,\n        db_schema_provider: ISQLDBSchemaProvider,\n        enable_column_pruning: bool = True,\n        config: dict[str, Any] | None = None\n    ):\n        \"\"\"\n        Initialize renderer.\n\n        Args:\n            db_schema_provider: SQL database schema provider\n            enable_column_pruning: Enable column pruning optimization (default: True)\n            config: Optional configuration dictionary for renderer behavior.\n                Supported keys:\n                - 'undirected_strategy': Strategy for undirected relationships.\n                  Values: 'union_edges' (default) or 'or_join'\n        \"\"\"\n\n    def render_plan(self, plan: LogicalPlan) -&gt; str:\n        \"\"\"\n        Generate SQL from logical plan.\n\n        Args:\n            plan: Logical plan (must be resolved)\n\n        Returns:\n            str: Databricks Spark SQL query\n\n        Raises:\n            RuntimeError: If plan is not resolved\n            TranspilerNotSupportedException: If unsupported pattern\n        \"\"\"\n</code></pre> <p>Example Usage (Default): Python<pre><code>from gsql2rsql import SQLRenderer\nfrom gsql2rsql.renderer.schema_provider import SimpleSQLSchemaProvider, SQLTableDescriptor\n\n# Create SQL schema provider\nsql_schema = SimpleSQLSchemaProvider()\nsql_schema.add_node(\n    node_schema,\n    SQLTableDescriptor(table_name=\"dbo.Person\", node_id_columns=[\"id\"])\n)\n\n# Render SQL with default optimizations\nrenderer = SQLRenderer(db_schema_provider=sql_schema)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n</code></pre></p> <p>Example Usage (Custom Configuration): Python<pre><code># Disable undirected relationship optimization (use legacy OR joins)\n# Only recommended for debugging or very small datasets\nrenderer = SQLRenderer(\n    db_schema_provider=sql_schema,\n    config={\"undirected_strategy\": \"or_join\"}  # Default: \"union_edges\"\n)\nsql = renderer.render_plan(plan)\n</code></pre></p> <p>Configuration Options:</p> Key Values Default Description <code>undirected_strategy</code> <code>\"union_edges\"</code> or <code>\"or_join\"</code> <code>\"union_edges\"</code> Strategy for undirected relationships (<code>-[:TYPE]-</code>). <code>\"union_edges\"</code> uses UNION ALL for O(n) performance. <code>\"or_join\"</code> uses OR conditions (legacy, slower). See limitations.md for details."},{"location":"api-reference/#schema-classes","title":"Schema Classes","text":""},{"location":"api-reference/#simplegraphschemaprovider","title":"<code>SimpleGraphSchemaProvider</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> <p>Purpose: In-memory graph schema provider.</p> Python<pre><code>class SimpleGraphSchemaProvider(IGraphSchemaProvider):\n    \"\"\"Simple in-memory graph schema provider.\"\"\"\n\n    def add_node(self, node: NodeSchema) -&gt; None:\n        \"\"\"Add node type to schema.\"\"\"\n\n    def add_edge(self, edge: EdgeSchema) -&gt; None:\n        \"\"\"Add edge type to schema.\"\"\"\n\n    def get_node_schema(self, label: str) -&gt; NodeSchema | None:\n        \"\"\"Get node schema by label.\"\"\"\n\n    def get_edge_schema(self, type_name: str) -&gt; EdgeSchema | None:\n        \"\"\"Get edge schema by type name.\"\"\"\n\n    def all_nodes(self) -&gt; list[NodeSchema]:\n        \"\"\"Get all node schemas.\"\"\"\n\n    def all_edges(self) -&gt; list[EdgeSchema]:\n        \"\"\"Get all edge schemas.\"\"\"\n</code></pre>"},{"location":"api-reference/#nodeschema","title":"<code>NodeSchema</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass NodeSchema(EntitySchema):\n    \"\"\"Schema for a node type.\"\"\"\n\n    name: str\n    properties: list[EntityProperty]\n    node_id_property: EntityProperty\n</code></pre>"},{"location":"api-reference/#edgeschema","title":"<code>EdgeSchema</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass EdgeSchema(EntitySchema):\n    \"\"\"Schema for an edge type.\"\"\"\n\n    name: str\n    source_node: str\n    sink_node: str\n    properties: list[EntityProperty]\n    source_id_property: EntityProperty\n    sink_id_property: EntityProperty\n</code></pre>"},{"location":"api-reference/#entityproperty","title":"<code>EntityProperty</code>","text":"<p>Module: <code>gsql2rsql.common.schema</code></p> Python<pre><code>@dataclass\nclass EntityProperty:\n    \"\"\"Property definition for node or edge.\"\"\"\n\n    name: str\n    python_type: type  # int, str, float, bool, etc.\n</code></pre>"},{"location":"api-reference/#full-example-transpilation-pipeline","title":"Full Example: Transpilation Pipeline","text":"Python<pre><code>from gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom gsql2rsql.common.schema import (\n    SimpleGraphSchemaProvider,\n    NodeSchema,\n    EdgeSchema,\n    EntityProperty,\n)\nfrom gsql2rsql.renderer.schema_provider import (\n    SimpleSQLSchemaProvider,\n    SQLTableDescriptor,\n)\n\n# 1. Define graph schema\ngraph_schema = SimpleGraphSchemaProvider()\n\ngraph_schema.add_node(\n    NodeSchema(\n        name=\"Person\",\n        node_id_property=EntityProperty(\"id\", int),\n        properties=[\n            EntityProperty(\"id\", int),\n            EntityProperty(\"name\", str),\n            EntityProperty(\"age\", int),\n        ],\n    )\n)\n\ngraph_schema.add_edge(\n    EdgeSchema(\n        name=\"KNOWS\",\n        source_node=\"Person\",\n        sink_node=\"Person\",\n        source_id_property=EntityProperty(\"source_id\", int),\n        sink_id_property=EntityProperty(\"target_id\", int),\n        properties=[],\n    )\n)\n\n# 2. Define SQL schema\nsql_schema = SimpleSQLSchemaProvider()\nsql_schema.add_node(\n    NodeSchema(name=\"Person\", node_id_property=EntityProperty(\"id\", int)),\n    SQLTableDescriptor(table_name=\"graph.Person\", node_id_columns=[\"id\"]),\n)\nsql_schema.add_edge(\n    EdgeSchema(\n        name=\"KNOWS\",\n        source_node=\"Person\",\n        sink_node=\"Person\",\n        source_id_property=EntityProperty(\"source_id\", int),\n        sink_id_property=EntityProperty(\"target_id\", int),\n    ),\n    SQLTableDescriptor(\n        table_name=\"graph.Knows\",\n        source_id_columns=[\"source_id\"],\n        sink_id_columns=[\"target_id\"],\n    ),\n)\n\n# 3. Parse query\nquery = \"MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name, f.name\"\nparser = OpenCypherParser()\nast = parser.parse(query)\n\n# 4. Create logical plan\nplan = LogicalPlan.process_query_tree(ast, graph_schema)\n\n# 5. Optimize (optional)\nfrom gsql2rsql.planner.subquery_optimizer import SubqueryFlatteningOptimizer\noptimizer = SubqueryFlatteningOptimizer(enabled=True)\noptimizer.optimize(plan)\n\n# 6. Resolve columns\nplan.resolve(original_query=query)\n\n# 7. Render SQL\nrenderer = SQLRenderer(db_schema_provider=sql_schema)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n</code></pre>"},{"location":"api-reference/#schema-definition-via-python-api","title":"Schema Definition via Python API","text":"<p>You can define schemas programmatically using Python dataclasses instead of JSON files.</p>"},{"location":"api-reference/#using-dataclasses-recommended","title":"Using Dataclasses (Recommended)","text":"Python<pre><code>from gsql2rsql.common.schema import NodeSchema, EdgeSchema, EntityProperty\nfrom gsql2rsql.planner.schema import SimpleGraphSchemaProvider\n\n# Create schema provider\nschema = SimpleGraphSchemaProvider()\n\n# Define nodes\nperson = NodeSchema(\n    name=\"Person\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"age\", data_type=int),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\ncompany = NodeSchema(\n    name=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"industry\", data_type=str),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\n# Define edges\nworks_at = EdgeSchema(\n    name=\"WORKS_AT\",\n    source_node_id=\"Person\",\n    sink_node_id=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"since\", data_type=int),\n    ],\n    source_id_property=EntityProperty(property_name=\"person_id\", data_type=int),\n    sink_id_property=EntityProperty(property_name=\"company_id\", data_type=int)\n)\n\n# Add to schema\nschema.add_node(person)\nschema.add_node(company)\nschema.add_edge(works_at)\n\n# Use with transpiler\nfrom gsql2rsql import OpenCypherParser, LogicalPlan, SQLRenderer\nfrom gsql2rsql.renderer.schema_provider import DatabricksSchemaProvider\n\n# Map entities to SQL tables\nsql_schema = DatabricksSchemaProvider()\nsql_schema.add_table_mapping(\n    entity_id=\"Person\",\n    table_name=\"catalog.mydb.Person\",\n    node_id_columns=[\"id\"],\n    column_mappings={\"id\": \"id\", \"name\": \"name\", \"age\": \"age\"}\n)\nsql_schema.add_table_mapping(\n    entity_id=\"Company\",\n    table_name=\"catalog.mydb.Company\",\n    node_id_columns=[\"id\"]\n)\nsql_schema.add_table_mapping(\n    entity_id=\"Person@WORKS_AT@Company\",\n    table_name=\"catalog.mydb.PersonWorksAt\",\n    column_mappings={\n        \"person_id\": \"person_id\",\n        \"company_id\": \"company_id\",\n        \"since\": \"since\"\n    }\n)\n\n# Transpile\nquery = \"\"\"\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, c.name\n\"\"\"\n\nparser = OpenCypherParser()\nast = parser.parse(query)\n\nplan = LogicalPlan(schema)\nplan.process_query_tree(ast, query)\nplan.resolve(query)\n\nrenderer = SQLRenderer(sql_schema)\nsql = renderer.render_plan(plan)\nprint(sql)\n</code></pre>"},{"location":"api-reference/#using-dictionary-alternative","title":"Using Dictionary (Alternative)","text":"<p>You can also pass dictionaries directly:</p> Python<pre><code>from gsql2rsql.planner.schema import SimpleGraphSchemaProvider\n\nschema_dict = {\n    \"nodes\": [\n        {\n            \"name\": \"Person\",\n            \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n            \"properties\": [\n                {\"name\": \"name\", \"type\": \"string\"},\n                {\"name\": \"age\", \"type\": \"int\"}\n            ]\n        }\n    ],\n    \"edges\": [\n        {\n            \"name\": \"KNOWS\",\n            \"sourceNode\": \"Person\",\n            \"sinkNode\": \"Person\",\n            \"sourceIdProperty\": {\"name\": \"person_id\", \"type\": \"int\"},\n            \"sinkIdProperty\": {\"name\": \"friend_id\", \"type\": \"int\"},\n            \"properties\": []\n        }\n    ]\n}\n\n# Convert to schema objects\n# (Implementation depends on your schema loader)\n</code></pre>"},{"location":"api-reference/#benefits-of-python-api","title":"Benefits of Python API","text":"<ul> <li>Type Safety: Catch errors at development time</li> <li>IDE Support: Autocompletion and type hints</li> <li>Dynamic Generation: Generate schemas from database introspection</li> <li>Validation: Built-in validation in dataclass <code>__post_init__</code></li> <li>Composability: Reuse schema components across projects</li> </ul>"},{"location":"api-reference/#pydantic-support-future","title":"Pydantic Support (Future)","text":"<p>While we currently use Python's built-in <code>dataclasses</code>, we're considering adding Pydantic models for enhanced validation and serialization in future versions. The API would be similar:</p> Python<pre><code># Future Pydantic API (not yet implemented)\nfrom gsql2rsql.schema import NodeModel, EdgeModel\n\nperson = NodeModel(\n    name=\"Person\",\n    properties={\"id\": int, \"name\": str, \"age\": int},\n    id_property=\"id\"\n)\n</code></pre>"},{"location":"api-reference/#cli-reference","title":"CLI Reference","text":""},{"location":"api-reference/#installation","title":"Installation","text":"Bash<pre><code># Install package\npip install gsql2rsql  # (INFERRED - when published)\n\n# Or install from source\ncd cyper2dsql/python\npip install -e .\n\n# Verify installation\ngsql2rsql --version\n</code></pre>"},{"location":"api-reference/#main-command","title":"Main Command","text":"Bash<pre><code>gsql2rsql [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Global Options: - <code>--help</code> \u2014 Show help message - <code>--version</code> \u2014 Show version information</p>"},{"location":"api-reference/#command-transpile","title":"Command: <code>transpile</code>","text":"<p>Purpose: Transpile OpenCypher query to Databricks SQL</p> <p>Usage: Bash<pre><code>gsql2rsql transpile [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-s, --schema PATH</code> File path Required JSON schema file <code>-i, --input PATH</code> File path stdin Input Cypher query file <code>-o, --output PATH</code> File path stdout Output SQL file <code>--optimize / --no-optimize</code> Flag <code>--optimize</code> Enable/disable subquery optimization <code>--resolve / --no-resolve</code> Flag <code>--resolve</code> Enable/disable column resolution <code>--explain-scopes</code> Flag Off Show scope information in output <code>--format</code> Choice <code>sql</code> Output format: <code>sql</code>, <code>ast</code>, <code>plan</code> <p>Examples:</p> Bash<pre><code># Transpile from stdin to stdout\necho \"MATCH (n:Person) RETURN n\" | gsql2rsql transpile -s schema.json\n\n# Transpile from file\ngsql2rsql transpile -s schema.json -i query.cypher -o output.sql\n\n# Disable optimization\ngsql2rsql transpile -s schema.json --no-optimize &lt; query.cypher\n\n# Show AST instead of SQL\ngsql2rsql transpile -s schema.json --format ast &lt; query.cypher\n\n# Show scope debugging info\ngsql2rsql transpile -s schema.json --explain-scopes &lt; query.cypher\n</code></pre>"},{"location":"api-reference/#command-parse","title":"Command: <code>parse</code>","text":"<p>Purpose: Parse OpenCypher query to AST (no transpilation)</p> <p>Usage: Bash<pre><code>gsql2rsql parse [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-i, --input PATH</code> File path stdin Input Cypher query file <code>-o, --output PATH</code> File path stdout Output AST file <code>--format</code> Choice <code>tree</code> Output format: <code>tree</code>, <code>json</code> <p>Examples:</p> Bash<pre><code># Parse and show AST tree\necho \"MATCH (n) RETURN n\" | gsql2rsql parse\n\n# Parse from file\ngsql2rsql parse -i query.cypher\n\n# Output as JSON\ngsql2rsql parse --format json &lt; query.cypher\n</code></pre>"},{"location":"api-reference/#command-init-schema","title":"Command: <code>init-schema</code>","text":"<p>Purpose: Generate schema template</p> <p>Usage: Bash<pre><code>gsql2rsql init-schema [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>-o, --output PATH</code> File path stdout Output schema file <code>--example</code> Choice <code>simple</code> Example type: <code>simple</code>, <code>movie</code>, <code>social</code> <p>Examples:</p> Bash<pre><code># Generate simple schema template\ngsql2rsql init-schema &gt; my_schema.json\n\n# Generate movie graph schema\ngsql2rsql init-schema --example movie -o movie_schema.json\n</code></pre>"},{"location":"api-reference/#command-tui","title":"Command: <code>tui</code>","text":"<p>Purpose: Launch interactive TUI (Text User Interface)</p> <p>Usage: Bash<pre><code>gsql2rsql tui [OPTIONS]\n</code></pre></p> <p>Options:</p> Option Type Default Description <code>--examples PATH</code> File path None YAML examples file <code>--schema PATH</code> File path None Default JSON schema file <p>Examples:</p> Bash<pre><code># Launch TUI with examples\ngsql2rsql tui --examples examples/credit_queries.yaml\n\n# Launch TUI with custom schema\ngsql2rsql tui --schema my_schema.json --examples examples/my_queries.yaml\n</code></pre> <p>TUI Features: - Browse curated examples from YAML files - Live transpilation as you type - Syntax highlighting for Cypher and SQL - Copy SQL to clipboard - Switch between schemas - View AST, logical plan, and scope info - Error highlighting with suggestions</p>"},{"location":"api-reference/#schema-format","title":"Schema Format","text":""},{"location":"api-reference/#json-schema-structure","title":"JSON Schema Structure","text":"JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"NodeLabel\",\n      \"tableName\": \"catalog.schema.Table\",\n      \"idProperty\": {\n        \"name\": \"id_column\",\n        \"type\": \"int\"\n      },\n      \"properties\": [\n        {\"name\": \"property1\", \"type\": \"string\"},\n        {\"name\": \"property2\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"RELATIONSHIP_TYPE\",\n      \"sourceNode\": \"SourceLabel\",\n      \"sinkNode\": \"SinkLabel\",\n      \"tableName\": \"catalog.schema.EdgeTable\",\n      \"sourceIdProperty\": {\n        \"name\": \"source_id_column\",\n        \"type\": \"int\"\n      },\n      \"sinkIdProperty\": {\n        \"name\": \"sink_id_column\",\n        \"type\": \"int\"\n      },\n      \"properties\": [\n        {\"name\": \"edge_property\", \"type\": \"float\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"api-reference/#supported-property-types","title":"Supported Property Types","text":"Type Python Type Databricks SQL Type <code>\"int\"</code> <code>int</code> <code>BIGINT</code> <code>\"string\"</code> <code>str</code> <code>STRING</code> <code>\"float\"</code> <code>float</code> <code>DOUBLE</code> <code>\"boolean\"</code> <code>bool</code> <code>BOOLEAN</code> <code>\"date\"</code> (INFERRED) Date <code>DATE</code> <code>\"timestamp\"</code> (INFERRED) Datetime <code>TIMESTAMP</code>"},{"location":"api-reference/#example-schemas","title":"Example Schemas","text":"<p>Simple Social Graph: JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"social.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"KNOWS\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Person\",\n      \"tableName\": \"social.Knows\",\n      \"sourceIdProperty\": {\"name\": \"person1_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"person2_id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"since\", \"type\": \"int\"}\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>Multi-Node Graph (Credit Risk): JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Customer\",\n      \"tableName\": \"credit.Customer\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"credit_score\", \"type\": \"int\"}\n      ]\n    },\n    {\n      \"name\": \"Account\",\n      \"tableName\": \"credit.Account\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"balance\", \"type\": \"float\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"HAS_ACCOUNT\",\n      \"sourceNode\": \"Customer\",\n      \"sinkNode\": \"Account\",\n      \"tableName\": \"credit.CustomerAccount\",\n      \"sourceIdProperty\": {\"name\": \"customer_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"account_id\", \"type\": \"int\"},\n      \"properties\": []\n    }\n  ]\n}\n</code></pre></p>"},{"location":"api-reference/#configuration-options","title":"Configuration Options","text":""},{"location":"api-reference/#pyprojecttoml","title":"pyproject.toml","text":"<p>Package Metadata: pyproject.toml</p> TOML<pre><code>[project]\nname = \"gsql2rsql\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\n\n[project.dependencies]\nantlr4-python3-runtime = \"&gt;=4.13.0\"\nclick = \"&gt;=8.1.0\"\n# ... more dependencies\n\n[project.scripts]\ngsql2rsql = \"gsql2rsql.cli:main\"\n</code></pre>"},{"location":"api-reference/#environment-variables","title":"Environment Variables","text":"<p>INFERRED - Update if implemented</p> Variable Purpose Default <code>GSQL2RSQL_SCHEMA</code> Default schema path None <code>GSQL2RSQL_LOG_LEVEL</code> Logging level <code>INFO</code> <code>GSQL2RSQL_CACHE_DIR</code> Cache directory <code>~/.gsql2rsql/cache</code>"},{"location":"api-reference/#exception-types","title":"Exception Types","text":""},{"location":"api-reference/#transpilersyntaxerrorexception","title":"<code>TranspilerSyntaxErrorException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Parser encounters syntax error in Cypher query</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>line: int | None</code> \u2014 Line number in query - <code>column: int | None</code> \u2014 Column number in query - <code>query: str | None</code> \u2014 Original query</p> <p>Example: Python<pre><code>from gsql2rsql import OpenCypherParser\nfrom gsql2rsql.common.exceptions import TranspilerSyntaxErrorException\n\nparser = OpenCypherParser()\ntry:\n    ast = parser.parse(\"MATCH (n RETURN n\")  # Missing closing paren\nexcept TranspilerSyntaxErrorException as e:\n    print(f\"Syntax error at line {e.line}, column {e.column}: {e.message}\")\n</code></pre></p>"},{"location":"api-reference/#transpilerbindingexception","title":"<code>TranspilerBindingException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Schema binding fails (e.g., undefined node label)</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>entity_name: str | None</code> \u2014 Entity that failed to bind</p> <p>Example: Python<pre><code>from gsql2rsql import LogicalPlan\nfrom gsql2rsql.common.exceptions import TranspilerBindingException\n\ntry:\n    plan = LogicalPlan.process_query_tree(ast, schema)\nexcept TranspilerBindingException as e:\n    print(f\"Schema binding failed: {e.message}\")\n    print(f\"Entity: {e.entity_name}\")\n</code></pre></p>"},{"location":"api-reference/#columnresolutionerror","title":"<code>ColumnResolutionError</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Column reference cannot be resolved</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>column_name: str | None</code> \u2014 Column that failed to resolve - <code>suggestions: list[str]</code> \u2014 Suggested column names (Levenshtein distance)</p> <p>Example: Python<pre><code>from gsql2rsql.common.exceptions import ColumnResolutionError\n\ntry:\n    plan.resolve(original_query=query)\nexcept ColumnResolutionError as e:\n    print(f\"Column resolution failed: {e.message}\")\n    if e.suggestions:\n        print(f\"Did you mean: {', '.join(e.suggestions)}?\")\n</code></pre></p>"},{"location":"api-reference/#transpilernotsupportedexception","title":"<code>TranspilerNotSupportedException</code>","text":"<p>Module: <code>gsql2rsql.common.exceptions</code></p> <p>Raised when: Unsupported Cypher feature is used</p> <p>Attributes: - <code>message: str</code> \u2014 Error description - <code>feature: str | None</code> \u2014 Unsupported feature name</p> <p>Example: Python<pre><code>from gsql2rsql.common.exceptions import TranspilerNotSupportedException\n\ntry:\n    sql = renderer.render_plan(plan)\nexcept TranspilerNotSupportedException as e:\n    print(f\"Unsupported feature: {e.feature}\")\n    print(f\"Details: {e.message}\")\n</code></pre></p>"},{"location":"api-reference/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>01-quickstart.md \u2014 Usage examples</li> <li>02-architecture.md \u2014 Component details</li> <li>07-developer-guide.md \u2014 Extending the API</li> <li>examples/ \u2014 Example schemas and queries</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":""},{"location":"architecture/#high-level-dataflow","title":"High-Level Dataflow","text":"<pre><code>graph TD\n    A[OpenCypher Query String] --&gt; B[Phase 1: Parser]\n    B --&gt; C[Abstract Syntax Tree]\n    C --&gt; D[Phase 2: Planner]\n    D --&gt; E[Logical Operator Tree + SymbolTable]\n    E --&gt; F[Phase 3: Optimizer]\n    F --&gt; G[Optimized Logical Plan]\n    G --&gt; H[Phase 4: Resolver]\n    H --&gt; I[ResolutionResult]\n    I --&gt; J[Phase 5: Renderer]\n    J --&gt; K[Databricks Spark SQL]\n\n    style B fill:#e1f5ff\n    style D fill:#fff4e1\n    style F fill:#ffe1f5\n    style H fill:#e1ffe1\n    style J fill:#ffe1e1</code></pre>"},{"location":"architecture/#separation-of-concerns-4-phase-design","title":"Separation of Concerns (4-Phase Design)","text":"<p>The transpiler strictly separates concerns across phases to ensure correctness and maintainability. This architecture is documented in CONTRIBUTING.md.</p>"},{"location":"architecture/#design-principle","title":"Design Principle","text":"<p>Each phase has a single responsibility and does not perform the responsibilities of other phases:</p> <ol> <li>Parser: Syntax only (what is valid Cypher?)</li> <li>Planner: Semantics only (what does it mean logically?)</li> <li>Resolver: Validation only (do all references exist?)</li> <li>Renderer: Implementation only (how to generate SQL?)</li> </ol> <p>This prevents: - Parser from needing schema knowledge - Planner from validating column references - Renderer from making semantic decisions</p>"},{"location":"architecture/#phase-1-parser-lexicalsyntactic-analysis","title":"Phase 1: Parser (Lexical/Syntactic Analysis)","text":"<p>Location: src/gsql2rsql/parser/</p>"},{"location":"architecture/#responsibility","title":"Responsibility","text":"<p>Convert OpenCypher query string to Abstract Syntax Tree (AST). Validates syntax only \u2014 does NOT validate semantics, resolve references, or access schema.</p>"},{"location":"architecture/#key-components","title":"Key Components","text":"File Purpose Lines <code>opencypher_parser.py</code> Main entry point, ANTLR runtime invocation ~60 <code>ast.py</code> AST node definitions (50+ node types) ~1500 <code>visitor.py</code> ANTLR visitor pattern implementation ~800 <code>operators.py</code> Operator enums (binary, aggregation, functions) ~300 <code>grammar/</code> ANTLR-generated parser/lexer ~8000"},{"location":"architecture/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: OpenCypher query string</li> <li>Output: <code>QueryNode</code> (root AST node)</li> </ul>"},{"location":"architecture/#key-classes","title":"Key Classes","text":"Python<pre><code># Base AST node\nclass TreeNode:\n    def dump_tree(self, indent: int = 0) -&gt; str\n    def evaluate_type(self, symbol_table: SymbolTable) -&gt; DataType\n\n# Expression nodes\nclass QueryExpression(TreeNode):\n    # Base for all expressions (binary, function calls, literals, etc.)\n\nclass QueryExpressionBinary(QueryExpression):\n    operator: BinaryOperator\n    left: QueryExpression\n    right: QueryExpression\n\nclass QueryExpressionFunction(QueryExpression):\n    function: Function\n    arguments: list[QueryExpression]\n\n# Entity nodes\nclass NodeEntity(TreeNode):\n    variable: str | None\n    labels: list[str]\n\nclass RelationshipEntity(TreeNode):\n    variable: str | None\n    types: list[str]\n    direction: Direction  # LEFT, RIGHT, BOTH\n\n# Query structure\nclass MatchClause(TreeNode):\n    pattern: QueryPattern\n    optional: bool\n\nclass WithClause(TreeNode):\n    projections: list[ProjectionItem]\n    aggregation: bool\n\nclass ReturnClause(TreeNode):\n    projections: list[ProjectionItem]\n    distinct: bool\n    order_by: list[OrderByItem]\n    limit: int | None\n</code></pre>"},{"location":"architecture/#example-parsing","title":"Example Parsing","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name\n</code></pre></p> <p>AST Structure (simplified): Text Only<pre><code>QueryNode\n\u2514\u2500\u2500 SingleQueryNode\n    \u251c\u2500\u2500 MatchClause\n    \u2502   \u2514\u2500\u2500 QueryPattern\n    \u2502       \u251c\u2500\u2500 NodeEntity(variable=\"p\", labels=[\"Person\"])\n    \u2502       \u251c\u2500\u2500 RelationshipEntity(types=[\"KNOWS\"], direction=RIGHT)\n    \u2502       \u2514\u2500\u2500 NodeEntity(variable=\"f\", labels=[\"Person\"])\n    \u251c\u2500\u2500 WhereClause\n    \u2502   \u2514\u2500\u2500 QueryExpressionBinary(\n    \u2502         operator=GREATER_THAN,\n    \u2502         left=QueryExpressionProperty(entity=\"p\", property=\"age\"),\n    \u2502         right=QueryExpressionValue(value=30)\n    \u2502       )\n    \u2514\u2500\u2500 ReturnClause\n        \u2514\u2500\u2500 ProjectionItem(\n              expression=QueryExpressionProperty(entity=\"p\", property=\"name\"),\n              alias=\"name\"\n            )\n</code></pre></p>"},{"location":"architecture/#phase-2-planner-logical-operator-construction","title":"Phase 2: Planner (Logical Operator Construction)","text":"<p>Location: src/gsql2rsql/planner/</p>"},{"location":"architecture/#responsibility_1","title":"Responsibility","text":"<p>Convert AST to logical relational algebra. Builds symbol table tracking variable definitions and scopes. Does NOT resolve column references or validate property access.</p>"},{"location":"architecture/#key-components_1","title":"Key Components","text":"File Purpose Lines <code>logical_plan.py</code> Main orchestrator, AST \u2192 operator conversion ~500 <code>operators.py</code> Logical operator definitions (11 operator types) ~1200 <code>symbol_table.py</code> Variable tracking with nested scopes ~400 <code>path_analyzer.py</code> Variable-length path optimization ~300 <code>schema.py</code> Internal schema representation ~200 <code>subquery_optimizer.py</code> Conservative subquery flattening ~400"},{"location":"architecture/#inputoutput_1","title":"Input/Output","text":"<ul> <li>Input: AST + GraphSchema</li> <li>Output: LogicalPlan (operator tree + symbol table)</li> </ul>"},{"location":"architecture/#logical-operators","title":"Logical Operators","text":"Python<pre><code># Base class\nclass LogicalOperator:\n    input_operators: list[LogicalOperator]\n    output_scope: Schema\n\n    def get_output_scope(self) -&gt; Schema\n    def propagate_data_types(self) -&gt; None\n    def dump_operator(self) -&gt; str\n\n# Terminal node (data source)\nclass StartLogicalOperator(LogicalOperator):\n    pass\n\n# Table/edge scan\nclass DataSourceOperator(StartLogicalOperator):\n    table_name: str\n    entity_schema: EntitySchema\n    filters: list[QueryExpression]  # pushable filters\n\n# Join\nclass JoinOperator(LogicalOperator):\n    join_type: JoinType  # INNER, LEFT, RIGHT, FULL\n    join_keys: list[JoinKeyPair]  # (left_col, right_col)\n\n# Variable-length path (WITH RECURSIVE)\nclass RecursiveTraversalOperator(LogicalOperator):\n    edge_type: str\n    min_depth: int\n    max_depth: int\n    direction: Direction\n    predicates: list[QueryExpression]  # pushable edge filters\n\n# Filters (WHERE)\nclass SelectionOperator(LogicalOperator):\n    predicate: QueryExpression\n\n# Projections (SELECT)\nclass ProjectionOperator(LogicalOperator):\n    projections: list[ProjectionItem]\n    aliases: dict[str, str]\n\n# Aggregation boundary (GROUP BY)\nclass AggregationBoundaryOperator(LogicalOperator):\n    group_by_keys: list[str]\n    aggregations: list[AggregationExpression]\n\n# UNWIND\nclass UnwindOperator(LogicalOperator):\n    list_expression: QueryExpression\n    alias: str\n\n# Set operations\nclass SetOperator(LogicalOperator):\n    set_type: SetOperationType  # UNION, INTERSECT, EXCEPT\n    distinct: bool\n</code></pre>"},{"location":"architecture/#operator-graph-example","title":"Operator Graph Example","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person)\nWHERE p.age &gt; 30\nRETURN p.name, COUNT(*) AS friends\n</code></pre></p> <p>Operator Tree: Text Only<pre><code>ProjectionOperator(projections=[p.name, COUNT(*) AS friends])\n\u2514\u2500\u2500 AggregationBoundaryOperator(group_by=[p])\n    \u2514\u2500\u2500 SelectionOperator(predicate: p.age &gt; 30)\n        \u2514\u2500\u2500 JoinOperator(join_type=INNER, keys=[(source.id, edge.source_id), (sink.id, edge.target_id)])\n            \u251c\u2500\u2500 DataSourceOperator(table=\"Person\", alias=\"source\")\n            \u251c\u2500\u2500 DataSourceOperator(table=\"Knows\", alias=\"edge\")\n            \u2514\u2500\u2500 DataSourceOperator(table=\"Person\", alias=\"sink\")\n</code></pre></p>"},{"location":"architecture/#phase-3-optimizer-conservative-transformations","title":"Phase 3: Optimizer (Conservative Transformations)","text":"<p>Location: src/gsql2rsql/planner/subquery_optimizer.py</p>"},{"location":"architecture/#responsibility_2","title":"Responsibility","text":"<p>Apply conservative transformations to reduce SQL nesting. Only flattens patterns guaranteed to preserve semantics.</p>"},{"location":"architecture/#optimization-rules","title":"Optimization Rules","text":"<p>Safe to Flatten: - \u2705 Selection \u2192 Projection: <code>WHERE</code> can be pushed before <code>SELECT</code> - \u2705 Selection \u2192 Selection: Multiple <code>WHERE</code> clauses can be merged</p> <p>NEVER Flattened: - \u274c Projection \u2192 Projection: Aliases must stay in separate subqueries - \u274c Anything involving aggregation boundaries: Aggregation semantics are fragile</p>"},{"location":"architecture/#configuration","title":"Configuration","text":"<ul> <li>Enabled by default</li> <li>Disable with <code>--no-optimize</code> CLI flag</li> </ul>"},{"location":"architecture/#phase-4-resolver-column-reference-validation","title":"Phase 4: Resolver (Column Reference Validation)","text":"<p>Location: src/gsql2rsql/planner/column_resolver.py</p>"},{"location":"architecture/#responsibility_3","title":"Responsibility","text":"<p>Validate ALL column references before rendering. Query schema for entity properties. Build resolution context for SQL generation.</p>"},{"location":"architecture/#key-components_2","title":"Key Components","text":"File Purpose <code>column_resolver.py</code> Main resolver implementation <code>column_ref.py</code> Resolved reference objects"},{"location":"architecture/#resolution-process","title":"Resolution Process","text":"<ol> <li>Visit operators in topological order</li> <li>Build symbol table tracking available columns at each operator</li> <li>Resolve all column references in expressions</li> <li>Validate property accesses against entity schemas</li> <li>Create <code>ResolvedColumnRef</code> objects with SQL column names</li> </ol>"},{"location":"architecture/#key-classes_1","title":"Key Classes","text":"Python<pre><code>class ResolvedColumnRef:\n    \"\"\"Resolved reference to a single column.\"\"\"\n    entity_name: str\n    property_name: str | None  # None = entire entity\n    sql_column_name: str\n    data_type: DataType\n\nclass ResolvedExpression:\n    \"\"\"Resolved expression with type info.\"\"\"\n    original_expression: QueryExpression\n    resolved_columns: list[ResolvedColumnRef]\n    data_type: DataType\n\nclass ResolutionResult:\n    \"\"\"Complete resolution context.\"\"\"\n    resolved_projections: dict[str, ResolvedProjection]\n    resolved_expressions: dict[int, ResolvedExpression]\n    column_mappings: dict[str, ResolvedColumnRef]\n</code></pre>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<p>The resolver provides rich error messages with suggestions:</p> Text Only<pre><code>Error: Column 'p.nam' not found in scope\nDid you mean: 'p.name'?\nAvailable columns: p.id, p.name, p.age\n</code></pre> <p>Uses Levenshtein distance for typo suggestions.</p>"},{"location":"architecture/#phase-5-renderer-sql-generation","title":"Phase 5: Renderer (SQL Generation)","text":"<p>Location: src/gsql2rsql/renderer/sql_renderer.py</p>"},{"location":"architecture/#responsibility_4","title":"Responsibility","text":"<p>Generate Databricks Spark SQL from logical plan using pre-resolved column references. Handle SQL dialect specifics.</p>"},{"location":"architecture/#key-components_3","title":"Key Components","text":"File Purpose Lines <code>sql_renderer.py</code> Main SQL code generator ~2000 <code>schema_provider.py</code> Database schema provider interface ~200"},{"location":"architecture/#inputoutput_2","title":"Input/Output","text":"<ul> <li>Input: LogicalPlan + ResolutionResult + GraphSchema</li> <li>Output: Databricks Spark SQL string</li> </ul>"},{"location":"architecture/#rendering-by-operator-type","title":"Rendering by Operator Type","text":"Python<pre><code>class SQLRenderer:\n    def render_plan(self, plan: LogicalPlan) -&gt; str:\n        \"\"\"Main entry point.\"\"\"\n\n    def _render_data_source(self, op: DataSourceOperator) -&gt; str:\n        \"\"\"Table scan with optional filters.\"\"\"\n\n    def _render_join(self, op: JoinOperator) -&gt; str:\n        \"\"\"JOIN clause with ON conditions.\"\"\"\n\n    def _render_selection(self, op: SelectionOperator) -&gt; str:\n        \"\"\"WHERE clause.\"\"\"\n\n    def _render_projection(self, op: ProjectionOperator) -&gt; str:\n        \"\"\"SELECT with aliases.\"\"\"\n\n    def _render_aggregation(self, op: AggregationBoundaryOperator) -&gt; str:\n        \"\"\"GROUP BY with HAVING.\"\"\"\n\n    def _render_recursive(self, op: RecursiveTraversalOperator) -&gt; str:\n        \"\"\"WITH RECURSIVE CTE for variable-length paths.\"\"\"\n\n    def _render_unwind(self, op: UnwindOperator) -&gt; str:\n        \"\"\"LATERAL VIEW EXPLODE for UNWIND.\"\"\"\n\n    def _render_set_operator(self, op: SetOperator) -&gt; str:\n        \"\"\"UNION / INTERSECT / EXCEPT.\"\"\"\n</code></pre>"},{"location":"architecture/#special-sql-patterns","title":"Special SQL Patterns","text":"<p>Variable-Length Paths (WITH RECURSIVE): SQL<pre><code>WITH RECURSIVE paths_1 AS (\n    -- Base case: direct edges\n    SELECT e.source_id AS start_node, e.target_id AS end_node, 1 AS depth,\n           ARRAY(e.source_id, e.target_id) AS path,\n           ARRAY(e.source_id) AS visited\n    FROM graph.Knows e\n    UNION ALL\n    -- Recursive case: extend paths\n    SELECT p.start_node, e.target_id AS end_node, p.depth + 1 AS depth,\n           CONCAT(p.path, ARRAY(e.target_id)) AS path,\n           CONCAT(p.visited, ARRAY(e.source_id)) AS visited\n    FROM paths_1 p\n    JOIN graph.Knows e ON p.end_node = e.source_id\n    WHERE p.depth &lt; 5 AND NOT array_contains(p.visited, e.target_id)\n)\n</code></pre></p> <p>OPTIONAL MATCH (LEFT JOIN with COALESCE): SQL<pre><code>LEFT JOIN graph.Person AS f ON p.friend_id = f.id\n-- Later: COALESCE(f.name, 'Unknown')\n</code></pre></p> <p>Edge Collection (STRUCT for properties): SQL<pre><code>COLLECT(STRUCT(edge.amount AS amount, edge.timestamp AS timestamp)) AS edges\n</code></pre></p>"},{"location":"architecture/#directory-structure","title":"Directory Structure","text":"Text Only<pre><code>src/gsql2rsql/\n\u251c\u2500\u2500 __init__.py                    # Public API exports\n\u251c\u2500\u2500 cli.py                         # CLI with TUI (2200 lines)\n\u251c\u2500\u2500 pyspark_executor.py            # Runtime validation\n\u2502\n\u251c\u2500\u2500 parser/                        # Phase 1: Syntax\n\u2502   \u251c\u2500\u2500 opencypher_parser.py      # Main entry point\n\u2502   \u251c\u2500\u2500 ast.py                    # AST nodes (50+ types)\n\u2502   \u251c\u2500\u2500 visitor.py                # ANTLR visitor\n\u2502   \u251c\u2500\u2500 operators.py              # Operator enums\n\u2502   \u2514\u2500\u2500 grammar/                  # ANTLR-generated\n\u2502\n\u251c\u2500\u2500 planner/                       # Phase 2-4: Semantics\n\u2502   \u251c\u2500\u2500 logical_plan.py           # AST \u2192 operators\n\u2502   \u251c\u2500\u2500 operators.py              # 11 operator types\n\u2502   \u251c\u2500\u2500 symbol_table.py           # Variable tracking\n\u2502   \u251c\u2500\u2500 path_analyzer.py          # Path optimization\n\u2502   \u251c\u2500\u2500 schema.py                 # Internal schema\n\u2502   \u251c\u2500\u2500 column_resolver.py        # Phase 4: Resolution\n\u2502   \u251c\u2500\u2500 column_ref.py             # Resolved refs\n\u2502   \u2514\u2500\u2500 subquery_optimizer.py     # Phase 3: Optimization\n\u2502\n\u251c\u2500\u2500 renderer/                      # Phase 5: SQL generation\n\u2502   \u251c\u2500\u2500 sql_renderer.py           # Main codegen (2000 lines)\n\u2502   \u2514\u2500\u2500 schema_provider.py        # Schema interface\n\u2502\n\u2514\u2500\u2500 common/                        # Shared infrastructure\n    \u251c\u2500\u2500 schema.py                 # Graph schema definitions\n    \u251c\u2500\u2500 exceptions.py             # Rich error types\n    \u251c\u2500\u2500 logging.py                # Debug logging\n    \u2514\u2500\u2500 utils.py                  # Utilities\n</code></pre>"},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Why 4-Phase Separation?</li> <li>Correctness: Each phase has clear invariants</li> <li>Testability: Each phase can be tested independently</li> <li>Maintainability: Changes to one phase don't ripple to others</li> <li> <p>Debuggability: Can inspect intermediate representations (AST, operators, resolution)</p> </li> <li> <p>Why ANTLR?</p> </li> <li>Industry-standard parser generator</li> <li>Excellent error recovery</li> <li>Visitor pattern for clean AST construction</li> <li> <p>OpenCypher grammar already exists (adapted here)</p> </li> <li> <p>Why Logical Operators?</p> </li> <li>Relational algebra is well-understood</li> <li>Easy to reason about correctness</li> <li>Enables optimization passes</li> <li> <p>Clear mapping to SQL constructs</p> </li> <li> <p>Why Separate Resolution Phase?</p> </li> <li>Column references can't be validated during planning (schema not fully known)</li> <li>Renderer needs pre-resolved refs to avoid semantic decisions</li> <li> <p>Clear error messages require full context</p> </li> <li> <p>Why Conservative Optimizer?</p> </li> <li>Safety over performance (correctness first)</li> <li>User-transparent (can disable with <code>--no-optimize</code>)</li> <li>Only proven-safe patterns</li> </ol>"},{"location":"architecture/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>03-decision-log.md \u2014 Detailed design rationale</li> <li>CONTRIBUTING.md \u2014 Full architectural boundaries</li> <li>07-developer-guide.md \u2014 How to extend the transpiler</li> <li>src/gsql2rsql/ \u2014 Source code</li> </ul>"},{"location":"cli-commands/","title":"CLI Commands Reference","text":"<p>The <code>gsql2rsql</code> CLI provides commands for transpiling OpenCypher queries to SQL.</p>"},{"location":"cli-commands/#installation-verification","title":"Installation Verification","text":"<p>Check if gsql2rsql is installed correctly:</p> Bash<pre><code>gsql2rsql --version\n</code></pre>"},{"location":"cli-commands/#main-commands","title":"Main Commands","text":""},{"location":"cli-commands/#translate","title":"<code>translate</code>","text":"<p>Transpile an OpenCypher query to SQL.</p> <p>Usage:</p> Bash<pre><code>gsql2rsql translate --schema SCHEMA_FILE [OPTIONS] &lt; query.cypher\n</code></pre> <p>Arguments:</p> <ul> <li><code>--schema PATH</code> (required): Path to the schema JSON file that defines your graph-to-table mapping</li> </ul> <p>Options:</p> <ul> <li><code>--output PATH, -o PATH</code>: Write SQL output to a file instead of stdout</li> <li><code>--format {spark,postgres,generic}</code>: Target SQL dialect (default: spark)</li> <li><code>--indent N</code>: Number of spaces for SQL indentation (default: 2)</li> <li><code>--debug</code>: Enable debug logging</li> <li><code>--no-optimize</code>: Disable query optimization passes</li> </ul> <p>Examples:</p> <p>Basic translation: Bash<pre><code>gsql2rsql translate --schema my_schema.json &lt; query.cypher\n</code></pre></p> <p>Save to file: Bash<pre><code>gsql2rsql translate --schema my_schema.json -o output.sql &lt; query.cypher\n</code></pre></p> <p>From file with piping: Bash<pre><code>cat query.cypher | gsql2rsql translate --schema my_schema.json\n</code></pre></p> <p>With debug output: Bash<pre><code>gsql2rsql translate --schema my_schema.json --debug &lt; query.cypher\n</code></pre></p>"},{"location":"cli-commands/#validate","title":"<code>validate</code>","text":"<p>Validate a schema file without transpiling.</p> <p>Usage:</p> Bash<pre><code>gsql2rsql validate --schema SCHEMA_FILE\n</code></pre> <p>Arguments:</p> <ul> <li><code>--schema PATH</code> (required): Path to the schema JSON file to validate</li> </ul> <p>Example:</p> Bash<pre><code>gsql2rsql validate --schema my_schema.json\n</code></pre> <p>Output: Text Only<pre><code>\u2713 Schema is valid\n- 3 node types defined\n- 2 edge types defined\n- All references resolved\n</code></pre></p>"},{"location":"cli-commands/#explain","title":"<code>explain</code>","text":"<p>Show the query plan for an OpenCypher query.</p> <p>Usage:</p> Bash<pre><code>gsql2rsql explain --schema SCHEMA_FILE [OPTIONS] &lt; query.cypher\n</code></pre> <p>Arguments:</p> <ul> <li><code>--schema PATH</code> (required): Path to the schema JSON file</li> </ul> <p>Options:</p> <ul> <li><code>--format {text,json}</code>: Output format (default: text)</li> <li><code>--verbose</code>: Show detailed operator information</li> </ul> <p>Example:</p> Bash<pre><code>gsql2rsql explain --schema my_schema.json &lt; query.cypher\n</code></pre> <p>Output: Text Only<pre><code>Query Plan:\n\u251c\u2500 ProjectionOperator\n\u2502  \u2514\u2500 FilterOperator (condition: c.industry = 'Technology')\n\u2502     \u2514\u2500 JoinOperator (INNER)\n\u2502        \u251c\u2500 JoinOperator (INNER)\n\u2502        \u2502  \u251c\u2500 DataSourceOperator (Person)\n\u2502        \u2502  \u2514\u2500 DataSourceOperator (PersonWorksAt)\n\u2502        \u2514\u2500 DataSourceOperator (Company)\n</code></pre></p>"},{"location":"cli-commands/#schema-file-format","title":"Schema File Format","text":"<p>The schema file is a JSON document that maps your graph model to SQL tables.</p> <p>Structure:</p> JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"NodeLabel\",\n      \"tableName\": \"catalog.schema.table_name\",\n      \"idProperty\": {\"name\": \"id_column\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"prop_name\", \"type\": \"string\"},\n        {\"name\": \"prop_name2\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"RELATIONSHIP_TYPE\",\n      \"sourceNode\": \"SourceNodeLabel\",\n      \"sinkNode\": \"SinkNodeLabel\",\n      \"tableName\": \"catalog.schema.relationship_table\",\n      \"sourceIdProperty\": {\"name\": \"source_id_column\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"sink_id_column\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"prop_name\", \"type\": \"string\"}\n      ]\n    }\n  ]\n}\n</code></pre> <p>Property Types:</p> <p>Supported types for node and edge properties:</p> <ul> <li><code>string</code> - Text data (maps to SQL VARCHAR/STRING)</li> <li><code>int</code> - Integer numbers (maps to SQL INT/BIGINT)</li> <li><code>long</code> - Large integers (maps to SQL BIGINT)</li> <li><code>float</code> - Floating point numbers (maps to SQL FLOAT)</li> <li><code>double</code> - Double precision floats (maps to SQL DOUBLE)</li> <li><code>boolean</code> - True/false values (maps to SQL BOOLEAN)</li> <li><code>date</code> - Date values (maps to SQL DATE)</li> <li><code>timestamp</code> - Date and time (maps to SQL TIMESTAMP)</li> </ul>"},{"location":"cli-commands/#environment-variables","title":"Environment Variables","text":""},{"location":"cli-commands/#gsql2rsql_log_level","title":"<code>GSQL2RSQL_LOG_LEVEL</code>","text":"<p>Set the logging level.</p> <p>Values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code></p> <p>Default: <code>INFO</code></p> <p>Example:</p> Bash<pre><code>export GSQL2RSQL_LOG_LEVEL=DEBUG\ngsql2rsql translate --schema my_schema.json &lt; query.cypher\n</code></pre>"},{"location":"cli-commands/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code> - Success</li> <li><code>1</code> - General error</li> <li><code>2</code> - Schema validation error</li> <li><code>3</code> - Query parsing error</li> <li><code>4</code> - Query planning error</li> <li><code>5</code> - SQL rendering error</li> </ul>"},{"location":"cli-commands/#common-workflows","title":"Common Workflows","text":""},{"location":"cli-commands/#batch-processing","title":"Batch Processing","text":"<p>Process multiple queries:</p> Bash<pre><code>for query in queries/*.cypher; do\n  echo \"Processing $query...\"\n  gsql2rsql translate --schema schema.json &lt; \"$query\" &gt; \"sql/$(basename $query .cypher).sql\"\ndone\n</code></pre>"},{"location":"cli-commands/#integration-with-pyspark","title":"Integration with PySpark","text":"Python<pre><code>from pyspark.sql import SparkSession\nimport subprocess\n\nspark = SparkSession.builder.appName(\"gsql2rsql\").getOrCreate()\n\n# Transpile query\ncypher_query = \"MATCH (p:Person) RETURN p.name\"\nresult = subprocess.run(\n    [\"gsql2rsql\", \"translate\", \"--schema\", \"schema.json\"],\n    input=cypher_query.encode(),\n    capture_output=True\n)\n\nsql_query = result.stdout.decode()\n\n# Execute on Spark\ndf = spark.sql(sql_query)\ndf.show()\n</code></pre>"},{"location":"cli-commands/#testing-generated-sql","title":"Testing Generated SQL","text":"<p>Validate generated SQL syntax:</p> Bash<pre><code># Using Spark SQL parser\nspark-sql -e \"$(gsql2rsql translate --schema schema.json &lt; query.cypher)\" --dry-run\n\n# Or save and execute\ngsql2rsql translate --schema schema.json &lt; query.cypher &gt; output.sql\nspark-sql -f output.sql\n</code></pre>"},{"location":"cli-commands/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-commands/#schema-validation-errors","title":"Schema Validation Errors","text":"<p>If you get schema validation errors:</p> <ol> <li>Ensure all node/edge names are unique</li> <li>Check that all edge source/sink nodes reference defined node types</li> <li>Verify table names are fully qualified (catalog.schema.table)</li> <li>Ensure ID property types match between nodes and edges</li> </ol>"},{"location":"cli-commands/#query-parsing-errors","title":"Query Parsing Errors","text":"<p>If your OpenCypher query fails to parse:</p> <ol> <li>Check that your query uses supported Cypher syntax</li> <li>See Limitations for unsupported features</li> <li>Use <code>--debug</code> flag to see detailed parsing information</li> </ol>"},{"location":"cli-commands/#sql-rendering-errors","title":"SQL Rendering Errors","text":"<p>If transpilation succeeds but SQL execution fails:</p> <ol> <li>Verify table names in schema match actual tables in your database</li> <li>Check that column names and types match</li> <li>Ensure catalog/schema names are accessible</li> <li>Review generated SQL for any obvious issues</li> </ol>"},{"location":"cli-commands/#see-also","title":"See Also","text":"<ul> <li>Query Translation Guide - How transpilation works</li> <li>API Reference - Python API documentation</li> <li>Examples - Real-world query examples</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>This document combines the architectural boundaries from CONTRIBUTING.md with practical developer workflow guidance.</p>"},{"location":"contributing/#quick-start-for-contributors","title":"Quick Start for Contributors","text":""},{"location":"contributing/#1-set-up-development-environment","title":"1. Set Up Development Environment","text":"Bash<pre><code># Clone repository (INFERRED - update with actual URL)\ngit clone https://github.com/your-org/cyper2dsql.git\ncd cyper2dsql/python\n\n# Create virtual environment with uv\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies (dev mode)\nuv sync --extra dev\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#2-verify-installation","title":"2. Verify Installation","text":"Bash<pre><code># Run fast test suite\nmake test-no-pyspark\n\n# Run linter and type checker\nmake check\n\n# Transpile example query\necho \"MATCH (n:Person) RETURN n\" | uv run gsql2rsql transpile -s examples/schema.json\n</code></pre>"},{"location":"contributing/#3-make-changes","title":"3. Make Changes","text":"<p>Before coding: 1. Read architectural boundaries (critical!) 2. Identify which phase your change affects (parser, planner, resolver, or renderer) 3. Add tests first (TDD recommended)</p> <p>Development loop: 1. Make changes to <code>src/</code> 2. Run <code>make test-no-pyspark</code> (fast feedback) 3. Run <code>make check</code> (lint + typecheck) 4. Add/update golden files if SQL output changes 5. Run full test suite: <code>make test</code></p>"},{"location":"contributing/#4-submit-changes","title":"4. Submit Changes","text":"Bash<pre><code># Format code\nmake format\n\n# Run all checks\nmake check\n\n# Run full test suite (including PySpark)\nmake test\n\n# Commit with descriptive message\ngit add .\ngit commit -m \"feat: add support for map projection in RETURN clause\"\n\n# Push and create PR\ngit push origin feature/map-projection\n</code></pre>"},{"location":"contributing/#architectural-separation-of-concerns","title":"Architectural Separation of Concerns","text":"<p>\u26a0\ufe0f CRITICAL: Read this before making any changes!</p> <p>The transpiler enforces strict separation of concerns across 4 phases. Violating these boundaries will cause architectural degradation and hard-to-debug issues.</p> <p>This section is derived from CONTRIBUTING.md.</p>"},{"location":"contributing/#phase-1-parser-opencypherparser","title":"Phase 1: Parser (OpenCypherParser)","text":"<p>Location: src/gsql2rsql/parser/</p> <p>Input: Cypher query string Output: Abstract Syntax Tree (AST)</p> <p>Responsibility: Lexical/syntactic analysis only</p> <p>Does NOT: - \u274c Validate semantics - \u274c Resolve references - \u274c Access schema - \u274c Perform type checking - \u274c Validate property names</p> <p>Rules: - Parser MUST NOT import from <code>planner/</code>, <code>renderer/</code>, or <code>common/schema.py</code> - Parser MUST NOT call graph schema provider - Parser MUST only validate syntax (grammar rules)</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding a new AST node type for a new Cypher construct\nclass QueryExpressionPatternComprehension(QueryExpression):\n    pattern: QueryPattern\n    where_clause: Optional[WhereClause]\n    projection: QueryExpression\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Parser accessing schema\nclass CypherVisitor:\n    def visitPropertyExpression(self, ctx):\n        entity_name = self._get_entity_name(ctx)\n        # \u274c WRONG: Don't validate property existence here\n        if not self.schema.has_property(entity_name, property_name):\n            raise Exception(\"Property not found\")\n</code></pre></p>"},{"location":"contributing/#phase-2-planning-logicalplan","title":"Phase 2: Planning (LogicalPlan)","text":"<p>Location: src/gsql2rsql/planner/logical_plan.py</p> <p>Input: AST + GraphSchema Output: Logical operator tree + SymbolTable</p> <p>Responsibility: - \u2705 Convert AST to logical operators - \u2705 Build symbol table (variable definitions, scopes) - \u2705 Track entity/value types - \u2705 Handle WITH boundaries, MATCH patterns, aggregations</p> <p>Does NOT: - \u274c Resolve column references - \u274c Validate property access - \u274c Generate SQL - \u274c Query database schema</p> <p>Rules: - Planner CAN import from <code>parser/</code> (uses AST) - Planner CAN import from <code>common/schema.py</code> (uses GraphSchema) - Planner MUST NOT import from <code>renderer/</code> - Planner MUST NOT perform column resolution (that's Phase 4)</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding a new logical operator\nclass WindowOperator(LogicalOperator):\n    \"\"\"Represents a window function (OVER clause).\"\"\"\n    partition_by: list[str]\n    order_by: list[OrderByItem]\n    window_function: WindowFunction\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Planner resolving column references\nclass LogicalPlan:\n    def _process_projection(self, projection: ProjectionItem):\n        # \u274c WRONG: Don't resolve column refs during planning\n        resolved_ref = self._resolve_column_reference(projection.expression)\n        # Column resolution belongs in Phase 4 (Resolver)\n</code></pre></p>"},{"location":"contributing/#phase-3-optimization-subqueryflatteningoptimizer","title":"Phase 3: Optimization (SubqueryFlatteningOptimizer)","text":"<p>Location: src/gsql2rsql/planner/subquery_optimizer.py</p> <p>Input: LogicalPlan Output: Optimized LogicalPlan (modified in-place)</p> <p>Responsibility: - \u2705 Apply conservative transformations - \u2705 Only flatten proven-safe patterns</p> <p>Does NOT: - \u274c Change query semantics - \u274c Resolve columns - \u274c Generate SQL</p> <p>Rules: - Optimizer MUST be conservative (safety first) - Optimizer MUST NOT flatten patterns that could change semantics - Optimizer CAN be disabled by user (<code>--no-optimize</code>)</p>"},{"location":"contributing/#phase-4-resolution-columnresolver","title":"Phase 4: Resolution (ColumnResolver)","text":"<p>Location: src/gsql2rsql/planner/column_resolver.py</p> <p>Input: LogicalPlan + AST + GraphSchema Output: ResolutionResult (resolved column refs, expressions, projections)</p> <p>Responsibility: - \u2705 Validate ALL column references against symbol table - \u2705 Query schema for entity properties - \u2705 Detect entity returns vs property returns - \u2705 Track property availability across boundaries - \u2705 Build ResolvedColumnRef/ResolvedExpression structures</p> <p>Does NOT: - \u274c Generate SQL - \u274c Modify logical plan structure - \u274c Perform optimizations</p> <p>Rules: - Resolver CAN import from <code>parser/</code>, <code>planner/</code>, <code>common/</code> - Resolver MUST NOT import from <code>renderer/</code> - Resolver MUST validate ALL column refs before SQL generation - Resolver MUST provide rich error messages with suggestions</p> <p>Example Valid Change: Python<pre><code># \u2705 Improving error messages with better suggestions\nclass ColumnResolver:\n    def _suggest_similar_columns(self, invalid_name: str, available: list[str]) -&gt; list[str]:\n        # Use Levenshtein distance to suggest typo fixes\n        distances = [(name, levenshtein(invalid_name, name)) for name in available]\n        return [name for name, dist in sorted(distances, key=lambda x: x[1]) if dist &lt;= 2]\n</code></pre></p>"},{"location":"contributing/#phase-5-rendering-sqlrenderer","title":"Phase 5: Rendering (SQLRenderer)","text":"<p>Location: src/gsql2rsql/renderer/sql_renderer.py</p> <p>Input: LogicalPlan + ResolutionResult + GraphSchema Output: SQL string</p> <p>Responsibility: - \u2705 Generate SQL from logical plan - \u2705 Use pre-resolved column references - \u2705 Handle SQL dialect specifics</p> <p>Does NOT: - \u274c Resolve columns - \u274c Validate references - \u274c Make semantic decisions</p> <p>Rules: - Renderer CAN import from all phases (uses everything) - Renderer MUST use <code>ResolutionResult</code> for all column refs - Renderer MUST NOT resolve columns itself - Renderer MUST NOT perform semantic validation</p> <p>Example Valid Change: Python<pre><code># \u2705 Adding support for new SQL function\nclass SQLRenderer:\n    def _render_function(self, func: Function, args: list[str]) -&gt; str:\n        if func == Function.RTRIM:\n            # New function mapping\n            return f\"RTRIM({', '.join(args)})\"\n        # ... existing functions\n</code></pre></p> <p>Example Invalid Change: Python<pre><code># \u274c WRONG: Renderer resolving columns\nclass SQLRenderer:\n    def _render_property_access(self, entity: str, property: str) -&gt; str:\n        # \u274c WRONG: Don't resolve property here\n        if not self.schema.has_property(entity, property):\n            raise Exception(\"Property not found\")\n        # Resolution should already be done in Phase 4\n</code></pre></p>"},{"location":"contributing/#branch-and-pr-workflow","title":"Branch and PR Workflow","text":""},{"location":"contributing/#branch-naming","title":"Branch Naming","text":"<p>Use descriptive branch names with prefixes:</p> <ul> <li><code>feat/</code> \u2014 New feature (e.g., <code>feat/add-shortest-path</code>)</li> <li><code>fix/</code> \u2014 Bug fix (e.g., <code>fix/aggregation-column-names</code>)</li> <li><code>refactor/</code> \u2014 Code refactoring (e.g., <code>refactor/simplify-resolver</code>)</li> <li><code>test/</code> \u2014 Test additions (e.g., <code>test/add-optional-match-tests</code>)</li> <li><code>docs/</code> \u2014 Documentation (e.g., <code>docs/update-quickstart</code>)</li> <li><code>chore/</code> \u2014 Maintenance (e.g., <code>chore/update-dependencies</code>)</li> </ul>"},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<p>Use conventional commits format:</p> Text Only<pre><code>&lt;type&gt;: &lt;short description&gt;\n\n&lt;optional longer description&gt;\n\n&lt;optional footer&gt;\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>refactor</code>, <code>test</code>, <code>docs</code>, <code>chore</code>, <code>perf</code>, <code>style</code></p> <p>Examples: Text Only<pre><code>feat: add support for CASE expression in RETURN clause\n\nImplements CASE WHEN ... THEN ... ELSE ... END syntax in projections.\nAdds new AST node QueryExpressionCaseExpression and rendering logic.\n\nCloses #42\n</code></pre></p> Text Only<pre><code>fix: preserve column names after aggregation boundaries\n\nColumn names were being lost when entity properties were projected after\nGROUP BY. Now uses full qualified names (_gsql2rsql_entity_property).\n\nFixes #51\n</code></pre>"},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<p>Before submitting a PR:</p> <ul> <li> All tests pass: <code>make test</code></li> <li> Code is formatted: <code>make format</code></li> <li> No lint errors: <code>make lint</code></li> <li> Type checking passes: <code>make typecheck</code></li> <li> Added tests for new features</li> <li> Updated golden files if SQL output changed</li> <li> Added/updated documentation</li> <li> Commit messages follow conventional format</li> <li> PR description explains the change and motivation</li> </ul>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<p>Formatter: Ruff (configured in pyproject.toml)</p> <p>Line length: 100 characters</p> <p>Imports: Sorted with isort (part of Ruff)</p> <p>Type hints: Required for all functions (strict mypy)</p> <p>Docstrings: Google style (recommended, not enforced)</p>"},{"location":"contributing/#running-formatters-and-linters","title":"Running Formatters and Linters","text":"Bash<pre><code># Auto-format code\nmake format\n\n# Check formatting (CI mode)\nmake format-check\n\n# Run linter\nmake lint\n\n# Auto-fix linting issues\nmake lint-fix\n\n# Run type checker\nmake typecheck\n\n# Run all checks\nmake check\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Required: All function signatures must have type hints</p> <p>Example: Python<pre><code># \u2705 Good\ndef render_expression(\n    self,\n    expr: QueryExpression,\n    context: RenderContext\n) -&gt; str:\n    \"\"\"Render an expression to SQL string.\"\"\"\n    ...\n\n# \u274c Bad (missing type hints)\ndef render_expression(self, expr, context):\n    ...\n</code></pre></p> <p>Mypy Configuration: See pyproject.toml <code>[tool.mypy]</code> section</p> <ul> <li><code>strict = true</code> (all strict checks enabled)</li> <li><code>disallow_untyped_defs = true</code> (no untyped functions)</li> <li><code>warn_return_any = true</code> (warn on <code>Any</code> returns)</li> </ul>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"Item Convention Example Classes PascalCase <code>LogicalOperator</code>, <code>ColumnResolver</code> Functions snake_case <code>render_plan()</code>, <code>resolve_column()</code> Constants UPPER_SNAKE_CASE <code>MAX_DEPTH</code>, <code>DEFAULT_TIMEOUT</code> Private methods <code>_snake_case</code> <code>_render_helper()</code> Type aliases PascalCase <code>EntityMap</code>, <code>ColumnMapping</code>"},{"location":"contributing/#file-organization","title":"File Organization","text":"<p>Within each module file: 1. Module docstring 2. Imports (stdlib, third-party, local) 3. Constants 4. Type aliases 5. Helper functions 6. Main classes 7. Module-level functions (if any)</p> <p>Example: Python<pre><code>\"\"\"Module for SQL rendering logic.\n\nThis module contains the SQLRenderer class which converts logical plans\nto Databricks Spark SQL.\n\"\"\"\n\n# Standard library\nfrom typing import Any, Optional\n\n# Third-party\nfrom antlr4 import InputStream\n\n# Local\nfrom gsql2rsql.planner.operators import LogicalOperator\nfrom gsql2rsql.planner.column_ref import ResolvedColumnRef\n\n# Constants\nMAX_RECURSION_DEPTH = 100\nDEFAULT_INDENT = \"  \"\n\n# Type aliases\nOperatorMap = dict[str, LogicalOperator]\n\n# Classes\nclass SQLRenderer:\n    ...\n</code></pre></p>"},{"location":"contributing/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"contributing/#adding-a-new-cypher-feature","title":"Adding a New Cypher Feature","text":"<p>Example: Add support for <code>range()</code> function</p> <ol> <li> <p>Phase 1 (Parser): Add to grammar or AST if needed    Python<pre><code># If new function, add to Function enum in operators.py\nclass Function(Enum):\n    RANGE = \"range\"  # Generate sequence of integers\n</code></pre></p> </li> <li> <p>Phase 2 (Planner): Handle in operator construction (if needed)    Python<pre><code># Usually functions are just expressions, no special operator needed\n</code></pre></p> </li> <li> <p>Phase 4 (Resolver): Type checking (if needed)    Python<pre><code># Add type evaluation rule\ndef _evaluate_function_type(self, func: Function, args: list[DataType]) -&gt; DataType:\n    if func == Function.RANGE:\n        return DataType.LIST_INT\n</code></pre></p> </li> <li> <p>Phase 5 (Renderer): Add SQL generation    Python<pre><code>class SQLRenderer:\n    def _render_function(self, func: Function, args: list[str]) -&gt; str:\n        if func == Function.RANGE:\n            # Databricks: sequence(start, stop, step)\n            return f\"sequence({args[0]}, {args[1]})\"\n</code></pre></p> </li> <li> <p>Add Tests: Golden file test + unit tests    Bash<pre><code># Create test_46_range_function.py\n# Generate golden file\nmake dump-sql-save ID=46 NAME=range_function\n</code></pre></p> </li> </ol>"},{"location":"contributing/#fixing-a-bug","title":"Fixing a Bug","text":"<p>Example: Fix incorrect null handling in OPTIONAL MATCH</p> <ol> <li> <p>Write failing test first (TDD)    Python<pre><code># tests/test_optional_match_null_bug.py\ndef test_optional_match_null_handling():\n    cypher = \"MATCH (p:Person) OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f) RETURN p.name, f.name\"\n    sql = transpile(cypher)\n    # Should use COALESCE for f.name\n    assert \"COALESCE\" in sql\n</code></pre></p> </li> <li> <p>Run test (should fail)    Bash<pre><code>pytest tests/test_optional_match_null_bug.py -v\n</code></pre></p> </li> <li> <p>Identify the phase where the bug is (use <code>--explain-scopes</code> for debugging)    Bash<pre><code>echo \"MATCH (p) OPTIONAL MATCH (p)-[:KNOWS]-&gt;(f) RETURN p.name, f.name\" | \\\n  uv run gsql2rsql transpile -s examples/schema.json --explain-scopes\n</code></pre></p> </li> <li> <p>Fix the bug in the appropriate phase    Python<pre><code># src/gsql2rsql/renderer/sql_renderer.py\ndef _render_optional_property(self, ref: ResolvedColumnRef) -&gt; str:\n    if ref.is_from_optional_match:\n        # \u2705 Add COALESCE for null handling\n        return f\"COALESCE({ref.sql_column_name}, NULL)\"\n    return ref.sql_column_name\n</code></pre></p> </li> <li> <p>Run tests (should pass now)    Bash<pre><code>pytest tests/test_optional_match_null_bug.py -v\nmake test-no-pyspark\n</code></pre></p> </li> <li> <p>Update golden files if SQL output changed    Bash<pre><code>make diff-all  # Review changes\nmake dump-sql-save ID=09 NAME=optional_match  # Update if correct\n</code></pre></p> </li> </ol>"},{"location":"contributing/#debugging-transpilation-issues","title":"Debugging Transpilation Issues","text":"<p>Step 1: Isolate the query Bash<pre><code># Save problematic query to file\necho \"MATCH (n:Node) WHERE n.prop &gt; 10 RETURN n\" &gt; debug_query.cypher\n</code></pre></p> <p>Step 2: Inspect AST Bash<pre><code>uv run gsql2rsql parse -i debug_query.cypher\n</code></pre></p> <p>Step 3: Inspect logical plan Python<pre><code># In Python REPL or script\nfrom gsql2rsql import OpenCypherParser, LogicalPlan\nfrom gsql2rsql.common.schema import SimpleGraphSchemaProvider\n\nparser = OpenCypherParser()\nast = parser.parse(open(\"debug_query.cypher\").read())\n\nschema = SimpleGraphSchemaProvider()\n# ... add schema\n\nplan = LogicalPlan.process_query_tree(ast, schema)\nprint(plan.dump_graph())  # Visualize operator tree\n</code></pre></p> <p>Step 4: Check scopes Bash<pre><code>uv run gsql2rsql transpile -s examples/schema.json -i debug_query.cypher --explain-scopes\n</code></pre></p> <p>Step 5: Enable verbose logging (INFERRED - check if implemented) Python<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p>"},{"location":"contributing/#grammar-changes","title":"Grammar Changes","text":""},{"location":"contributing/#when-to-modify-grammar","title":"When to Modify Grammar","text":"<p>Rare: Grammar changes are needed only for: - New Cypher syntax not currently supported - Parser error recovery improvements - Performance optimizations</p> <p>Not needed for: - New functions (add to <code>Function</code> enum in <code>operators.py</code>) - New operators (add to <code>BinaryOperator</code> enum) - Semantic changes (those belong in planner/renderer)</p>"},{"location":"contributing/#modifying-the-grammar","title":"Modifying the Grammar","text":"<p>File: CypherParser.g4 (INFERRED - root level)</p> <p>After changes: Bash<pre><code># Regenerate parser\nmake grammar\n\n# Verify grammar compiles\njavac -version  # Ensure Java is installed\n\n# Run parser tests\nmake test-parser  # (INFERRED command)\n</code></pre></p> <p>Note: Generated files in <code>src/gsql2rsql/parser/grammar/</code> will change. Commit them.</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>INFERRED - Update with actual process</p>"},{"location":"contributing/#versioning","title":"Versioning","text":"<p>Follow Semantic Versioning (SemVer): <code>MAJOR.MINOR.PATCH</code></p> <ul> <li>MAJOR: Breaking changes (API, CLI, SQL output incompatibility)</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li> All tests pass on main branch</li> <li> CHANGELOG.md updated (INFERRED - if exists)</li> <li> Version bumped in pyproject.toml</li> <li> Documentation updated</li> <li> Tag created: <code>git tag -a v0.2.0 -m \"Release v0.2.0\"</code></li> <li> Build package: <code>make build</code></li> <li> Publish to PyPI: <code>make publish</code> (or <code>make publish-test</code> for TestPyPI)</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":""},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>Architecture: 02-architecture.md</li> <li>Decisions: 03-decision-log.md</li> <li>Limitations: 04-limitations.md</li> <li>Developer Guide: 07-developer-guide.md</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>INFERRED - Update with actual channels</p> <ul> <li>Issues: GitHub Issues (bug reports, feature requests)</li> <li>Discussions: GitHub Discussions (questions, ideas)</li> <li>Slack/Discord: (if available)</li> </ul>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Include: 1. Cypher query that causes the issue 2. Schema definition (JSON) 3. Expected SQL output (or behavior) 4. Actual SQL output (or error message) 5. Transpiler version: <code>uv run gsql2rsql --version</code> 6. Python version: <code>python --version</code></p> <p>Template: Markdown<pre><code>## Bug Report\n\n**Cypher Query:**\n```cypher\nMATCH (n:Node) WHERE n.prop &gt; 10 RETURN n\n</code></pre></p> <p>Schema: JSON<pre><code>{ \"nodes\": [ ... ] }\n</code></pre></p> <p>Expected SQL: SQL<pre><code>SELECT * FROM Node WHERE prop &gt; 10\n</code></pre></p> <p>Actual SQL: SQL<pre><code>SELECT * FROM Node  -- WHERE clause missing!\n</code></pre></p> <p>Environment: - Transpiler version: 0.1.0 - Python version: 3.12.1 - OS: Ubuntu 22.04 ```</p>"},{"location":"contributing/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>07-developer-guide.md \u2014 Detailed extension guide</li> <li>05-testing-and-examples.md \u2014 Testing patterns</li> <li>02-architecture.md \u2014 Component details</li> </ul>"},{"location":"decision-log/","title":"Architectural Decision Log","text":"<p>This document captures important design decisions, their rationale, and where they're implemented in the codebase.</p>"},{"location":"decision-log/#decision-1-strict-4-phase-separation-of-concerns","title":"Decision 1: Strict 4-Phase Separation of Concerns","text":"<p>Status: \u2705 Adopted (documented in CONTRIBUTING.md)</p>"},{"location":"decision-log/#context","title":"Context","text":"<p>Transpilers often mix parsing, semantic analysis, and code generation, leading to tight coupling and hard-to-debug issues.</p>"},{"location":"decision-log/#decision","title":"Decision","text":"<p>Enforce strict separation across 4 phases: 1. Parser: Syntax only (no schema access) 2. Planner: Semantics only (no column validation) 3. Resolver: Validation only (no SQL generation) 4. Renderer: Code generation only (no semantic decisions)</p>"},{"location":"decision-log/#rationale","title":"Rationale","text":"<ul> <li>Correctness: Each phase has clear invariants and responsibilities</li> <li>Testability: Can test each phase independently with mocked inputs</li> <li>Debuggability: Can inspect intermediate representations (AST, operators, resolution results)</li> <li>Maintainability: Changes to one phase don't cascade to others</li> </ul>"},{"location":"decision-log/#trade-offs","title":"Trade-offs","text":"<ul> <li>More code: Separate data structures for each phase (AST, operators, resolved refs)</li> <li>More memory: Keep AST + operators + resolution in memory</li> <li>Performance: Multiple passes over the data structure (acceptable for query compilation)</li> </ul>"},{"location":"decision-log/#implementation","title":"Implementation","text":"<ul> <li>Parser: src/gsql2rsql/parser/</li> <li>Planner: src/gsql2rsql/planner/logical_plan.py</li> <li>Resolver: src/gsql2rsql/planner/column_resolver.py</li> <li>Renderer: src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"decision-log/#decision-2-antlr-for-parsing-vs-hand-written-parser","title":"Decision 2: ANTLR for Parsing (vs. Hand-Written Parser)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_1","title":"Context","text":"<p>OpenCypher grammar is complex (multiple expression types, operator precedence, pattern matching). Need robust error recovery.</p>"},{"location":"decision-log/#decision_1","title":"Decision","text":"<p>Use ANTLR 4.13+ parser generator with visitor pattern to build AST.</p>"},{"location":"decision-log/#rationale_1","title":"Rationale","text":"<ul> <li>Correctness: Proven grammar, handles precedence and associativity correctly</li> <li>Maintainability: Grammar is declarative and separate from code</li> <li>Error recovery: ANTLR has excellent error reporting and recovery</li> <li>Standards: OpenCypher community provides reference ANTLR grammars</li> </ul>"},{"location":"decision-log/#trade-offs_1","title":"Trade-offs","text":"<ul> <li>Dependency: Requires ANTLR runtime (<code>antlr4-python3-runtime&gt;=4.13.0</code>)</li> <li>Build step: Must regenerate parser when grammar changes (<code>make grammar</code>)</li> <li>Generated code: ~8000 lines of generated parser code in repo</li> </ul>"},{"location":"decision-log/#implementation_1","title":"Implementation","text":"<ul> <li>Grammar: CypherParser.g4 (INFERRED: root-level grammar file)</li> <li>Generated files: src/gsql2rsql/parser/grammar/</li> <li>Visitor: src/gsql2rsql/parser/visitor.py</li> <li>Entry point: src/gsql2rsql/parser/opencypher_parser.py</li> </ul>"},{"location":"decision-log/#decision-3-logical-operators-vs-direct-ast-sql","title":"Decision 3: Logical Operators (vs. Direct AST \u2192 SQL)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_2","title":"Context","text":"<p>Direct AST-to-SQL translation is fragile and hard to optimize. SQL generation logic becomes deeply nested.</p>"},{"location":"decision-log/#decision_2","title":"Decision","text":"<p>Introduce a logical operator layer (relational algebra) between AST and SQL.</p>"},{"location":"decision-log/#rationale_2","title":"Rationale","text":"<ul> <li>Optimization: Can apply transformations on operators before SQL generation</li> <li>Clarity: Operators have clear semantics (DataSource, Join, Selection, Projection, etc.)</li> <li>Correctness: Easier to reason about query semantics in relational algebra terms</li> <li>Extensibility: New SQL backends can reuse operator layer</li> </ul>"},{"location":"decision-log/#trade-offs_2","title":"Trade-offs","text":"<ul> <li>Complexity: Another intermediate representation</li> <li>Memory: Operator tree + AST in memory simultaneously during planning</li> </ul>"},{"location":"decision-log/#implementation_2","title":"Implementation","text":"<ul> <li>Operators: src/gsql2rsql/planner/operators.py</li> <li>Conversion: src/gsql2rsql/planner/logical_plan.py:LogicalPlan.process_query_tree() (~line 100+)</li> <li>Rendering: src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"decision-log/#decision-4-symbol-table-with-nested-scopes","title":"Decision 4: Symbol Table with Nested Scopes","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_3","title":"Context","text":"<p>Cypher <code>WITH</code> clauses create new scopes. Variables can be redefined. Need to track what's available at each point in the query.</p>"},{"location":"decision-log/#decision_3","title":"Decision","text":"<p>Build a symbol table during planning with support for nested scopes. Each <code>WITH</code> creates a new scope.</p>"},{"location":"decision-log/#rationale_3","title":"Rationale","text":"<ul> <li>Correctness: Prevents variable name collisions</li> <li>Error messages: Can show available variables when reference is invalid</li> <li>Type tracking: Track entity vs. value types for each variable</li> </ul>"},{"location":"decision-log/#trade-offs_3","title":"Trade-offs","text":"<ul> <li>Complexity: Scope management adds cognitive overhead</li> <li>Edge cases: Handling variable shadowing, scope boundaries</li> </ul>"},{"location":"decision-log/#implementation_3","title":"Implementation","text":"<ul> <li>Symbol table: src/gsql2rsql/planner/symbol_table.py</li> <li>Usage in planner: src/gsql2rsql/planner/logical_plan.py (built during AST traversal)</li> <li>Scope tracking: <code>SymbolTable.enter_scope()</code> / <code>SymbolTable.exit_scope()</code> methods</li> </ul>"},{"location":"decision-log/#decision-5-separate-resolution-phase-vs-resolving-during-planning","title":"Decision 5: Separate Resolution Phase (vs. Resolving During Planning)","text":"<p>Status: \u2705 Adopted (documented in CONTRIBUTING.md)</p>"},{"location":"decision-log/#context_4","title":"Context","text":"<p>Column references can't be validated during planning because: 1. Schema propagation happens bottom-up 2. Aggregation boundaries affect column availability 3. Need full operator graph to compute scopes</p>"},{"location":"decision-log/#decision_4","title":"Decision","text":"<p>Add a separate resolution phase after planning. Walk operator graph, build scopes, resolve all column references.</p>"},{"location":"decision-log/#rationale_4","title":"Rationale","text":"<ul> <li>Correctness: Full context available for validation</li> <li>Error messages: Can provide suggestions using Levenshtein distance</li> <li>Separation: Renderer doesn't need to make semantic decisions</li> </ul>"},{"location":"decision-log/#trade-offs_4","title":"Trade-offs","text":"<ul> <li>Extra pass: Must visit operator graph twice (once for planning, once for resolution)</li> <li>Memory: Store ResolutionResult alongside LogicalPlan</li> </ul>"},{"location":"decision-log/#implementation_4","title":"Implementation","text":"<ul> <li>Resolver: src/gsql2rsql/planner/column_resolver.py:ColumnResolver.resolve() (~line 50+)</li> <li>Invocation: src/gsql2rsql/planner/logical_plan.py:LogicalPlan.resolve()</li> <li>Resolution data: src/gsql2rsql/planner/column_ref.py</li> </ul>"},{"location":"decision-log/#decision-6-conservative-optimizer-vs-aggressive-flattening","title":"Decision 6: Conservative Optimizer (vs. Aggressive Flattening)","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_5","title":"Context","text":"<p>SQL subquery nesting can be deep, affecting readability and performance. But aggressive flattening can change semantics.</p>"},{"location":"decision-log/#decision_5","title":"Decision","text":"<p>Only flatten patterns proven safe: - \u2705 Selection \u2192 Projection (WHERE before SELECT) - \u2705 Selection \u2192 Selection (merge WHERE clauses) - \u274c Projection \u2192 Projection (aliases must stay in separate subqueries) - \u274c Anything involving aggregation</p>"},{"location":"decision-log/#rationale_5","title":"Rationale","text":"<ul> <li>Safety first: Wrong results are worse than slower SQL</li> <li>Transparency: User can disable with <code>--no-optimize</code></li> <li>Debuggability: Easier to debug generated SQL when flattening is conservative</li> </ul>"},{"location":"decision-log/#trade-offs_5","title":"Trade-offs","text":"<ul> <li>Performance: May generate more subqueries than necessary</li> <li>SQL readability: Nested queries can be hard to read</li> </ul>"},{"location":"decision-log/#implementation_5","title":"Implementation","text":"<ul> <li>Optimizer: src/gsql2rsql/planner/subquery_optimizer.py:SubqueryFlatteningOptimizer</li> <li>Invocation: CLI flag <code>--optimize</code> (enabled by default, can disable with <code>--no-optimize</code>)</li> <li>Rules: <code>_can_flatten_selection_projection()</code>, <code>_can_flatten_selection_selection()</code> methods</li> </ul>"},{"location":"decision-log/#decision-7-with-recursive-for-variable-length-paths","title":"Decision 7: WITH RECURSIVE for Variable-Length Paths","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_6","title":"Context","text":"<p>Variable-length paths (<code>-[:KNOWS*1..5]-&gt;</code>) require graph traversal. Options: 1. Generate PySpark DataFrame code (requires runtime, not pure SQL) 2. Use <code>WITH RECURSIVE</code> CTE (SQL standard, Databricks 17+) 3. Expand paths at transpile time (exponential blowup)</p>"},{"location":"decision-log/#decision_6","title":"Decision","text":"<p>Use <code>WITH RECURSIVE</code> CTEs for variable-length paths. Generate BFS/DFS traversal in SQL.</p>"},{"location":"decision-log/#rationale_6","title":"Rationale","text":"<ul> <li>Pure SQL: No runtime dependencies beyond Databricks SQL</li> <li>Scalability: Recursive CTE scales to large graphs</li> <li>Correctness: Cycle detection with <code>visited</code> array</li> <li>Standard: SQL standard feature, widely supported</li> </ul>"},{"location":"decision-log/#trade-offs_6","title":"Trade-offs","text":"<ul> <li>Databricks 17+ only: Not supported in older runtimes</li> <li>Performance: Recursive CTEs can be slow for deep/wide graphs</li> <li>Complexity: Generated SQL is verbose</li> </ul>"},{"location":"decision-log/#implementation_6","title":"Implementation","text":"<ul> <li>Operator: src/gsql2rsql/planner/operators.py:RecursiveTraversalOperator</li> <li>Rendering: src/gsql2rsql/renderer/sql_renderer.py:_render_recursive() (~line 800+)</li> <li>Example: See tests/output/expected/21_variable_length_zero.sql</li> </ul>"},{"location":"decision-log/#decision-8-path-analyzer-for-edge-optimization","title":"Decision 8: Path Analyzer for Edge Optimization","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_7","title":"Context","text":"<p>Variable-length paths generate edge collections by default. But if the user doesn't access <code>relationships(path)</code>, we're collecting edges for nothing.</p>"},{"location":"decision-log/#decision_7","title":"Decision","text":"<p>Analyze usage of <code>relationships(path)</code> function. If not used, skip edge collection in recursive CTE.</p>"},{"location":"decision-log/#rationale_7","title":"Rationale","text":"<ul> <li>Performance: Collecting edges is expensive (ARRAY CONCAT on every recursion)</li> <li>Memory: Edge arrays grow linearly with path depth</li> <li>Optimization: Zero-cost abstraction when edges aren't needed</li> </ul>"},{"location":"decision-log/#trade-offs_7","title":"Trade-offs","text":"<ul> <li>Complexity: Must analyze expression trees to detect usage</li> <li>Correctness: Must handle ALL/ANY predicates that access edges</li> </ul>"},{"location":"decision-log/#implementation_7","title":"Implementation","text":"<ul> <li>Analyzer: src/gsql2rsql/planner/path_analyzer.py:PathAnalyzer</li> <li>Usage: Called during planning for <code>RecursiveTraversalOperator</code> construction</li> <li>Optimization: <code>needs_edge_collection</code> flag in <code>RecursiveTraversalOperator</code></li> </ul>"},{"location":"decision-log/#decision-9-structured-sql-column-naming","title":"Decision 9: Structured SQL Column Naming","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_8","title":"Context","text":"<p>Cypher variables (<code>p</code>, <code>f</code>) must map to SQL columns. Need to avoid collisions and track entity vs. property.</p>"},{"location":"decision-log/#decision_8","title":"Decision","text":"<p>Use structured naming convention: <code>_gsql2rsql_{variable}_{property}</code> - Entity projection: <code>_gsql2rsql_p_id AS p</code> - Property projection: <code>_gsql2rsql_p_name AS name</code></p>"},{"location":"decision-log/#rationale_8","title":"Rationale","text":"<ul> <li>Collision avoidance: Prefix prevents conflicts with user column names</li> <li>Tracking: Can parse column name to extract variable and property</li> <li>Debugging: Clear provenance in generated SQL</li> </ul>"},{"location":"decision-log/#trade-offs_8","title":"Trade-offs","text":"<ul> <li>Verbosity: Column names are long</li> <li>SQL readability: Generated SQL is harder to read for humans</li> </ul>"},{"location":"decision-log/#implementation_8","title":"Implementation","text":"<ul> <li>Naming: src/gsql2rsql/planner/column_ref.py:compute_sql_column_name()</li> <li>Usage: Throughout src/gsql2rsql/renderer/sql_renderer.py</li> </ul>"},{"location":"decision-log/#decision-10-schema-as-json-vs-code-or-yaml","title":"Decision 10: Schema as JSON (vs. Code or YAML)","text":"<p>Status: \u2705 Adopted (JSON), \u26a0\ufe0f YAML for examples</p>"},{"location":"decision-log/#context_9","title":"Context","text":"<p>Graph schema defines nodes, edges, properties, and their SQL table mappings. Need a format that's: - Human-readable for editing - Machine-parseable for validation - Version-controllable</p>"},{"location":"decision-log/#decision_9","title":"Decision","text":"<p>Use JSON for schema definitions. YAML for example collections (with embedded schemas).</p>"},{"location":"decision-log/#rationale_9","title":"Rationale","text":"<ul> <li>JSON: Strict, widely supported, easy to validate with JSON Schema</li> <li>YAML: More readable for large example collections with comments</li> <li>Separation: Schema is separate from queries (vs. inline)</li> </ul>"},{"location":"decision-log/#trade-offs_9","title":"Trade-offs","text":"<ul> <li>JSON verbosity: More braces and quotes than YAML</li> <li>No comments: JSON doesn't support comments (use description fields)</li> <li>Two formats: Inconsistency between schema.json and examples/*.yaml</li> </ul>"},{"location":"decision-log/#implementation_9","title":"Implementation","text":"<ul> <li>JSON schema: examples/schema.json</li> <li>YAML examples: examples/credit_queries.yaml</li> <li>Schema loading: src/gsql2rsql/common/schema.py</li> <li>YAML loading: src/gsql2rsql/pyspark_executor.py:load_schema_from_yaml()</li> </ul>"},{"location":"decision-log/#decision-11-golden-file-testing","title":"Decision 11: Golden File Testing","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_10","title":"Context","text":"<p>Need to validate that transpiler generates correct SQL. Can't compare generated SQL directly (whitespace, formatting differences).</p>"},{"location":"decision-log/#decision_10","title":"Decision","text":"<p>Use golden file testing: store expected SQL in <code>tests/output/expected/</code>, compare normalized SQL.</p>"},{"location":"decision-log/#rationale_10","title":"Rationale","text":"<ul> <li>Regression detection: Any change to generated SQL is caught</li> <li>Visual diff: Can use standard diff tools to review changes</li> <li>Documentation: Expected files serve as examples</li> </ul>"},{"location":"decision-log/#trade-offs_10","title":"Trade-offs","text":"<ul> <li>Maintenance: Must update expected files when SQL format changes</li> <li>False positives: Formatting changes trigger test failures</li> </ul>"},{"location":"decision-log/#implementation_10","title":"Implementation","text":"<ul> <li>Expected SQL: tests/output/expected/</li> <li>Comparison: tests/utils/sql_test_utils.py:assert_sql_equal()</li> <li>Diff output: tests/output/diff/</li> <li>Update command: <code>make dump-sql-save ID=01 NAME=simple_node_lookup</code></li> </ul>"},{"location":"decision-log/#decision-12-pyspark-validation-tests","title":"Decision 12: PySpark Validation Tests","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_11","title":"Context","text":"<p>Generated SQL might be syntactically correct but semantically wrong. Need to validate against real execution.</p>"},{"location":"decision-log/#decision_11","title":"Decision","text":"<p>Run transpiled SQL on PySpark DataFrames, compare results against expected outcomes.</p>"},{"location":"decision-log/#rationale_11","title":"Rationale","text":"<ul> <li>Correctness: Catches semantic bugs that golden files miss</li> <li>Integration: Validates entire pipeline end-to-end</li> <li>Realistic: Uses actual Spark SQL engine</li> </ul>"},{"location":"decision-log/#trade-offs_11","title":"Trade-offs","text":"<ul> <li>Slow: PySpark tests take 10-20x longer than unit tests</li> <li>Dependency: Requires PySpark (only for dev, not for users)</li> <li>Flakiness: Spark startup/shutdown can be flaky</li> </ul>"},{"location":"decision-log/#implementation_11","title":"Implementation","text":"<ul> <li>Test file: tests/test_examples_with_pyspark.py</li> <li>Executor: src/gsql2rsql/pyspark_executor.py</li> <li>Run command: <code>make test-pyspark-examples</code></li> <li>Quick validation: <code>make test-pyspark-quick</code> (first 5 examples)</li> </ul>"},{"location":"decision-log/#decision-13-textual-tui-for-interactive-development","title":"Decision 13: Textual TUI for Interactive Development","text":"<p>Status: \u2705 Adopted</p>"},{"location":"decision-log/#context_12","title":"Context","text":"<p>Developers need to iterate on queries quickly. Command-line round-trip is slow. Need visual feedback for errors.</p>"},{"location":"decision-log/#decision_12","title":"Decision","text":"<p>Build interactive TUI using Textual framework with live transpilation, schema switching, and clipboard support.</p>"},{"location":"decision-log/#rationale_12","title":"Rationale","text":"<ul> <li>Productivity: Edit query, see SQL immediately</li> <li>Discoverability: Browse curated examples from YAML files</li> <li>Debugging: See AST, operators, scope info in UI</li> <li>UX: Rich formatting, syntax highlighting, copy-paste support</li> </ul>"},{"location":"decision-log/#trade-offs_12","title":"Trade-offs","text":"<ul> <li>Dependency: Requires <code>textual&gt;=0.47.0</code> (heavy framework)</li> <li>Maintenance: TUI code is complex (2200 lines in cli.py)</li> </ul>"},{"location":"decision-log/#implementation_12","title":"Implementation","text":"<ul> <li>TUI command: <code>gsql2rsql tui --examples examples/credit_queries.yaml</code></li> <li>Implementation: src/gsql2rsql/cli.py:tui() (~line 1500+)</li> <li>Dependencies: <code>textual</code>, <code>rich</code>, <code>prompt-toolkit</code></li> </ul>"},{"location":"decision-log/#decision-14-defensive-handling-of-pre-rendered-field-names","title":"Decision 14: Defensive Handling of Pre-Rendered Field Names","text":"<p>Status: \u2705 Adopted (2026-01-19)</p>"},{"location":"decision-log/#context_13","title":"Context","text":"<p>Different operators produce entity fields with different naming states: - DataSourceOperator: <code>field_name = \"id\"</code> (simple property name) - RecursiveTraversalOperator: <code>field_name = \"_gsql2rsql_peer_id\"</code> (full SQL name)</p> <p>Renderer methods assumed all <code>field_name</code> values were simple property names, causing double-prefixing for recursive traversal outputs.</p>"},{"location":"decision-log/#decision_13","title":"Decision","text":"<p>Add defensive checks in renderer to detect pre-rendered SQL column names: Python<pre><code>if field.field_name and field.field_name.startswith(COLUMN_PREFIX):\n    # Already a full SQL name - use as-is\n    key = field.field_name\nelse:\n    # Simple property name - construct full SQL name\n    key = self._get_field_name(entity_alias, field.field_alias)\n</code></pre></p>"},{"location":"decision-log/#rationale_13","title":"Rationale","text":"<ul> <li>Correctness: Prevents double-prefixing (<code>_gsql2rsql_peer_peer_id</code> \u274c)</li> <li>Backward compatible: Doesn't break existing DataSourceOperator behavior</li> <li>Minimal changes: Only affects renderer, no planner changes needed</li> <li>Preserves information: Keeps pre-rendered names for debugging</li> </ul>"},{"location":"decision-log/#trade-offs_13","title":"Trade-offs","text":"<ul> <li>More complex: Requires checks in 10+ locations across renderer</li> <li>Implicit contract: Relies on <code>COLUMN_PREFIX</code> to detect pre-rendered names</li> <li>Alternative rejected: Could have cleared <code>field_name</code> after recursive traversal, but loses debugging information</li> </ul>"},{"location":"decision-log/#implementation_13","title":"Implementation","text":"<ul> <li>Renderer: src/gsql2rsql/renderer/sql_renderer.py</li> <li><code>_collect_required_columns()</code> (~lines 447-490)</li> <li><code>_render_join_conditions()</code> (~lines 2311-2394)</li> <li><code>_render_join_projection()</code> (~lines 1941-2049)</li> <li><code>_extract_columns_from_schema()</code> (~lines 2166-2200)</li> <li>Tests: tests/test_variable_length_path_field_naming.py</li> <li>Documentation: docs/development/VARLEN_PATH_FIELD_NAMING_FIX.md</li> </ul>"},{"location":"decision-log/#open-questions-todos","title":"Open Questions / TODOs","text":""},{"location":"decision-log/#1-support-for-neo4j-specific-functions","title":"1. Support for Neo4j-Specific Functions","text":"<p>Question: Should we add support for Neo4j-specific functions (e.g., <code>apoc.*</code>)? Trade-off: Would require custom UDFs in Databricks, breaking pure SQL generation. Decision needed: User feedback on priority of Neo4j compatibility.</p>"},{"location":"decision-log/#2-multi-graph-support","title":"2. Multi-Graph Support","text":"<p>Question: Should schema support multiple named graphs (vs. single graph per schema)? Current: Single graph per schema file. Trade-off: More complex schema format, but enables multi-tenancy patterns.</p>"},{"location":"decision-log/#3-query-planner-hints","title":"3. Query Planner Hints","text":"<p>Question: Should we allow users to hint join order or index usage in Cypher comments? Current: No hint support, Databricks optimizer makes all decisions. Trade-off: Power users want control, but hints are non-portable.</p>"},{"location":"decision-log/#4-alternative-sql-backends","title":"4. Alternative SQL Backends","text":"<p>Question: Should we support other SQL dialects (PostgreSQL, DuckDB, SQLite)? Current: Databricks-only. Trade-off: Broader adoption vs. maintenance burden. Need to abstract SQL dialect differences. File to modify: src/gsql2rsql/renderer/sql_renderer.py (add dialect parameter)</p>"},{"location":"decision-log/#5-property-graph-schema-import","title":"5. Property Graph Schema Import","text":"<p>Question: Should we auto-generate schema from existing Databricks tables using <code>DESCRIBE TABLE</code>? Current: Manual JSON schema creation. Trade-off: Convenience vs. requiring database connection at transpile time. Related command: <code>gsql2rsql init-schema</code> (currently generates template, could introspect DB)</p>"},{"location":"decision-log/#6-incremental-compilation","title":"6. Incremental Compilation","text":"<p>Question: Should we cache AST/operators for repeated queries? Current: Full transpilation on every invocation. Trade-off: Performance vs. complexity. Only matters for hot-path query generation.</p>"},{"location":"decision-log/#7-aggregation-after-aggregation","title":"7. Aggregation After Aggregation","text":"<p>Question: Current architecture allows aggregation after aggregation. Is this correct? Context: Recent commits mention \"aggregation entity projection\" bugs. Investigation needed: Review src/gsql2rsql/planner/operators.py:AggregationBoundaryOperator (~line 600+) Related test: tests/test_aggregation_entity_projection.py</p>"},{"location":"decision-log/#8-predicate-pushdown-completeness","title":"8. Predicate Pushdown Completeness","text":"<p>Question: Are all pushable predicates being pushed to recursive CTEs? Context: PathAnalyzer handles some cases, but complex predicates might not be covered. Investigation needed: Review src/gsql2rsql/planner/path_analyzer.py Related tests: <code>test_37_source_node_filter_pushdown.py</code>, <code>test_39_recursive_sink_filter_pushdown.py</code></p>"},{"location":"decision-log/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>04-limitations.md \u2014 Known limitations and unsupported features</li> <li>CONTRIBUTING.md \u2014 Detailed phase boundaries</li> <li>02-architecture.md \u2014 Component breakdown</li> </ul>"},{"location":"first-query/","title":"Your First Query","text":"<p>This tutorial walks through transpiling your first OpenCypher query to SQL.</p>"},{"location":"first-query/#step-1-create-a-schema","title":"Step 1: Create a Schema","text":"<p>gsql2rsql needs a schema that maps your graph to SQL tables. Create <code>my_schema.json</code>:</p> JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"catalog.mydb.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    },\n    {\n      \"name\": \"Company\",\n      \"tableName\": \"catalog.mydb.Company\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"industry\", \"type\": \"string\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"WORKS_AT\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Company\",\n      \"tableName\": \"catalog.mydb.PersonWorksAt\",\n      \"sourceIdProperty\": {\"name\": \"person_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"company_id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"since\", \"type\": \"int\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"first-query/#step-2-write-your-query","title":"Step 2: Write Your Query","text":"<p>Create a Cypher query in <code>my_query.cypher</code>:</p> Cypher<pre><code>MATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, p.age, c.name AS company\nORDER BY p.age DESC\nLIMIT 10\n</code></pre>"},{"location":"first-query/#step-3-transpile","title":"Step 3: Transpile","text":"<p>Use the CLI to generate SQL:</p> Bash<pre><code>gsql2rsql translate --schema my_schema.json &lt; my_query.cypher\n</code></pre> <p>Output: SQL<pre><code>SELECT\n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_p_age AS age\n  ,_gsql2rsql_c_name AS company\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_right._gsql2rsql_c_industry AS _gsql2rsql_c_industry\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,age AS _gsql2rsql_p_age\n      FROM\n        catalog.mydb.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,company_id AS _gsql2rsql__anon1_company_id\n      FROM\n        catalog.mydb.PersonWorksAt\n    ) AS _right\n    ON (_left._gsql2rsql_p_id) = (_right._gsql2rsql__anon1_person_id)\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,name AS _gsql2rsql_c_name\n      ,industry AS _gsql2rsql_c_industry\n    FROM\n      catalog.mydb.Company\n  ) AS _right\n  ON (_left._gsql2rsql__anon1_company_id) = (_right._gsql2rsql_c_id)\n  WHERE (_gsql2rsql_c_industry) = ('Technology')\n) AS _proj\nORDER BY _gsql2rsql_p_age DESC\nLIMIT 10\n</code></pre></p>"},{"location":"first-query/#step-4-execute-on-databricks","title":"Step 4: Execute on Databricks","text":"<p>Save the SQL to a file and execute it:</p> Python<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"gsql2rsql\").getOrCreate()\n\n# Read generated SQL\nwith open(\"output.sql\") as f:\n    sql = f.read()\n\n# Execute\nresult = spark.sql(sql)\nresult.show()\n</code></pre>"},{"location":"first-query/#understanding-the-output","title":"Understanding the Output","text":"<p>The generated SQL:</p> <ol> <li>Reads from tables: <code>catalog.mydb.Person</code>, <code>catalog.mydb.PersonWorksAt</code>, <code>catalog.mydb.Company</code></li> <li>Projects columns with prefixed names (e.g., <code>_gsql2rsql_p_name</code>)</li> <li>Joins tables based on relationship IDs</li> <li>Applies WHERE filter on <code>c.industry = 'Technology'</code></li> <li>Orders and limits results</li> </ol> <p>The prefixed column names (<code>_gsql2rsql_*</code>) avoid collisions with user column names.</p>"},{"location":"first-query/#next-steps","title":"Next Steps","text":"<ul> <li>Examples Gallery: See real-world queries</li> <li>CLI Commands: Full command reference</li> <li>Query Translation: How it works</li> </ul>"},{"location":"installation/","title":"Installation and Quick Start","text":"<p>This guide covers installing gsql2rsql and getting started with your first query.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.12 or later</li> <li>Databricks Runtime: 17+ (for executing generated SQL - only needed at runtime)</li> </ul> <p>Core Dependencies (automatically installed): - ANTLR4 runtime - Click (CLI framework) - Prompt Toolkit - PyYAML - Rich &amp; Textual (TUI)</p> <p>Note: PySpark is NOT required to use gsql2rsql. It's only a dev dependency for running validation tests.</p>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>Once published, install using pip:</p> Bash<pre><code>pip install gsql2rsql\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":""},{"location":"installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package manager:</p> Bash<pre><code># Clone repository\ngit clone https://github.com/devmessias/gsql2rsql\ncd gsql2rsql/python\n\n# Install uv if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install\nuv sync\nuv pip install -e .\n</code></pre>"},{"location":"installation/#using-pip","title":"Using pip","text":"Bash<pre><code># Clone repository\ngit clone https://github.com/devmessias/gsql2rsql\ncd gsql2rsql/python\n\n# Create virtual environment\npython3 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development with all testing dependencies:</p> Bash<pre><code># Using uv\nuv sync --extra dev\nuv pip install -e \".[dev]\"\n\n# Or using pip\npip install -e \".[dev]\"\n</code></pre> <p>This installs additional tools: - pytest: Testing framework - ruff: Linting and formatting - mypy: Type checking - PySpark: For validation tests</p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>Check that gsql2rsql is installed:</p> Bash<pre><code>gsql2rsql --version\n</code></pre> <p>Test transpilation:</p> Bash<pre><code>echo \"MATCH (p:Person) RETURN p.name\" | gsql2rsql translate --schema examples/schema.json\n</code></pre>"},{"location":"installation/#development-only-pyspark-setup","title":"Development Only: PySpark Setup","text":"<p>PySpark is only needed if you're contributing to the project and want to run validation tests locally.</p> <p>Regular users do NOT need PySpark - the transpiler works without it.</p>"},{"location":"installation/#install-development-dependencies","title":"Install Development Dependencies","text":"Bash<pre><code># Install all dev dependencies including PySpark\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#install-java-pyspark-requirement","title":"Install Java (PySpark requirement)","text":"<p>PySpark requires Java 8 or 11:</p> Bash<pre><code># Ubuntu/Debian\nsudo apt-get install openjdk-11-jdk\n\n# macOS\nbrew install openjdk@11\n</code></pre>"},{"location":"installation/#verify-pyspark","title":"Verify PySpark","text":"Bash<pre><code>python -c \"import pyspark; print(pyspark.__version__)\"\n</code></pre>"},{"location":"installation/#run-validation-tests","title":"Run Validation Tests","text":"Bash<pre><code># Run tests without PySpark (fast)\nmake test-no-pyspark\n\n# Run PySpark validation tests (slower)\nmake test-pyspark\n</code></pre>"},{"location":"installation/#your-first-query","title":"Your First Query","text":"<p>This tutorial walks through transpiling your first OpenCypher query to SQL.</p>"},{"location":"installation/#step-1-create-a-schema","title":"Step 1: Create a Schema","text":"<p>gsql2rsql needs a schema that maps your graph to SQL tables. You can use Python dataclasses or JSON format.</p>"},{"location":"installation/#using-python-recommended","title":"Using Python (Recommended)","text":"Python<pre><code>from gsql2rsql.common.schema import NodeSchema, EdgeSchema, EntityProperty\nfrom gsql2rsql.planner.schema import SimpleGraphSchemaProvider\n\n# Create schema provider\nschema = SimpleGraphSchemaProvider()\n\n# Define Person node\nperson = NodeSchema(\n    name=\"Person\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"age\", data_type=int),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\n# Define Company node\ncompany = NodeSchema(\n    name=\"Company\",\n    properties=[\n        EntityProperty(property_name=\"id\", data_type=int),\n        EntityProperty(property_name=\"name\", data_type=str),\n        EntityProperty(property_name=\"industry\", data_type=str),\n    ],\n    node_id_property=EntityProperty(property_name=\"id\", data_type=int)\n)\n\n# Define WORKS_AT relationship\nworks_at = EdgeSchema(\n    name=\"WORKS_AT\",\n    source_node_id=\"Person\",\n    sink_node_id=\"Company\",\n    source_id_property=EntityProperty(property_name=\"person_id\", data_type=int),\n    sink_id_property=EntityProperty(property_name=\"company_id\", data_type=int),\n    properties=[\n        EntityProperty(property_name=\"since\", data_type=int)\n    ]\n)\n\n# Add to schema\nschema.add_node(person)\nschema.add_node(company)\nschema.add_edge(works_at)\n</code></pre>"},{"location":"installation/#using-json-schema","title":"Using JSON Schema","text":"<p>Alternatively, create <code>my_schema.json</code>:</p> JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"catalog.mydb.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    },\n    {\n      \"name\": \"Company\",\n      \"tableName\": \"catalog.mydb.Company\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"industry\", \"type\": \"string\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"WORKS_AT\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Company\",\n      \"tableName\": \"catalog.mydb.PersonWorksAt\",\n      \"sourceIdProperty\": {\"name\": \"person_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"company_id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"since\", \"type\": \"int\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"installation/#step-2-write-your-query","title":"Step 2: Write Your Query","text":"<p>Create a Cypher query in <code>my_query.cypher</code>:</p> Cypher<pre><code>MATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, p.age, c.name AS company\nORDER BY p.age DESC\nLIMIT 10\n</code></pre>"},{"location":"installation/#step-3-transpile-to-sql","title":"Step 3: Transpile to SQL","text":""},{"location":"installation/#using-python-api","title":"Using Python API","text":"Python<pre><code>from gsql2rsql.parser.opencypher_parser import OpenCypherParser\nfrom gsql2rsql.planner.logical_plan import LogicalPlan\nfrom gsql2rsql.renderer.sql_renderer import SQLRenderer\nfrom gsql2rsql.planner.schema import DatabricksSchemaProvider\n\n# Setup (using schema from Step 1)\nschema_provider = DatabricksSchemaProvider(schema)\nparser = OpenCypherParser()\nrenderer = SQLRenderer(schema_provider)\n\n# Parse and transpile\nquery = \"\"\"\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, p.age, c.name AS company\nORDER BY p.age DESC\nLIMIT 10\n\"\"\"\n\nast = parser.parse(query)\nplan = LogicalPlan.from_ast(ast, schema)\nplan.resolve(query)\nsql = renderer.render_plan(plan)\n\nprint(sql)\n</code></pre>"},{"location":"installation/#using-cli","title":"Using CLI","text":"Bash<pre><code>gsql2rsql translate --schema my_schema.json &lt; my_query.cypher\n</code></pre> <p>Output: SQL<pre><code>SELECT\n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_p_age AS age\n  ,_gsql2rsql_c_name AS company\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_right._gsql2rsql_c_industry AS _gsql2rsql_c_industry\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,age AS _gsql2rsql_p_age\n      FROM\n        catalog.mydb.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,company_id AS _gsql2rsql__anon1_company_id\n      FROM\n        catalog.mydb.PersonWorksAt\n    ) AS _right\n    ON (_left._gsql2rsql_p_id) = (_right._gsql2rsql__anon1_person_id)\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,name AS _gsql2rsql_c_name\n      ,industry AS _gsql2rsql_c_industry\n    FROM\n      catalog.mydb.Company\n  ) AS _right\n  ON (_left._gsql2rsql__anon1_company_id) = (_right._gsql2rsql_c_id)\n  WHERE (_gsql2rsql_c_industry) = ('Technology')\n) AS _proj\nORDER BY _gsql2rsql_p_age DESC\nLIMIT 10\n</code></pre></p>"},{"location":"installation/#step-4-execute-on-databricks","title":"Step 4: Execute on Databricks","text":"<p>Save the SQL to a file and execute it:</p> Python<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"gsql2rsql\").getOrCreate()\n\n# Read generated SQL\nwith open(\"output.sql\") as f:\n    sql = f.read()\n\n# Execute\nresult = spark.sql(sql)\nresult.show()\n</code></pre>"},{"location":"installation/#understanding-the-output","title":"Understanding the Output","text":"<p>The generated SQL:</p> <ol> <li>Reads from tables: <code>catalog.mydb.Person</code>, <code>catalog.mydb.PersonWorksAt</code>, <code>catalog.mydb.Company</code></li> <li>Projects columns with prefixed names (e.g., <code>_gsql2rsql_p_name</code>)</li> <li>Joins tables based on relationship IDs</li> <li>Applies WHERE filter on <code>c.industry = 'Technology'</code></li> <li>Orders and limits results</li> </ol> <p>The prefixed column names (<code>_gsql2rsql_*</code>) avoid collisions with user column names.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Examples Gallery: See real-world queries</li> <li>Python API Reference: Full API documentation</li> <li>CLI Commands: Command reference</li> </ul>"},{"location":"limitations/","title":"Limitations and Unsupported Features","text":"<p>This document lists known limitations, unsupported OpenCypher features, runtime caveats, and recommended workarounds.</p>"},{"location":"limitations/#runtime-requirements","title":"Runtime Requirements","text":""},{"location":"limitations/#databricks-runtime-17-required","title":"Databricks Runtime 17+ Required","text":"<p>Requirement: Databricks Runtime 17 or higher</p> <p>Reason: The transpiler generates <code>WITH RECURSIVE</code> CTEs for variable-length paths. This feature was added in Databricks Runtime 17.</p> <p>Affected Features: - Variable-length paths: <code>-[:TYPE*1..N]-&gt;</code> - Any query with <code>*</code> in relationship patterns</p> <p>Workaround: Upgrade to Databricks Runtime 17+. No workaround for older runtimes (would require complete rewrite to PySpark DataFrame code).</p>"},{"location":"limitations/#spark-sql-limitations","title":"Spark SQL Limitations","text":"<p>ARRAY Operations: Generated SQL uses <code>ARRAY()</code>, <code>CONCAT()</code>, <code>array_contains()</code> functions. - These require Spark SQL, not standard ANSI SQL - Older Spark versions may have incomplete array support</p> <p>STRUCT for Edge Collections: Edge property collections use <code>COLLECT(STRUCT(...))</code>. - Requires Spark SQL struct support - May have performance implications on very large result sets</p>"},{"location":"limitations/#undirected-relationship-performance-optimized","title":"Undirected Relationship Performance (OPTIMIZED)","text":"<p>Status: \u2705 Optimized (as of 2026-01-19)</p> <p>Default Behavior: Undirected relationships (<code>-[:TYPE]-</code>) now use UNION ALL edge expansion for optimal performance.</p> <p>Example Query: Cypher<pre><code>-- Undirected relationship (fast with optimization)\nMATCH (a:Person)-[:KNOWS]-(b:Person)\nWHERE a.name = 'Alice'\nRETURN b.name\n</code></pre></p> <p>Generated SQL (Optimized - Default): SQL<pre><code>-- Edges expanded bidirectionally before joining\nJOIN (\n  SELECT source_id AS node_id, target_id AS other_id, props FROM Knows\n  UNION ALL\n  SELECT target_id AS node_id, source_id AS other_id, props FROM Knows\n) k ON person.id = k.node_id\n</code></pre></p> <p>Performance: - Small datasets (&lt; 1000 rows): Fast (hash join) - Medium datasets (1K-100K rows): Fast (hash join with indexes) - Large datasets (&gt; 100K rows): Optimized (O(n) instead of O(n\u00b2))</p> <p>Known Limitation: Self-loops (e.g., <code>(a)-[:KNOWS]-(a)</code>) may appear twice in results. - Workaround: Add <code>WHERE a.id &lt;&gt; b.id</code> or use <code>DISTINCT</code></p> <p>Disabling Optimization (for debugging or compatibility): Python<pre><code>from gsql2rsql import SQLRenderer\n\n# Use legacy OR join strategy (slower but simpler SQL)\nrenderer = SQLRenderer(\n    db_schema_provider=schema,\n    config={\"undirected_strategy\": \"or_join\"}  # Not recommended\n)\n</code></pre></p> <p>Generated SQL (Legacy - OR Join): SQL<pre><code>-- Only use for small datasets or debugging\nON (person.id = knows.source_id OR person.id = knows.target_id)\n</code></pre></p> <p>Learn More: See UNDIRECTED_OPTIMIZATION_IMPLEMENTATION.md for implementation details and trade-off analysis.</p>"},{"location":"limitations/#unsupported-opencypher-features","title":"Unsupported OpenCypher Features","text":""},{"location":"limitations/#1-merge-upsert-operations","title":"1. MERGE (Upsert Operations)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: <code>MERGE</code> requires write operations. The transpiler generates read-only SQL queries.</p> <p>Example (unsupported): Cypher<pre><code>MERGE (p:Person {id: 123})\nON CREATE SET p.created_at = timestamp()\nON MATCH SET p.last_seen = timestamp()\n</code></pre></p> <p>Workaround: Use Databricks <code>MERGE INTO</code> statement directly (not via transpiler).</p>"},{"location":"limitations/#2-create-delete-set-remove-write-operations","title":"2. CREATE, DELETE, SET, REMOVE (Write Operations)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: Transpiler is read-only. No support for graph mutations.</p> <p>Examples (all unsupported): Cypher<pre><code>CREATE (p:Person {name: 'Alice'})\nDELETE p\nSET p.age = 30\nREMOVE p.age\n</code></pre></p> <p>Workaround: Use standard SQL <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> statements on your Databricks tables.</p>"},{"location":"limitations/#3-shortest-path-shortestpath-allshortestpaths","title":"3. Shortest Path (shortestPath, allShortestPaths)","text":"<p>Status: \u26a0\ufe0f Partial Support (INFERRED)</p> <p>Supported: BFS traversal with depth tracking via <code>-[:TYPE*1..N]-&gt;</code> Not Supported: Built-in <code>shortestPath()</code> and <code>allShortestPaths()</code> functions</p> <p>Example: Cypher<pre><code>-- \u274c Not supported\nMATCH p = shortestPath((a:Person)-[:KNOWS*]-(b:Person))\nRETURN p\n\n-- \u2705 Workaround: use bounded BFS with ORDER BY depth LIMIT 1\nMATCH (a:Person)-[:KNOWS*1..10]-(b:Person)\nWHERE a.id = 1 AND b.id = 100\nRETURN a, b\nORDER BY length(relationships(path)) ASC\nLIMIT 1\n</code></pre></p> <p>Limitation: Workaround requires explicit max depth and may be inefficient for large graphs.</p>"},{"location":"limitations/#4-foreach-iteration-with-side-effects","title":"4. FOREACH (Iteration with Side Effects)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: <code>FOREACH</code> is for mutations (side effects). Transpiler is read-only.</p> <p>Example (unsupported): Cypher<pre><code>MATCH p = (a)-[:KNOWS*]-(b)\nFOREACH (n IN nodes(p) | SET n.visited = true)\n</code></pre></p> <p>Workaround: None (fundamentally incompatible with read-only SQL).</p>"},{"location":"limitations/#5-call-procedures-apoc-custom-procedures","title":"5. CALL Procedures (APOC, Custom Procedures)","text":"<p>Status: \u274c Not Supported</p> <p>Reason: No concept of stored procedures in the transpilation model. APOC functions are Neo4j-specific.</p> <p>Example (unsupported): Cypher<pre><code>CALL apoc.periodic.iterate(...)\nCALL db.stats.retrieve(...)\n</code></pre></p> <p>Workaround: Use Databricks SQL UDFs or built-in functions where equivalent functionality exists.</p>"},{"location":"limitations/#6-pattern-comprehension-with-where","title":"6. Pattern Comprehension with WHERE","text":"<p>Status: \u26a0\ufe0f Partial Support (INFERRED)</p> <p>Supported: Simple pattern comprehension <code>[(n)-[:KNOWS]-&gt;(f) | f.name]</code> Not Supported: Pattern comprehension with complex <code>WHERE</code> clauses (INFERRED from lack of tests)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not be fully supported\nRETURN [(n)-[:KNOWS]-&gt;(f) WHERE f.age &gt; 30 | f.name] AS friends\n</code></pre></p> <p>Workaround: Use standard <code>MATCH</code> with <code>WITH</code> clause instead: Cypher<pre><code>MATCH (n)-[:KNOWS]-&gt;(f)\nWHERE f.age &gt; 30\nWITH n, COLLECT(f.name) AS friends\nRETURN n, friends\n</code></pre></p>"},{"location":"limitations/#7-multiple-match-patterns-in-single-clause","title":"7. Multiple MATCH Patterns in Single Clause","text":"<p>Status: \u26a0\ufe0f Limited Support</p> <p>Supported: Single pattern per <code>MATCH</code> clause Not Supported: Comma-separated patterns in one <code>MATCH</code> (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nMATCH (a)-[:KNOWS]-&gt;(b), (b)-[:LIKES]-&gt;(c)\nRETURN a, b, c\n\n-- \u2705 Workaround: use multiple MATCH clauses\nMATCH (a)-[:KNOWS]-&gt;(b)\nMATCH (b)-[:LIKES]-&gt;(c)\nRETURN a, b, c\n</code></pre></p>"},{"location":"limitations/#8-map-projections","title":"8. Map Projections","text":"<p>Status: \u26a0\ufe0f Partial Support</p> <p>Supported: Basic map literals <code>{key: value}</code> (test: test_35_map_literals.py) Not Supported: Map projections with property selectors <code>n{.id, .name}</code> (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nRETURN n{.id, .name, .age} AS person\n\n-- \u2705 Workaround: explicit map construction\nRETURN {id: n.id, name: n.name, age: n.age} AS person\n</code></pre></p>"},{"location":"limitations/#9-temporal-types-duration-temporal-arithmetic","title":"9. Temporal Types (Duration, Temporal Arithmetic)","text":"<p>Status: \u26a0\ufe0f Partial Support</p> <p>Supported: Basic datetime functions (test: test_34_datetime.py) Not Supported: <code>duration()</code>, temporal arithmetic (INFERRED)</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May not work\nRETURN datetime() - duration({days: 7}) AS last_week\n\n-- \u2705 Workaround: use Databricks date functions\nRETURN date_sub(current_timestamp(), 7) AS last_week\n</code></pre></p>"},{"location":"limitations/#10-geospatial-functions","title":"10. Geospatial Functions","text":"<p>Status: \u274c Not Supported (INFERRED - no tests found)</p> <p>Reason: No OpenCypher geospatial support in transpiler.</p> <p>Example (unsupported): Cypher<pre><code>RETURN distance(point({x: 0, y: 0}), point({x: 3, y: 4})) AS dist\n</code></pre></p> <p>Workaround: Use Databricks Geospatial functions directly in SQL.</p>"},{"location":"limitations/#known-correctness-caveats","title":"Known Correctness Caveats","text":""},{"location":"limitations/#1-aggregation-after-aggregation","title":"1. Aggregation After Aggregation","text":"<p>Status: \u26a0\ufe0f Under Investigation</p> <p>Issue: Recent commits mention fixes for \"aggregation entity projection\" bugs (see commit: <code>155da2f</code>).</p> <p>Potentially Problematic Pattern: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nWITH p, COUNT(*) AS purchases\nWITH p.name, SUM(purchases) AS total  -- Aggregation after aggregation\nRETURN p.name, total\n</code></pre></p> <p>Related Test: tests/test_aggregation_entity_projection.py</p> <p>Workaround: If you encounter <code>UNRESOLVED_COLUMN</code> errors, try flattening the aggregation: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nWITH p.name, COUNT(*) AS total\nRETURN p.name, total\n</code></pre></p>"},{"location":"limitations/#2-multi-with-entity-continuation","title":"2. Multi-WITH Entity Continuation","text":"<p>Status: \u26a0\ufe0f Fixed Recently</p> <p>Issue: Recent tests added for \"multi-WITH entity continuation bug\" (commit: <code>7ec0add</code>).</p> <p>Affected Pattern: Cypher<pre><code>MATCH (n:Node)\nWITH n\nWITH n, n.property AS prop\nRETURN n, prop\n</code></pre></p> <p>Related Test: tests/test_multi_with_entity_continuation.py</p> <p>Status: Should be fixed in latest version. If you encounter issues, report with minimal repro case.</p>"},{"location":"limitations/#3-unresolved_column-errors-with-aggregations","title":"3. UNRESOLVED_COLUMN Errors with Aggregations","text":"<p>Status: \u26a0\ufe0f Edge Cases Remain (INFERRED)</p> <p>Context: Column resolution after aggregation boundaries is complex. Some edge cases may trigger Databricks <code>UNRESOLVED_COLUMN</code> errors.</p> <p>Symptom: Transpilation succeeds, but Databricks SQL execution fails with \"column not found\".</p> <p>Workaround: 1. Simplify <code>WITH</code> clauses (fewer intermediate steps) 2. Explicitly alias all aggregated columns 3. Avoid referencing entity properties after aggregation (use aliases instead)</p> <p>Related Commits: - <code>155da2f</code>: \"preserve full column names for entity projections after aggregation\" - <code>6388679</code>: \"use the AggregationBoundaryOperator itself\"</p>"},{"location":"limitations/#performance-caveats","title":"Performance Caveats","text":""},{"location":"limitations/#1-deep-variable-length-paths","title":"1. Deep Variable-Length Paths","text":"<p>Issue: <code>WITH RECURSIVE</code> CTEs for deep paths (e.g., <code>-[:TYPE*1..20]-&gt;</code>) can be slow or hit Spark recursion limits.</p> <p>Recommendation: - Keep max depth \u2264 10 for most queries - Use <code>LIMIT</code> to reduce result set size - Consider pre-computing transitive closures for frequent deep traversals</p> <p>Example: Cypher<pre><code>-- \u26a0\ufe0f May be slow\nMATCH (a)-[:FOLLOWS*1..20]-&gt;(b)\nRETURN DISTINCT b\n\n-- \u2705 Better: bounded depth with LIMIT\nMATCH (a)-[:FOLLOWS*1..5]-&gt;(b)\nRETURN DISTINCT b\nLIMIT 100\n</code></pre></p>"},{"location":"limitations/#2-cartesian-products","title":"2. Cartesian Products","text":"<p>Issue: Certain query patterns generate cartesian products (cross joins without predicates).</p> <p>Detection: Transpiler attempts to avoid cartesian products (test: test_10_relationship_join_no_cartesian.py)</p> <p>Recommendation: Always connect patterns with shared variables or predicates.</p> <p>Example: Cypher<pre><code>-- \u274c Cartesian product (no connection between patterns)\nMATCH (p:Person), (c:Company)\nRETURN p, c\n\n-- \u2705 Better: connected patterns\nMATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nRETURN p, c\n</code></pre></p>"},{"location":"limitations/#3-large-collect-aggregations","title":"3. Large COLLECT Aggregations","text":"<p>Issue: <code>COLLECT()</code> aggregations create arrays in memory. Very large collections can cause OOM.</p> <p>Affected Pattern: Cypher<pre><code>MATCH (p:Person)-[:BOUGHT]-&gt;(product)\nRETURN p.name, COLLECT(product) AS all_products  -- Could be millions\n</code></pre></p> <p>Recommendation: - Use <code>LIMIT</code> within aggregation: <code>COLLECT(product)[0..100]</code> (if supported) - Filter before aggregation: <code>WHERE product.category = 'Electronics'</code> - Use <code>COUNT()</code> instead of <code>COLLECT()</code> when possible</p>"},{"location":"limitations/#4-edge-collection-in-recursive-ctes","title":"4. Edge Collection in Recursive CTEs","text":"<p>Issue: Path analyzer optimizes away edge collection when not needed (see Decision 8 in decision log).</p> <p>Impact: If you access <code>relationships(path)</code>, the transpiler collects edges in recursive CTE (expensive).</p> <p>Recommendation: Only use <code>relationships(path)</code> when necessary. Use <code>length(path)</code> for simple path length checks.</p>"},{"location":"limitations/#sql-dialect-constraints","title":"SQL Dialect Constraints","text":""},{"location":"limitations/#1-databricks-specific-functions","title":"1. Databricks-Specific Functions","text":"<p>The transpiler generates SQL using Databricks-specific functions:</p> Function Purpose Databricks-Specific? <code>array_contains()</code> Cycle detection Yes (some DBs use <code>ANY()</code>) <code>CONCAT(array1, array2)</code> Array concatenation Yes (PostgreSQL uses <code>||</code>) <code>COLLECT()</code> Aggregation to array No (SQL standard <code>ARRAY_AGG()</code>) <code>STRUCT()</code> Named tuple Yes (PostgreSQL uses <code>ROW()</code>) <p>Implication: Generated SQL is not portable to other databases without modification.</p> <p>Related Decision: Decision 4 in decision log</p>"},{"location":"limitations/#2-recursive-cte-limitations","title":"2. Recursive CTE Limitations","text":"<p>Databricks Specifics: - No <code>CYCLE</code> clause (must manually track visited nodes with array) - No <code>SEARCH</code> clause (must manually compute depth) - Performance: Recursive CTEs may not be optimized as well as native graph traversal</p>"},{"location":"limitations/#open-questions-todos","title":"Open Questions / TODOs","text":""},{"location":"limitations/#1-full-list-of-unsupported-functions","title":"1. Full List of Unsupported Functions","text":"<p>Status: Documentation incomplete</p> <p>TODO: Audit all OpenCypher functions and document which are unsupported.</p> <p>Known gaps: - \u274c Geospatial: <code>distance()</code>, <code>point()</code>, <code>withinBBox()</code> - \u274c Graph algorithms: <code>pageRank()</code>, <code>betweenness()</code>, etc. (Neo4j GDS) - \u274c APOC: All <code>apoc.*</code> functions - \u26a0\ufe0f Date/time: Partial support (needs audit)</p> <p>Where to add: This document (section above)</p>"},{"location":"limitations/#2-neo4j-compatibility-matrix","title":"2. Neo4j Compatibility Matrix","text":"<p>Status: No formal compatibility documentation</p> <p>TODO: Create a compatibility matrix showing which Neo4j features work vs. don't work.</p> <p>Format: Text Only<pre><code>| Feature              | Neo4j | gsql2rsql | Notes |\n|----------------------|-------|-----------|-------|\n| MATCH pattern        | \u2705    | \u2705        | Full  |\n| Variable-length path | \u2705    | \u2705        | Max depth 10 recommended |\n| shortestPath()       | \u2705    | \u274c        | Use workaround |\n| ...                  | ...   | ...       | ...   |\n</code></pre></p> <p>Where to add: New doc <code>docs/09-neo4j-compatibility.md</code></p>"},{"location":"limitations/#3-pyspark-test-coverage","title":"3. PySpark Test Coverage","text":"<p>Status: Not all tests have PySpark validation</p> <p>Context: Golden file tests (44 tests) validate SQL output, but not all are executed on PySpark.</p> <p>TODO: Identify which patterns are not covered by PySpark tests.</p> <p>Command: Compare test counts: Bash<pre><code># Golden file tests\nls tests/transpile_tests/*.py | wc -l  # 44+\n\n# PySpark examples\nls examples/*.yaml  # 3 files (credit, fraud, features)\n</code></pre></p> <p>Related Files: - tests/test_examples_with_pyspark.py - examples/</p>"},{"location":"limitations/#4-aggregation-semantics-edge-cases","title":"4. Aggregation Semantics Edge Cases","text":"<p>Status: Recent bug fixes suggest edge cases remain</p> <p>Context: Commits <code>155da2f</code>, <code>6388679</code>, <code>7ec0add</code> all relate to aggregation bugs.</p> <p>TODO: Document known patterns that may still have issues.</p> <p>Investigation needed: 1. Review all recent aggregation-related commits 2. Document specific patterns that were buggy 3. Add regression tests for each pattern</p> <p>Related Tests: - tests/test_aggregation_entity_projection.py - tests/test_multi_with_entity_continuation.py - tests/transpile_tests/test_41_column_projection_through_aggregation.py</p>"},{"location":"limitations/#5-subquery-optimizer-completeness","title":"5. Subquery Optimizer Completeness","text":"<p>Status: Conservative optimizer only handles simple cases</p> <p>TODO: Document all patterns that could be flattened but aren't (yet).</p> <p>Examples: - Multiple consecutive <code>SelectionOperator</code> (already handled) - <code>SelectionOperator</code> \u2192 <code>ProjectionOperator</code> (already handled) - <code>ProjectionOperator</code> \u2192 <code>SelectionOperator</code> (NOT handled - could be safe)</p> <p>Related File: src/gsql2rsql/planner/subquery_optimizer.py</p>"},{"location":"limitations/#6-error-message-quality","title":"6. Error Message Quality","text":"<p>Status: Resolver provides good suggestions, but other phases may not</p> <p>TODO: Audit error messages across all phases: - Parser: ANTLR errors (often cryptic) - Planner: Schema binding errors - Resolver: Column resolution errors (\u2705 good) - Renderer: SQL generation errors</p> <p>Where to improve: src/gsql2rsql/common/exceptions.py</p>"},{"location":"limitations/#recommended-workarounds-summary","title":"Recommended Workarounds Summary","text":"Limitation Workaround MERGE / CREATE / DELETE Use native Databricks SQL statements shortestPath() Use bounded BFS with <code>ORDER BY depth LIMIT 1</code> Deep paths (&gt;10 hops) Pre-compute transitive closure or use external graph DB Large COLLECT() Filter before aggregation, use LIMIT APOC functions Find equivalent Databricks SQL function or UDF Geospatial Use Databricks Geospatial functions directly Pattern comprehension with WHERE Use explicit <code>MATCH</code> + <code>WITH</code> + <code>COLLECT()</code> Cartesian products Always connect patterns with shared variables UNRESOLVED_COLUMN errors Simplify <code>WITH</code> clauses, use explicit aliases"},{"location":"limitations/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>05-testing-and-examples.md \u2014 How to add tests for new patterns</li> <li>07-developer-guide.md \u2014 How to extend support for new features</li> <li>03-decision-log.md \u2014 Why certain features are not supported</li> </ul>"},{"location":"query-translation/","title":"Query Translation Guide","text":"<p>This guide explains how gsql2rsql transpiles OpenCypher queries to SQL.</p>"},{"location":"query-translation/#overview","title":"Overview","text":"<p>The transpilation process follows a four-phase pipeline:</p> <pre><code>graph LR\n    A[OpenCypher Query] --&gt; B[Parser]\n    B --&gt; C[Planner]\n    C --&gt; D[Resolver]\n    D --&gt; E[Renderer]\n    E --&gt; F[SQL Query]</code></pre> <p>Each phase has a specific responsibility and operates independently, maintaining strict Separation of Concerns (SoC).</p>"},{"location":"query-translation/#phase-1-parser","title":"Phase 1: Parser","text":"<p>Input: OpenCypher query string Output: Abstract Syntax Tree (AST)</p> <p>The parser uses ANTLR4 with an OpenCypher grammar to convert the query string into a structured AST.</p> <p>Example:</p> Cypher<pre><code>MATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, c.name\n</code></pre> <p>AST Structure:</p> Text Only<pre><code>MatchQuery\n\u251c\u2500 MatchClause\n\u2502  \u2514\u2500 Pattern\n\u2502     \u2514\u2500 Path\n\u2502        \u251c\u2500 Node(p, Person)\n\u2502        \u251c\u2500 Edge(WORKS_AT, \u2192)\n\u2502        \u2514\u2500 Node(c, Company)\n\u251c\u2500 WhereClause\n\u2502  \u2514\u2500 Comparison(=)\n\u2502     \u251c\u2500 Property(c.industry)\n\u2502     \u2514\u2500 Literal('Technology')\n\u2514\u2500 ReturnClause\n   \u251c\u2500 Property(p.name)\n   \u2514\u2500 Property(c.name)\n</code></pre> <p>Key Components:</p> <ul> <li>Nodes: Graph vertices with labels and variable bindings</li> <li>Edges: Graph edges with types and directionality</li> <li>Patterns: Combinations of nodes and edges</li> <li>Clauses: MATCH, WHERE, RETURN, WITH, ORDER BY, LIMIT, etc.</li> <li>Expressions: Property access, functions, operators, literals</li> </ul>"},{"location":"query-translation/#phase-2-planner","title":"Phase 2: Planner","text":"<p>Input: AST Output: Logical query plan (operator tree)</p> <p>The planner converts the declarative AST into an imperative execution plan using relational operators.</p> <p>Operators:</p> <ul> <li><code>DataSourceOperator</code> - Read from a table</li> <li><code>JoinOperator</code> - Join two data sources</li> <li><code>FilterOperator</code> - Apply WHERE conditions</li> <li><code>ProjectionOperator</code> - Select columns (RETURN clause)</li> <li><code>AggregationOperator</code> - GROUP BY and aggregation functions</li> <li><code>SortOperator</code> - ORDER BY</li> <li><code>LimitOperator</code> - LIMIT clause</li> <li><code>RecursiveTraversalOperator</code> - Variable-length paths (<code>-[:REL*1..N]-</code>)</li> <li><code>UnionOperator</code> - UNION queries</li> <li><code>SubqueryOperator</code> - Subqueries and WITH clauses</li> </ul> <p>Example Query Plan:</p> <p>For the query above:</p> Text Only<pre><code>ProjectionOperator [p.name, c.name]\n\u2514\u2500 FilterOperator [c.industry = 'Technology']\n   \u2514\u2500 JoinOperator [INNER]\n      \u251c\u2500 JoinOperator [INNER]\n      \u2502  \u251c\u2500 DataSourceOperator [Person]\n      \u2502  \u2514\u2500 DataSourceOperator [PersonWorksAt]\n      \u2514\u2500 DataSourceOperator [Company]\n</code></pre> <p>Planning Strategy:</p> <ol> <li>Pattern Decomposition: Break MATCH patterns into node and edge components</li> <li>Join Planning: Determine join order and conditions based on relationships</li> <li>Filter Pushdown: Move WHERE conditions as close to data sources as possible</li> <li>Projection Planning: Track which columns are needed at each stage</li> </ol>"},{"location":"query-translation/#phase-3-resolver","title":"Phase 3: Resolver","text":"<p>Input: Logical plan Output: Resolved plan with column references</p> <p>The resolver performs semantic analysis and column resolution:</p> <ol> <li>Schema Lookup: Map node/edge labels to table names</li> <li>Column Resolution: Resolve property references to table columns</li> <li>Type Checking: Validate expression types</li> <li>Alias Generation: Create unique column aliases to avoid conflicts</li> <li>Dependency Tracking: Ensure required columns flow through operators</li> </ol> <p>Column Naming Convention:</p> <p>All columns are prefixed with <code>_gsql2rsql_</code> to avoid conflicts:</p> <ul> <li><code>p.id</code> \u2192 <code>_gsql2rsql_p_id</code></li> <li><code>c.name</code> \u2192 <code>_gsql2rsql_c_name</code></li> <li><code>rel.since</code> \u2192 <code>_gsql2rsql_rel_since</code></li> </ul> <p>Example Resolution:</p> Cypher<pre><code>MATCH (p:Person)\nWHERE p.age &gt; 30\nRETURN p.name\n</code></pre> <p>Schema: JSON<pre><code>{\n  \"nodes\": [{\n    \"name\": \"Person\",\n    \"tableName\": \"catalog.db.Person\",\n    \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n    \"properties\": [\n      {\"name\": \"name\", \"type\": \"string\"},\n      {\"name\": \"age\", \"type\": \"int\"}\n    ]\n  }]\n}\n</code></pre></p> <p>Resolved References: - <code>p</code> \u2192 <code>catalog.db.Person</code> table - <code>p.age</code> \u2192 <code>catalog.db.Person.age</code> column \u2192 <code>_gsql2rsql_p_age</code> - <code>p.name</code> \u2192 <code>catalog.db.Person.name</code> column \u2192 <code>_gsql2rsql_p_name</code></p>"},{"location":"query-translation/#phase-4-renderer","title":"Phase 4: Renderer","text":"<p>Input: Resolved logical plan Output: SQL query string</p> <p>The renderer converts the logical plan into executable SQL.</p> <p>Rendering Strategy:</p> <ol> <li>Bottom-Up Traversal: Start from leaf operators (data sources)</li> <li>Subquery Generation: Each operator generates a SQL fragment</li> <li>Column Tracking: Maintain available columns at each level</li> <li>Join Rendering: Generate INNER/LEFT/CROSS joins based on operator type</li> <li>Alias Management: Create unique table aliases (<code>_left</code>, <code>_right</code>, <code>_proj</code>)</li> </ol> <p>Example Rendering:</p> <p>DataSourceOperator(Person): SQL<pre><code>SELECT\n   id AS _gsql2rsql_p_id\n  ,name AS _gsql2rsql_p_name\n  ,age AS _gsql2rsql_p_age\nFROM\n  catalog.db.Person\n</code></pre></p> <p>FilterOperator(age &gt; 30): SQL<pre><code>SELECT *\nFROM (\n  -- previous operator SQL\n) AS _filter\nWHERE (_gsql2rsql_p_age) &gt; (30)\n</code></pre></p> <p>ProjectionOperator(p.name): SQL<pre><code>SELECT\n  _gsql2rsql_p_name AS name\nFROM (\n  -- previous operator SQL\n) AS _proj\n</code></pre></p>"},{"location":"query-translation/#pattern-translation","title":"Pattern Translation","text":""},{"location":"query-translation/#simple-match","title":"Simple Match","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)\nRETURN p.name\n</code></pre></p> <p>SQL: SQL<pre><code>SELECT\n  _gsql2rsql_p_name AS name\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n  FROM\n    catalog.db.Person\n) AS _proj\n</code></pre></p>"},{"location":"query-translation/#relationship-traversal","title":"Relationship Traversal","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nRETURN p.name, c.name\n</code></pre></p> <p>SQL: SQL<pre><code>SELECT\n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_c_name AS name_2\nFROM (\n  SELECT\n     _left._gsql2rsql_p_name\n    ,_right._gsql2rsql_c_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_company_id\n    FROM (\n      SELECT id AS _gsql2rsql_p_id, name AS _gsql2rsql_p_name\n      FROM catalog.db.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT person_id AS _gsql2rsql__anon1_person_id,\n             company_id AS _gsql2rsql__anon1_company_id\n      FROM catalog.db.PersonWorksAt\n    ) AS _right\n    ON (_left._gsql2rsql_p_id) = (_right._gsql2rsql__anon1_person_id)\n  ) AS _left\n  INNER JOIN (\n    SELECT id AS _gsql2rsql_c_id, name AS _gsql2rsql_c_name\n    FROM catalog.db.Company\n  ) AS _right\n  ON (_left._gsql2rsql__anon1_company_id) = (_right._gsql2rsql_c_id)\n) AS _proj\n</code></pre></p>"},{"location":"query-translation/#variable-length-paths","title":"Variable-Length Paths","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:KNOWS*1..3]-(friend:Person)\nRETURN p.name, friend.name\n</code></pre></p> <p>SQL: SQL<pre><code>WITH RECURSIVE _recursive_traversal AS (\n  -- Base case: 1 hop\n  SELECT\n     source_id AS _gsql2rsql_start_id\n    ,sink_id AS _gsql2rsql_end_id\n    ,1 AS _gsql2rsql_depth\n  FROM catalog.db.PersonKnows\n\n  UNION ALL\n\n  -- Recursive case: up to 3 hops\n  SELECT\n     rt._gsql2rsql_start_id\n    ,edge.sink_id AS _gsql2rsql_end_id\n    ,rt._gsql2rsql_depth + 1 AS _gsql2rsql_depth\n  FROM _recursive_traversal rt\n  INNER JOIN catalog.db.PersonKnows edge\n    ON rt._gsql2rsql_end_id = edge.source_id\n  WHERE rt._gsql2rsql_depth &lt; 3\n)\nSELECT\n   p.name AS name\n  ,friend.name AS name_2\nFROM catalog.db.Person p\nINNER JOIN _recursive_traversal rt\n  ON p.id = rt._gsql2rsql_start_id\nINNER JOIN catalog.db.Person friend\n  ON rt._gsql2rsql_end_id = friend.id\n</code></pre></p>"},{"location":"query-translation/#aggregation","title":"Aggregation","text":"<p>Cypher: Cypher<pre><code>MATCH (p:Person)-[:WORKS_AT]-&gt;(c:Company)\nRETURN c.name, COUNT(p) AS employee_count\nORDER BY employee_count DESC\n</code></pre></p> <p>SQL: SQL<pre><code>SELECT\n   _gsql2rsql_c_name AS name\n  ,_gsql2rsql_employee_count AS employee_count\nFROM (\n  SELECT\n     _gsql2rsql_c_name\n    ,COUNT(_gsql2rsql_p_id) AS _gsql2rsql_employee_count\n  FROM (\n    -- join query from previous example\n  ) AS _agg_input\n  GROUP BY _gsql2rsql_c_name\n) AS _proj\nORDER BY _gsql2rsql_employee_count DESC\n</code></pre></p>"},{"location":"query-translation/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"query-translation/#1-filter-pushdown","title":"1. Filter Pushdown","text":"<p>Move WHERE conditions as close to data sources as possible to reduce intermediate data size.</p> <p>Before: SQL<pre><code>SELECT * FROM (\n  SELECT * FROM person p JOIN company c ON p.company_id = c.id\n) WHERE c.industry = 'Technology'\n</code></pre></p> <p>After: SQL<pre><code>SELECT * FROM person p\nJOIN (SELECT * FROM company WHERE industry = 'Technology') c\nON p.company_id = c.id\n</code></pre></p>"},{"location":"query-translation/#2-column-pruning","title":"2. Column Pruning","text":"<p>Only select columns that are actually needed by downstream operators.</p> <p>Before: SQL<pre><code>SELECT id, name, age, email FROM person\n</code></pre></p> <p>After (only name is needed): SQL<pre><code>SELECT id, name FROM person\n</code></pre></p>"},{"location":"query-translation/#3-join-reordering","title":"3. Join Reordering","text":"<p>Order joins to minimize intermediate result sizes (currently basic heuristics; future: cost-based optimization).</p>"},{"location":"query-translation/#4-predicate-deduplication","title":"4. Predicate Deduplication","text":"<p>Remove duplicate filter conditions that appear multiple times in the query.</p>"},{"location":"query-translation/#limitations-and-edge-cases","title":"Limitations and Edge Cases","text":"<p>See Limitations for a complete list of unsupported features and known issues.</p> <p>Key Limitations:</p> <ul> <li>Undirected Relationships: Cypher <code>-[:REL]-</code> requires UNION of both directions</li> <li>Multiple Labels: Cypher <code>(n:Label1:Label2)</code> not fully supported</li> <li>Map Projections: <code>RETURN {name: p.name, age: p.age}</code> not supported</li> <li>List Comprehensions: <code>[x IN list | x.property]</code> not supported</li> <li>Pattern Comprehensions: <code>[(a)--&gt;(b) | b.name]</code> not supported</li> </ul>"},{"location":"query-translation/#debugging-translation","title":"Debugging Translation","text":""},{"location":"query-translation/#view-query-plan","title":"View Query Plan","text":"<p>Use the <code>explain</code> command to see the logical plan:</p> Bash<pre><code>gsql2rsql explain --schema schema.json &lt; query.cypher\n</code></pre>"},{"location":"query-translation/#enable-debug-logging","title":"Enable Debug Logging","text":"Bash<pre><code>gsql2rsql translate --schema schema.json --debug &lt; query.cypher\n</code></pre> <p>This shows: - Parse tree structure - Logical plan operators - Column resolution details - SQL generation steps</p>"},{"location":"query-translation/#manual-inspection","title":"Manual Inspection","text":"<p>Save generated SQL and inspect it:</p> Bash<pre><code>gsql2rsql translate --schema schema.json &lt; query.cypher &gt; output.sql\ncat output.sql | jq  # if SQL is formatted as JSON\n</code></pre>"},{"location":"query-translation/#best-practices","title":"Best Practices","text":"<ol> <li>Schema Design: Use meaningful table and column names that reflect graph semantics</li> <li>Indexing: Create indexes on ID columns used in joins for better performance</li> <li>Query Structure: Break complex queries into WITH clauses for better readability</li> <li>Testing: Validate generated SQL on a sample dataset before production use</li> <li>Monitoring: Track query performance and optimize based on execution plans</li> </ol>"},{"location":"query-translation/#see-also","title":"See Also","text":"<ul> <li>Architecture - Detailed architecture documentation</li> <li>CLI Commands - Command reference</li> <li>Examples - Real-world query examples</li> <li>Decision Log - Design decisions and rationale</li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":""},{"location":"quickstart/#installation","title":"Installation","text":"Bash<pre><code># Clone repository (INFERRED - replace with actual repo URL)\ncd /path/to/cyper2dsql/python\n\n# Create virtual environment and install dependencies\nuv venv\nuv sync --extra dev\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"quickstart/#environment-requirements","title":"Environment Requirements","text":"<ul> <li>Python: 3.12 or 3.13</li> <li>Target Runtime: Databricks Runtime 17+ (requires <code>WITH RECURSIVE</code> CTE support)</li> <li>Spark Version: 3.5+ (for PySpark validation tests)</li> <li>Key Dependencies:</li> <li><code>antlr4-python3-runtime&gt;=4.13.0</code> (parser)</li> <li><code>click&gt;=8.1.0</code> (CLI)</li> <li><code>textual&gt;=0.47.0</code> (interactive TUI)</li> <li><code>pyspark&gt;=3.5.0</code> (dev only, for validation)</li> </ul>"},{"location":"quickstart/#minimal-example-simple-query","title":"Minimal Example: Simple Query","text":""},{"location":"quickstart/#1-create-a-schema-file","title":"1. Create a Schema File","text":"<p>OpenCypher queries need a graph schema defining nodes and edges. Create <code>my_schema.json</code>:</p> JSON<pre><code>{\n  \"nodes\": [\n    {\n      \"name\": \"Person\",\n      \"tableName\": \"graph.Person\",\n      \"idProperty\": {\"name\": \"id\", \"type\": \"int\"},\n      \"properties\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n      ]\n    }\n  ],\n  \"edges\": [\n    {\n      \"name\": \"KNOWS\",\n      \"sourceNode\": \"Person\",\n      \"sinkNode\": \"Person\",\n      \"tableName\": \"graph.Knows\",\n      \"sourceIdProperty\": {\"name\": \"source_id\", \"type\": \"int\"},\n      \"sinkIdProperty\": {\"name\": \"target_id\", \"type\": \"int\"},\n      \"properties\": []\n    }\n  ]\n}\n</code></pre>"},{"location":"quickstart/#2-transpile-a-query","title":"2. Transpile a Query","text":"<p>Cypher input: Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-&gt;(f:Person)\nWHERE p.age &gt; 30\nRETURN p.name, f.name, f.age\n</code></pre></p> <p>Command: Bash<pre><code>echo \"MATCH (p:Person)-[:KNOWS]-&gt;(f:Person) WHERE p.age &gt; 30 RETURN p.name, f.name, f.age\" | \\\n  uv run gsql2rsql transpile --schema my_schema.json\n</code></pre></p> <p>Generated SQL: SQL<pre><code>SELECT\n  _gsql2rsql_p_name AS name,\n  _gsql2rsql_f_name AS name,\n  _gsql2rsql_f_age AS age\nFROM (\n  SELECT\n    sink.id AS _gsql2rsql_f_id,\n    sink.name AS _gsql2rsql_f_name,\n    sink.age AS _gsql2rsql_f_age,\n    source.id AS _gsql2rsql_p_id,\n    source.name AS _gsql2rsql_p_name,\n    source.age AS _gsql2rsql_p_age\n  FROM\n    graph.Knows AS edge\n  INNER JOIN graph.Person AS source ON edge.source_id = source.id\n  INNER JOIN graph.Person AS sink ON edge.target_id = sink.id\n  WHERE source.age &gt; 30\n) AS _proj\n</code></pre></p>"},{"location":"quickstart/#3-variable-length-path-example","title":"3. Variable-Length Path Example","text":"<p>Cypher input (BFS traversal): Cypher<pre><code>MATCH (root:Person)-[:KNOWS*1..5]-&gt;(neighbor:Person)\nWHERE root.id = 1\nRETURN DISTINCT neighbor.id, neighbor.name\n</code></pre></p> <p>Command: Bash<pre><code>echo \"MATCH (root:Person)-[:KNOWS*1..5]-&gt;(neighbor:Person) WHERE root.id = 1 RETURN DISTINCT neighbor.id, neighbor.name\" | \\\n  uv run gsql2rsql transpile --schema my_schema.json\n</code></pre></p> <p>Generated SQL (uses WITH RECURSIVE): SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_id AS start_node,\n      e.target_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_id, e.target_id) AS path,\n      ARRAY(e.source_id) AS visited\n    FROM graph.Knows e\n    UNION ALL\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.source_id)) AS visited\n    FROM paths_1 p\n    JOIN graph.Knows e ON p.end_node = e.source_id\n    WHERE p.depth &lt; 5\n      AND NOT array_contains(p.visited, e.target_id)\n  )\nSELECT DISTINCT\n  _gsql2rsql_neighbor_id AS id,\n  _gsql2rsql_neighbor_name AS name\nFROM (\n  SELECT\n    sink.id AS _gsql2rsql_neighbor_id,\n    sink.name AS _gsql2rsql_neighbor_name,\n    source.id AS _gsql2rsql_root_id\n  FROM paths_1 p\n  JOIN graph.Person sink ON sink.id = p.end_node\n  JOIN graph.Person source ON source.id = p.start_node\n  WHERE p.depth &gt;= 1 AND p.depth &lt;= 5\n    AND source.id = 1\n) AS _proj\n</code></pre></p>"},{"location":"quickstart/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests (fast, excludes PySpark)\nmake test-no-pyspark\n\n# Run all tests including PySpark validation (slower)\nmake test\n\n# Run with coverage report\nmake test-cov\n\n# Run specific feature test\nuv run pytest tests/transpile_tests/test_01_simple_node_lookup.py -v\n</code></pre>"},{"location":"quickstart/#running-examples-from-yaml","title":"Running Examples from YAML","text":"<p>The transpiler includes curated examples in <code>examples/*.yaml</code>:</p> Bash<pre><code># Interactive TUI mode (browse examples, live transpilation)\nuv run gsql2rsql tui --examples examples/credit_queries.yaml\n\n# Run PySpark validation on all examples\nmake test-pyspark-examples\n\n# Run quick subset (first 5 credit queries)\nmake test-pyspark-quick\n</code></pre>"},{"location":"quickstart/#cli-commands-reference","title":"CLI Commands Reference","text":"Bash<pre><code># Show help\nuv run gsql2rsql --help\n\n# Transpile from stdin\necho \"MATCH (n:Person) RETURN n\" | uv run gsql2rsql transpile -s schema.json\n\n# Transpile from file\nuv run gsql2rsql transpile -s schema.json -i query.cypher\n\n# Enable optimization (conservative subquery flattening)\nuv run gsql2rsql transpile -s schema.json --optimize\n\n# Show scope debugging information\nuv run gsql2rsql transpile -s schema.json --explain-scopes\n\n# Parse only (show AST without transpilation)\nuv run gsql2rsql parse -i query.cypher\n\n# Generate schema template\nuv run gsql2rsql init-schema &gt; my_schema.json\n</code></pre>"},{"location":"quickstart/#databricks-runtime-constraints","title":"Databricks Runtime Constraints","text":"<ul> <li>Minimum Runtime: 17+ (requires <code>WITH RECURSIVE</code> support)</li> <li>SQL Features Used:</li> <li><code>WITH RECURSIVE</code> for variable-length paths</li> <li><code>ARRAY()</code> and <code>CONCAT()</code> for path tracking</li> <li><code>array_contains()</code> for cycle detection</li> <li><code>STRUCT()</code> for edge property collections</li> <li><code>COALESCE()</code> for <code>OPTIONAL MATCH</code> null handling</li> <li>Known Issues: Certain aggregation patterns may require runtime 18+ (INFERRED from test history)</li> </ul>"},{"location":"quickstart/#where-to-look-next","title":"Where to Look Next","text":"<ul> <li>examples/credit_queries.yaml \u2014 Credit analysis domain examples</li> <li>examples/fraud_queries.yaml \u2014 Fraud detection patterns</li> <li>examples/features_queries.yaml \u2014 Feature showcase</li> <li>02-architecture.md \u2014 Understand the transpilation pipeline</li> <li>05-testing-and-examples.md \u2014 Test structure and golden files</li> </ul>"},{"location":"examples/","title":"Query Examples","text":"<p>Welcome to the gsql2rsql query examples gallery!</p> <p>This section demonstrates the transpiler's capabilities across different domains. Each example shows the original OpenCypher query alongside the generated Databricks SQL.</p>"},{"location":"examples/#available-categories","title":"Available Categories","text":""},{"location":"examples/#credit","title":"Credit","text":"<ul> <li>Total Queries: 15</li> <li>Successful: 15</li> <li>Failed: 0</li> </ul>"},{"location":"examples/#fraud","title":"Fraud","text":"<ul> <li>Total Queries: 17</li> <li>Successful: 17</li> <li>Failed: 0</li> </ul>"},{"location":"examples/#features","title":"Features","text":"<ul> <li>Total Queries: 37</li> <li>Successful: 37</li> <li>Failed: 0</li> </ul>"},{"location":"examples/#about-these-examples","title":"About These Examples","text":"<p>All queries are sourced from real-world use cases in:</p> <ul> <li>Fraud Detection: Graph-based fraud ring detection, anomaly identification</li> <li>Credit Analysis: Relationship-based credit risk assessment</li> <li>Feature Engineering: Graph features for ML models</li> </ul> <p>Try These Yourself</p> <p>You can run any of these queries through the transpiler using: Bash<pre><code>gsql2rsql translate --schema examples/fraud_queries.yaml \"&lt;your-query&gt;\"\n</code></pre></p>"},{"location":"examples/credit/","title":"Credit Queries","text":"<p>This page contains transpiled examples for credit queries queries.</p> <p>Each example shows the original OpenCypher query and its corresponding Databricks SQL translation.</p>"},{"location":"examples/credit/#1-calculate-credit-risk-scores-based-on-transaction-history","title":"1. Calculate credit risk scores based on transaction history","text":"<p>Application: Credit: Risk scoring</p> Notes <p>Analyzes recent transaction patterns to assess credit risk. High overdraft rates indicate elevated default risk.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)-[:TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P90D')\nWITH c, a,\n     COUNT(t) AS tx_count,\n     AVG(t.amount) AS avg_transaction,\n     SUM(CASE WHEN t.type = 'overdraft' THEN 1 ELSE 0 END) AS overdraft_count\nRETURN c.id, c.name,\n       tx_count,\n       avg_transaction,\n       overdraft_count,\n       (overdraft_count * 1.0 / tx_count) AS overdraft_rate\nORDER BY overdraft_rate DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,tx_count AS tx_count\n  ,avg_transaction AS avg_transaction\n  ,overdraft_count AS overdraft_count\n  ,((overdraft_count) * (1.0)) / (tx_count) AS overdraft_rate\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,COUNT(_gsql2rsql_t_id) AS tx_count\n    ,AVG(CAST(_gsql2rsql_t_amount AS DOUBLE)) AS avg_transaction\n    ,SUM(CASE WHEN (_gsql2rsql_t_type) = ('overdraft') THEN 1 ELSE 0 END) AS overdraft_count\n    ,_gsql2rsql_a_balance AS _gsql2rsql_a_balance\n    ,_gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n      ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n      ,_right._gsql2rsql_t_type AS _gsql2rsql_t_type\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n        ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_right._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n          ,_right._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,account_id AS _gsql2rsql__anon1_account_id\n            FROM\n              catalog.credit.CustomerAccount\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a_id\n            ,balance AS _gsql2rsql_a_balance\n            ,customer_id AS _gsql2rsql_a_customer_id\n          FROM\n            catalog.credit.Account\n        ) AS _right ON\n          _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.AccountTransaction\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n        ,type AS _gsql2rsql_t_type\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 90 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_a_id, _gsql2rsql_a_balance, _gsql2rsql_a_customer_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n) AS _proj\nORDER BY overdraft_rate DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, a=a, tx_count=COUNT(t), avg_transaction=AVG(t.amount), overdraft_count=SUM(CASE WHEN (t.type EQ 'overdraft') THEN 1 ELSE 0 END)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P90D')))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, tx_count=tx_count, avg_transaction=avg_transaction, overdraft_count=overdraft_count, overdraft_rate=((overdraft_count MULTIPLY 1.0) DIVIDE tx_count)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#2-identify-credit-worthy-customers-via-payment-consistency","title":"2. Identify credit-worthy customers via payment consistency","text":"<p>Application: Credit: Payment reliability assessment</p> Notes <p>Finds customers with excellent payment history for credit line increases. High on-time rates indicate low default probability.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_LOAN]-&gt;(l:Loan)-[:PAYMENT]-&gt;(p:Payment)\nWHERE l.status = 'active'\nWITH c, l,\n     COUNT(p) AS total_payments,\n     SUM(CASE WHEN p.on_time = true THEN 1 ELSE 0 END) AS on_time_payments\nWHERE total_payments &gt; 6\nWITH c, l, total_payments, on_time_payments,\n     (on_time_payments * 1.0 / total_payments) AS on_time_rate\nWHERE on_time_rate &gt; 0.95\nRETURN c.id, c.name, l.amount, on_time_rate, total_payments\nORDER BY l.amount DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,_gsql2rsql_l_amount AS amount\n  ,on_time_rate AS on_time_rate\n  ,total_payments AS total_payments\nFROM (\n  SELECT *\n  FROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_l_id AS _gsql2rsql_l_id\n    ,total_payments AS total_payments\n    ,on_time_payments AS on_time_payments\n    ,((on_time_payments) * (1.0)) / (total_payments) AS on_time_rate\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    ,_gsql2rsql_l_amount AS _gsql2rsql_l_amount\n    ,_gsql2rsql_l_balance AS _gsql2rsql_l_balance\n    ,_gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n    ,_gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n    ,_gsql2rsql_l_status AS _gsql2rsql_l_status\n  FROM (\n    SELECT \n       _gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,COUNT(_gsql2rsql_p_id) AS total_payments\n      ,SUM(CASE WHEN (_gsql2rsql_p_on_time) = (TRUE) THEN 1 ELSE 0 END) AS on_time_payments\n      ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_gsql2rsql_l_amount AS _gsql2rsql_l_amount\n      ,_gsql2rsql_l_balance AS _gsql2rsql_l_balance\n      ,_gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n      ,_gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n      ,_gsql2rsql_l_status AS _gsql2rsql_l_status\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n        ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n        ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n        ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n        ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n        ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n        ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n        ,_left._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n        ,_left._gsql2rsql__anon2_payment_id AS _gsql2rsql__anon2_payment_id\n        ,_right._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_right._gsql2rsql_p_on_time AS _gsql2rsql_p_on_time\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n          ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n          ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n          ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n          ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n          ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n          ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n          ,_right._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n          ,_right._gsql2rsql__anon2_payment_id AS _gsql2rsql__anon2_payment_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n            ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n            ,_right._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n            ,_right._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n            ,_right._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n            ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n            ,_right._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n          FROM (\n            SELECT\n               _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n              ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n              ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n              ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n            FROM (\n              SELECT\n                 id AS _gsql2rsql_c_id\n                ,name AS _gsql2rsql_c_name\n                ,status AS _gsql2rsql_c_status\n              FROM\n                catalog.credit.Customer\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 customer_id AS _gsql2rsql__anon1_customer_id\n                ,loan_id AS _gsql2rsql__anon1_loan_id\n              FROM\n                catalog.credit.CustomerLoan\n            ) AS _right ON\n              _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               id AS _gsql2rsql_l_id\n              ,amount AS _gsql2rsql_l_amount\n              ,balance AS _gsql2rsql_l_balance\n              ,interest_rate AS _gsql2rsql_l_interest_rate\n              ,status AS _gsql2rsql_l_status\n              ,origination_date AS _gsql2rsql_l_origination_date\n            FROM\n              catalog.credit.Loan\n            WHERE ((status) = ('active'))\n          ) AS _right ON\n            _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             loan_id AS _gsql2rsql__anon2_loan_id\n            ,payment_id AS _gsql2rsql__anon2_payment_id\n          FROM\n            catalog.credit.LoanPayment\n        ) AS _right ON\n          _left._gsql2rsql_l_id = _right._gsql2rsql__anon2_loan_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_p_id\n          ,on_time AS _gsql2rsql_p_on_time\n        FROM\n          catalog.credit.Payment\n      ) AS _right ON\n        _right._gsql2rsql_p_id = _left._gsql2rsql__anon2_payment_id\n    ) AS _proj\n    GROUP BY _gsql2rsql_c_id, _gsql2rsql_l_id, _gsql2rsql_c_name, _gsql2rsql_c_status, _gsql2rsql_l_amount, _gsql2rsql_l_balance, _gsql2rsql_l_interest_rate, _gsql2rsql_l_origination_date, _gsql2rsql_l_status\n    HAVING (total_payments) &gt; (6)\n  ) AS _proj\n  ) AS _filter\n  WHERE (on_time_rate) &gt; (0.95)\n) AS _proj\nORDER BY _gsql2rsql_l_amount DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_LOAN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: l:Loan\n    Filter: (l.status EQ 'active')\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:PAYMENT]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: p:Payment\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, l=l, total_payments=COUNT(p), on_time_payments=SUM(CASE WHEN (p.on_time EQ true) THEN 1 ELSE 0 END)\n    Having: (total_payments GT 6)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=13;\n  ProjectionOperator(id=12)\n    Projections: c=c, l=l, total_payments=total_payments, on_time_payments=on_time_payments, on_time_rate=((on_time_payments MULTIPLY 1.0) DIVIDE total_payments)\n    Having: (on_time_rate GT 0.95)\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=13 Op=ProjectionOperator; InOpIds=12; OutOpIds=;\n  ProjectionOperator(id=13)\n    Projections: id=c.id, name=c.name, amount=l.amount, on_time_rate=on_time_rate, total_payments=total_payments\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#3-trace-debt-consolidation-opportunities-via-multiple-loan-analysis","title":"3. Trace debt consolidation opportunities via multiple loan analysis","text":"<p>Application: Credit: Debt consolidation</p> Notes <p>Identifies customers with multiple active loans suitable for consolidation. Can improve customer retention and reduce default risk.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_LOAN]-&gt;(l:Loan)\nWHERE l.status = 'active'\nWITH c, COUNT(l) AS active_loans, SUM(l.balance) AS total_debt, AVG(l.interest_rate) AS avg_rate\nWHERE active_loans &gt;= 3 AND total_debt &gt; 10000\nRETURN c.id, c.name, active_loans, total_debt, avg_rate\nORDER BY total_debt DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,active_loans AS active_loans\n  ,total_debt AS total_debt\n  ,avg_rate AS avg_rate\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,COUNT(_gsql2rsql_l_id) AS active_loans\n    ,SUM(_gsql2rsql_l_balance) AS total_debt\n    ,AVG(CAST(_gsql2rsql_l_interest_rate AS DOUBLE)) AS avg_rate\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,_right._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n      ,_right._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n      ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,name AS _gsql2rsql_c_name\n          ,status AS _gsql2rsql_c_status\n        FROM\n          catalog.credit.Customer\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           customer_id AS _gsql2rsql__anon1_customer_id\n          ,loan_id AS _gsql2rsql__anon1_loan_id\n        FROM\n          catalog.credit.CustomerLoan\n      ) AS _right ON\n        _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_l_id\n        ,balance AS _gsql2rsql_l_balance\n        ,interest_rate AS _gsql2rsql_l_interest_rate\n        ,status AS _gsql2rsql_l_status\n      FROM\n        catalog.credit.Loan\n      WHERE ((status) = ('active'))\n    ) AS _right ON\n      _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING ((active_loans) &gt;= (3)) AND ((total_debt) &gt; (10000))\n) AS _proj\nORDER BY total_debt DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_LOAN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: l:Loan\n    Filter: (l.status EQ 'active')\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: c=c, active_loans=COUNT(l), total_debt=SUM(l.balance), avg_rate=AVG(l.interest_rate)\n    Having: ((active_loans GEQ 3) AND (total_debt GT 10000))\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=c.id, name=c.name, active_loans=active_loans, total_debt=total_debt, avg_rate=avg_rate\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#4-predict-default-probability-using-behavioral-patterns","title":"4. Predict default probability using behavioral patterns","text":"<p>Application: Credit: Default prediction</p> Notes <p>Combines multiple risk indicators to predict default probability. NSF fees and late payments are strong default predictors.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)-[:TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P60D')\nWITH c, a,\n     COUNT(CASE WHEN t.type = 'NSF' THEN 1 END) AS nsf_count,\n     COUNT(CASE WHEN t.type = 'late_fee' THEN 1 END) AS late_fee_count,\n     MIN(a.balance) AS min_balance\nWHERE nsf_count &gt; 2 OR late_fee_count &gt; 3 OR min_balance &lt; 0\nRETURN c.id, c.name, nsf_count, late_fee_count, min_balance,\n       (nsf_count + late_fee_count * 2) AS default_risk_score\nORDER BY default_risk_score DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,nsf_count AS nsf_count\n  ,late_fee_count AS late_fee_count\n  ,min_balance AS min_balance\n  ,(nsf_count) + ((late_fee_count) * (2)) AS default_risk_score\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,COUNT(CASE WHEN (_gsql2rsql_t_type) = ('NSF') THEN 1 END) AS nsf_count\n    ,COUNT(CASE WHEN (_gsql2rsql_t_type) = ('late_fee') THEN 1 END) AS late_fee_count\n    ,MIN(_gsql2rsql_a_balance) AS min_balance\n    ,_gsql2rsql_a_balance AS _gsql2rsql_a_balance\n    ,_gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n      ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n      ,_right._gsql2rsql_t_type AS _gsql2rsql_t_type\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n        ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_right._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n          ,_right._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,account_id AS _gsql2rsql__anon1_account_id\n            FROM\n              catalog.credit.CustomerAccount\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a_id\n            ,balance AS _gsql2rsql_a_balance\n            ,customer_id AS _gsql2rsql_a_customer_id\n          FROM\n            catalog.credit.Account\n        ) AS _right ON\n          _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.AccountTransaction\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,timestamp AS _gsql2rsql_t_timestamp\n        ,type AS _gsql2rsql_t_type\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 60 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_a_id, _gsql2rsql_a_balance, _gsql2rsql_a_customer_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING (((nsf_count) &gt; (2)) OR ((late_fee_count) &gt; (3))) OR ((min_balance) &lt; (0))\n) AS _proj\nORDER BY default_risk_score DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, a=a, nsf_count=COUNT(CASE WHEN (t.type EQ 'NSF') THEN 1 END), late_fee_count=COUNT(CASE WHEN (t.type EQ 'late_fee') THEN 1 END), min_balance=MIN(a.balance)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P60D')))\n    Having: (((nsf_count GT 2) OR (late_fee_count GT 3)) OR (min_balance LT 0))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, nsf_count=nsf_count, late_fee_count=late_fee_count, min_balance=min_balance, default_risk_score=(nsf_count PLUS (late_fee_count MULTIPLY 2))\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#5-analyze-transaction-chains-to-assess-liquidity-patterns","title":"5. Analyze transaction chains to assess liquidity patterns","text":"<p>Application: Credit: Liquidity assessment</p> Notes <p>Examines internal transfer patterns to understand liquidity management. Frequent internal transfers may indicate cash flow stress.</p> OpenCypher Query Cypher<pre><code>MATCH path = (source:Account)-[:TRANSFER*1..3]-&gt;(sink:Account)\nWHERE source.customer_id = sink.customer_id\n  AND ALL(rel IN relationships(path) WHERE rel.timestamp &gt; TIMESTAMP() - DURATION('P30D'))\nWITH source.customer_id AS customer_id,\n     COUNT(DISTINCT path) AS transfer_chains,\n     AVG(LENGTH(path)) AS avg_chain_length\nRETURN customer_id, transfer_chains, avg_chain_length\nORDER BY transfer_chains DESC\nLIMIT 20\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_account_id AS start_node,\n      e.target_account_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_account_id, e.target_account_id) AS path,\n      ARRAY(NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      ARRAY(e.source_account_id) AS visited\n    FROM catalog.credit.Transfer e\n    WHERE (e.timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY))\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_account_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_account_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.source_account_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.credit.Transfer e\n      ON p.end_node = e.source_account_id\n    WHERE p.depth &lt; 3\n      AND NOT ARRAY_CONTAINS(p.visited, e.target_account_id)\n      AND (e.timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY))\n  )\nSELECT \n   customer_id AS customer_id\n  ,transfer_chains AS transfer_chains\n  ,avg_chain_length AS avg_chain_length\nFROM (\n  SELECT \n     _gsql2rsql_source_customer_id AS customer_id\n    ,COUNT(DISTINCT _gsql2rsql_path_id) AS transfer_chains\n    ,AVG(CAST((SIZE(_gsql2rsql_path_id) - 1) AS DOUBLE)) AS avg_chain_length\n  FROM (\n    SELECT\n       sink.id AS _gsql2rsql_sink_id\n      ,sink.balance AS _gsql2rsql_sink_balance\n      ,sink.customer_id AS _gsql2rsql_sink_customer_id\n      ,source.id AS _gsql2rsql_source_id\n      ,source.balance AS _gsql2rsql_source_balance\n      ,source.customer_id AS _gsql2rsql_source_customer_id\n      ,p.start_node\n      ,p.end_node\n      ,p.depth\n      ,p.path AS _gsql2rsql_path_id\n      ,p.path_edges AS _gsql2rsql_path_edges\n    FROM paths_1 p\n    JOIN catalog.credit.Account sink\n      ON sink.id = p.end_node\n    JOIN catalog.credit.Account source\n      ON source.id = p.start_node\n    WHERE p.depth &gt;= 1 AND p.depth &lt;= 3\n  ) AS _proj\n  WHERE (_gsql2rsql_source_customer_id) = (_gsql2rsql_sink_customer_id)\n  GROUP BY _gsql2rsql_source_customer_id\n) AS _proj\nORDER BY transfer_chains DESC\nLIMIT 20\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: source:Account\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: sink:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(TRANSFER*1..3, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=6;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=sink RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=4; OutOpIds=7;\n  ProjectionOperator(id=6)\n    Projections: customer_id=source.customer_id, transfer_chains=COUNT(DISTINCT path), avg_chain_length=AVG(LENGTH(path))\n    Filter: (source.customer_id EQ sink.customer_id)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=6; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: customer_id=customer_id, transfer_chains=transfer_chains, avg_chain_length=avg_chain_length\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#6-find-high-value-customers-for-premium-credit-products","title":"6. Find high-value customers for premium credit products","text":"<p>Application: Credit: Customer segmentation</p> Notes <p>Identifies high-value customers suitable for premium offerings. High transaction volume and balances indicate creditworthiness.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)-[:TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P180D')\nWITH c, SUM(t.amount) AS total_volume, AVG(a.balance) AS avg_balance, COUNT(DISTINCT a) AS account_count\nWHERE total_volume &gt; 100000 AND avg_balance &gt; 10000\nRETURN c.id, c.name, total_volume, avg_balance, account_count\nORDER BY total_volume DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,total_volume AS total_volume\n  ,avg_balance AS avg_balance\n  ,account_count AS account_count\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,SUM(_gsql2rsql_t_amount) AS total_volume\n    ,AVG(CAST(_gsql2rsql_a_balance AS DOUBLE)) AS avg_balance\n    ,COUNT(DISTINCT _gsql2rsql_a_id) AS account_count\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_right._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,account_id AS _gsql2rsql__anon1_account_id\n            FROM\n              catalog.credit.CustomerAccount\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a_id\n            ,balance AS _gsql2rsql_a_balance\n          FROM\n            catalog.credit.Account\n        ) AS _right ON\n          _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.AccountTransaction\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 180 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING ((total_volume) &gt; (100000)) AND ((avg_balance) &gt; (10000))\n) AS _proj\nORDER BY total_volume DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, total_volume=SUM(t.amount), avg_balance=AVG(a.balance), account_count=COUNT(DISTINCT a)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P180D')))\n    Having: ((total_volume GT 100000) AND (avg_balance GT 10000))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, total_volume=total_volume, avg_balance=avg_balance, account_count=account_count\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#7-detect-early-warning-signs-of-financial-distress","title":"7. Detect early warning signs of financial distress","text":"<p>Application: Credit: Early warning system</p> Notes <p>Identifies customers with sudden balance declines. Sharp drops may indicate financial distress or income loss.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)-[:TRANSACTION]-&gt;(t:Transaction)\nWITH c, a,\n     AVG(CASE WHEN t.timestamp &gt; TIMESTAMP() - DURATION('P7D') THEN a.balance END) AS recent_avg,\n     AVG(CASE WHEN t.timestamp &lt;= TIMESTAMP() - DURATION('P30D') AND t.timestamp &gt; TIMESTAMP() - DURATION('P60D') THEN a.balance END) AS historical_avg\nWHERE historical_avg &gt; 0 AND recent_avg &lt; historical_avg * 0.5\nRETURN c.id, c.name, historical_avg, recent_avg,\n       ((historical_avg - recent_avg) / historical_avg) AS balance_decline_pct\nORDER BY balance_decline_pct DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,historical_avg AS historical_avg\n  ,recent_avg AS recent_avg\n  ,((historical_avg) - (recent_avg)) / (historical_avg) AS balance_decline_pct\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,AVG(CAST(CASE WHEN (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 7 DAY)) THEN _gsql2rsql_a_balance END AS DOUBLE)) AS recent_avg\n    ,AVG(CAST(CASE WHEN ((_gsql2rsql_t_timestamp) &lt;= ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY))) AND ((_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 60 DAY))) THEN _gsql2rsql_a_balance END AS DOUBLE)) AS historical_avg\n    ,_gsql2rsql_a_balance AS _gsql2rsql_a_balance\n    ,_gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n      ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n        ,_left._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_right._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n          ,_right._gsql2rsql_a_customer_id AS _gsql2rsql_a_customer_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,account_id AS _gsql2rsql__anon1_account_id\n            FROM\n              catalog.credit.CustomerAccount\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a_id\n            ,balance AS _gsql2rsql_a_balance\n            ,customer_id AS _gsql2rsql_a_customer_id\n          FROM\n            catalog.credit.Account\n        ) AS _right ON\n          _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.AccountTransaction\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_a_id, _gsql2rsql_a_balance, _gsql2rsql_a_customer_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING ((historical_avg) &gt; (0)) AND ((recent_avg) &lt; ((historical_avg) * (0.5)))\n) AS _proj\nORDER BY balance_decline_pct DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=10;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=ProjectionOperator; InOpIds=9; OutOpIds=11;\n  ProjectionOperator(id=10)\n    Projections: c=c, a=a, recent_avg=AVG(CASE WHEN (t.timestamp GT (DATETIME() MINUS DURATION('P7D'))) THEN a.balance END), historical_avg=AVG(CASE WHEN ((t.timestamp LEQ (DATETIME() MINUS DURATION('P30D'))) AND (t.timestamp GT (DATETIME() MINUS DURATION('P60D')))) THEN a.balance END)\n    Having: ((historical_avg GT 0) AND (recent_avg LT (historical_avg MULTIPLY 0.5)))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=10; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: id=c.id, name=c.name, historical_avg=historical_avg, recent_avg=recent_avg, balance_decline_pct=((historical_avg MINUS recent_avg) DIVIDE historical_avg)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#8-assess-creditworthiness-via-social-network-analysis","title":"8. Assess creditworthiness via social network analysis","text":"<p>Application: Credit: Network-based scoring</p> Notes <p>Analyzes credit risk based on social network connections. Proximity to defaulted borrowers increases risk score.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:KNOWS*1..2]-(peer:Customer)-[:HAS_LOAN]-&gt;(l:Loan)\nWHERE l.status = 'defaulted'\nWITH c, COUNT(DISTINCT peer) AS defaulted_peers, COUNT(DISTINCT l) AS defaulted_loans\nWHERE defaulted_peers &gt; 0\nRETURN c.id, c.name, defaulted_peers, defaulted_loans,\n       (defaulted_peers * 1.0) AS network_risk_score\nORDER BY network_risk_score DESC\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.customer_id AS start_node,\n      e.knows_customer_id AS end_node,\n      1 AS depth,\n      ARRAY(e.customer_id, e.knows_customer_id) AS path,\n      ARRAY(e.customer_id) AS visited\n    FROM catalog.credit.CustomerKnows e\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.knows_customer_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.knows_customer_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.customer_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.credit.CustomerKnows e\n      ON p.end_node = e.customer_id\n    WHERE p.depth &lt; 2\n      AND NOT ARRAY_CONTAINS(p.visited, e.knows_customer_id)\n  )\nSELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,defaulted_peers AS defaulted_peers\n  ,defaulted_loans AS defaulted_loans\n  ,(defaulted_peers) * (1.0) AS network_risk_score\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,COUNT(DISTINCT _gsql2rsql_peer_id) AS defaulted_peers\n    ,COUNT(DISTINCT _gsql2rsql_l_id) AS defaulted_loans\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql_peer_id AS _gsql2rsql_peer_id\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql_peer_id AS _gsql2rsql_peer_id\n        ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      FROM (\n        SELECT\n           sink.id AS _gsql2rsql_peer_id\n          ,sink.name AS _gsql2rsql_peer_name\n          ,sink.status AS _gsql2rsql_peer_status\n          ,source.id AS _gsql2rsql_c_id\n          ,source.name AS _gsql2rsql_c_name\n          ,source.status AS _gsql2rsql_c_status\n          ,p.start_node\n          ,p.end_node\n          ,p.depth\n          ,p.path\n        FROM paths_1 p\n        JOIN catalog.credit.Customer sink\n          ON sink.id = p.end_node\n        JOIN catalog.credit.Customer source\n          ON source.id = p.start_node\n        WHERE p.depth &gt;= 1 AND p.depth &lt;= 2\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           customer_id AS _gsql2rsql__anon1_customer_id\n          ,loan_id AS _gsql2rsql__anon1_loan_id\n        FROM\n          catalog.credit.CustomerLoan\n      ) AS _right ON\n        _left._gsql2rsql_peer_id = _right._gsql2rsql__anon1_customer_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_l_id\n        ,status AS _gsql2rsql_l_status\n      FROM\n        catalog.credit.Loan\n    ) AS _right ON\n      _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n  ) AS _proj\n  WHERE (_gsql2rsql_l_status) = ('defaulted')\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING (defaulted_peers) &gt; (0)\n) AS _proj\nORDER BY network_risk_score DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: peer:Customer\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=5)\n    DataSource: [_anon1:HAS_LOAN]-&gt;\n*\nOpId=7 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=7)\n    DataSource: l:Loan\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*1..2)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=6;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=peer RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=4,5; OutOpIds=8;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=peer RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=6,7; OutOpIds=10;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=ProjectionOperator; InOpIds=8; OutOpIds=11;\n  ProjectionOperator(id=10)\n    Projections: c=c, defaulted_peers=COUNT(DISTINCT peer), defaulted_loans=COUNT(DISTINCT l)\n    Filter: (l.status EQ 'defaulted')\n    Having: (defaulted_peers GT 0)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=10; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: id=c.id, name=c.name, defaulted_peers=defaulted_peers, defaulted_loans=defaulted_loans, network_risk_score=(defaulted_peers MULTIPLY 1.0)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#9-identify-seasonal-spending-patterns-for-credit-limit-adjustments","title":"9. Identify seasonal spending patterns for credit limit adjustments","text":"<p>Application: Credit: Seasonal analysis</p> Notes <p>Identifies months with above-average spending for each customer. Useful for temporary credit limit increases during peak periods.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_CARD]-&gt;(card:CreditCard)-[:CARD_TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P365D')\nWITH c, card,\n     month(t.timestamp) AS month,\n     SUM(t.amount) AS monthly_spend\nWITH c, card, month, monthly_spend,\n     AVG(monthly_spend) OVER (PARTITION BY c.id) AS avg_monthly_spend\nWHERE monthly_spend &gt; avg_monthly_spend * 1.5\nRETURN c.id, month, monthly_spend, avg_monthly_spend\nORDER BY monthly_spend DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS _gsql2rsql_c_id\n  ,_gsql2rsql_card_id AS _gsql2rsql_card_id\n  ,month AS month\n  ,monthly_spend AS monthly_spend\n  ,AVG(CAST(monthly_spend AS DOUBLE)) AS \n  ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n  ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  ,_gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n  ,_gsql2rsql_card_number AS _gsql2rsql_card_number\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_card_id AS _gsql2rsql_card_id\n    ,MONTH(_gsql2rsql_t_timestamp) AS month\n    ,SUM(_gsql2rsql_t_amount) AS monthly_spend\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    ,_gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n    ,_gsql2rsql_card_number AS _gsql2rsql_card_number\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n      ,_left._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n      ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n      ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n        ,_left._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n        ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n        ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_right._gsql2rsql_card_id AS _gsql2rsql_card_id\n          ,_right._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n          ,_right._gsql2rsql_card_number AS _gsql2rsql_card_number\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,card_id AS _gsql2rsql__anon1_card_id\n            FROM\n              catalog.credit.CustomerCard\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_card_id\n            ,credit_limit AS _gsql2rsql_card_credit_limit\n            ,number AS _gsql2rsql_card_number\n          FROM\n            catalog.credit.CreditCard\n        ) AS _right ON\n          _right._gsql2rsql_card_id = _left._gsql2rsql__anon1_card_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           card_id AS _gsql2rsql__anon2_card_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.CardTransaction\n      ) AS _right ON\n        _left._gsql2rsql_card_id = _right._gsql2rsql__anon2_card_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 365 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_card_id, MONTH(_gsql2rsql_t_timestamp), _gsql2rsql_c_name, _gsql2rsql_c_status, _gsql2rsql_card_credit_limit, _gsql2rsql_card_number\n) AS _proj\nGROUP BY _gsql2rsql_c_id, _gsql2rsql_card_id, month, monthly_spend, _gsql2rsql_c_name, _gsql2rsql_c_status, _gsql2rsql_card_credit_limit, _gsql2rsql_card_number\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: card:CreditCard\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:CARD_TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, card=card, month=DATE_MONTH(t.timestamp), monthly_spend=SUM(t.amount)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P365D')))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: c=c, card=card, month=month, monthly_spend=monthly_spend, =AVG(monthly_spend)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#10-calculate-debt-to-income-ratio-estimates-from-transaction-data","title":"10. Calculate debt-to-income ratio estimates from transaction data","text":"<p>Application: Credit: DTI estimation</p> Notes <p>Estimates debt-to-income ratio from transaction patterns. DTI is a critical metric for credit approval decisions.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)-[:TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P90D')\nWITH c,\n     SUM(CASE WHEN t.category = 'income' THEN t.amount ELSE 0 END) AS income,\n     SUM(CASE WHEN t.category = 'debt_payment' THEN t.amount ELSE 0 END) AS debt_payments\nWHERE income &gt; 0\nRETURN c.id, c.name, income, debt_payments,\n       (debt_payments * 1.0 / income) AS estimated_dti\nORDER BY estimated_dti DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,income AS income\n  ,debt_payments AS debt_payments\n  ,((debt_payments) * (1.0)) / (income) AS estimated_dti\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,SUM(CASE WHEN (_gsql2rsql_t_category) = ('income') THEN _gsql2rsql_t_amount ELSE 0 END) AS income\n    ,SUM(CASE WHEN (_gsql2rsql_t_category) = ('debt_payment') THEN _gsql2rsql_t_amount ELSE 0 END) AS debt_payments\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n      ,_right._gsql2rsql_t_category AS _gsql2rsql_t_category\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,account_id AS _gsql2rsql__anon1_account_id\n            FROM\n              catalog.credit.CustomerAccount\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a_id\n          FROM\n            catalog.credit.Account\n        ) AS _right ON\n          _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.AccountTransaction\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n        ,category AS _gsql2rsql_t_category\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 90 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING (income) &gt; (0)\n) AS _proj\nORDER BY estimated_dti DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, income=SUM(CASE WHEN (t.category EQ 'income') THEN t.amount ELSE 0 END), debt_payments=SUM(CASE WHEN (t.category EQ 'debt_payment') THEN t.amount ELSE 0 END)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P90D')))\n    Having: (income GT 0)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, income=income, debt_payments=debt_payments, estimated_dti=((debt_payments MULTIPLY 1.0) DIVIDE income)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#11-find-cross-sell-opportunities-for-additional-credit-products","title":"11. Find cross-sell opportunities for additional credit products","text":"<p>Application: Credit: Cross-sell targeting</p> Notes <p>Identifies customers without loans but with strong deposit relationships. Prime candidates for personal loan or credit card offers.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_ACCOUNT]-&gt;(a:Account)\nWHERE NOT (c)-[:HAS_LOAN]-&gt;(:Loan) AND a.balance &gt; 5000\nWITH c, AVG(a.balance) AS avg_balance, COUNT(a) AS account_count\nWHERE account_count &gt;= 2\nRETURN c.id, c.name, avg_balance, account_count\nORDER BY avg_balance DESC\nLIMIT 50\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,avg_balance AS avg_balance\n  ,account_count AS account_count\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,AVG(CAST(_gsql2rsql_a_balance AS DOUBLE)) AS avg_balance\n    ,COUNT(_gsql2rsql_a_id) AS account_count\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_right._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_right._gsql2rsql_a_balance AS _gsql2rsql_a_balance\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,name AS _gsql2rsql_c_name\n          ,status AS _gsql2rsql_c_status\n        FROM\n          catalog.credit.Customer\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           customer_id AS _gsql2rsql__anon1_customer_id\n          ,account_id AS _gsql2rsql__anon1_account_id\n        FROM\n          catalog.credit.CustomerAccount\n      ) AS _right ON\n        _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_a_id\n        ,balance AS _gsql2rsql_a_balance\n      FROM\n        catalog.credit.Account\n      WHERE ((balance) &gt; (5000))\n    ) AS _right ON\n      _right._gsql2rsql_a_id = _left._gsql2rsql__anon1_account_id\n  ) AS _proj\n  WHERE NOT (EXISTS (SELECT 1 FROM catalog.credit.CustomerLoan _exists_rel JOIN catalog.credit.Loan _exists_target ON _exists_rel.loan_id = _exists_target.id WHERE _exists_rel.customer_id = _gsql2rsql_c_id))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n  HAVING (account_count) &gt;= (2)\n) AS _proj\nORDER BY avg_balance DESC\nLIMIT 50\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n    Filter: (a.balance GT 5000)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: c=c, avg_balance=AVG(a.balance), account_count=COUNT(a)\n    Filter: NOT(EXISTS { c:, [:HAS_LOAN]-&gt;, :Loan })\n    Having: (account_count GEQ 2)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=c.id, name=c.name, avg_balance=avg_balance, account_count=account_count\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#12-analyze-payment-velocity-to-detect-cash-flow-improvements","title":"12. Analyze payment velocity to detect cash flow improvements","text":"<p>Application: Credit: Payment velocity analysis</p> Notes <p>Detects customers increasing loan payment amounts. Indicates improved cash flow and reduced default risk.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_LOAN]-&gt;(l:Loan)-[:PAYMENT]-&gt;(p:Payment)\nWHERE p.timestamp &gt; TIMESTAMP() - DURATION('P180D')\nWITH c, l,\n     AVG(CASE WHEN p.timestamp &gt; TIMESTAMP() - DURATION('P30D') THEN p.amount END) AS recent_avg,\n     AVG(CASE WHEN p.timestamp &lt;= TIMESTAMP() - DURATION('P90D') THEN p.amount END) AS historical_avg\nWHERE historical_avg &gt; 0 AND recent_avg &gt; historical_avg * 1.2\nRETURN c.id, c.name, l.id AS loan_id, historical_avg, recent_avg,\n       ((recent_avg - historical_avg) / historical_avg) AS payment_increase_pct\nORDER BY payment_increase_pct DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,_gsql2rsql_l_id AS loan_id\n  ,historical_avg AS historical_avg\n  ,recent_avg AS recent_avg\n  ,((recent_avg) - (historical_avg)) / (historical_avg) AS payment_increase_pct\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_l_id AS _gsql2rsql_l_id\n    ,AVG(CAST(CASE WHEN (_gsql2rsql_p_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY)) THEN _gsql2rsql_p_amount END AS DOUBLE)) AS recent_avg\n    ,AVG(CAST(CASE WHEN (_gsql2rsql_p_timestamp) &lt;= ((CURRENT_TIMESTAMP()) - (INTERVAL 90 DAY)) THEN _gsql2rsql_p_amount END AS DOUBLE)) AS historical_avg\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    ,_gsql2rsql_l_amount AS _gsql2rsql_l_amount\n    ,_gsql2rsql_l_balance AS _gsql2rsql_l_balance\n    ,_gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n    ,_gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n    ,_gsql2rsql_l_status AS _gsql2rsql_l_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n      ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n      ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n      ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n      ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n      ,_left._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n      ,_left._gsql2rsql__anon2_payment_id AS _gsql2rsql__anon2_payment_id\n      ,_right._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_right._gsql2rsql_p_amount AS _gsql2rsql_p_amount\n      ,_right._gsql2rsql_p_timestamp AS _gsql2rsql_p_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n        ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n        ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n        ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n        ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n        ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n        ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n        ,_right._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n        ,_right._gsql2rsql__anon2_payment_id AS _gsql2rsql__anon2_payment_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n          ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n          ,_right._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n          ,_right._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n          ,_right._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n          ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n          ,_right._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,loan_id AS _gsql2rsql__anon1_loan_id\n            FROM\n              catalog.credit.CustomerLoan\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_l_id\n            ,amount AS _gsql2rsql_l_amount\n            ,balance AS _gsql2rsql_l_balance\n            ,interest_rate AS _gsql2rsql_l_interest_rate\n            ,status AS _gsql2rsql_l_status\n            ,origination_date AS _gsql2rsql_l_origination_date\n          FROM\n            catalog.credit.Loan\n        ) AS _right ON\n          _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           loan_id AS _gsql2rsql__anon2_loan_id\n          ,payment_id AS _gsql2rsql__anon2_payment_id\n        FROM\n          catalog.credit.LoanPayment\n      ) AS _right ON\n        _left._gsql2rsql_l_id = _right._gsql2rsql__anon2_loan_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,amount AS _gsql2rsql_p_amount\n        ,timestamp AS _gsql2rsql_p_timestamp\n      FROM\n        catalog.credit.Payment\n    ) AS _right ON\n      _right._gsql2rsql_p_id = _left._gsql2rsql__anon2_payment_id\n  ) AS _proj\n  WHERE (_gsql2rsql_p_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 180 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_l_id, _gsql2rsql_c_name, _gsql2rsql_c_status, _gsql2rsql_l_amount, _gsql2rsql_l_balance, _gsql2rsql_l_interest_rate, _gsql2rsql_l_origination_date, _gsql2rsql_l_status\n  HAVING ((historical_avg) &gt; (0)) AND ((recent_avg) &gt; ((historical_avg) * (1.2)))\n) AS _proj\nORDER BY payment_increase_pct DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_LOAN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: l:Loan\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:PAYMENT]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: p:Payment\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, l=l, recent_avg=AVG(CASE WHEN (p.timestamp GT (DATETIME() MINUS DURATION('P30D'))) THEN p.amount END), historical_avg=AVG(CASE WHEN (p.timestamp LEQ (DATETIME() MINUS DURATION('P90D'))) THEN p.amount END)\n    Filter: (p.timestamp GT (DATETIME() MINUS DURATION('P180D')))\n    Having: ((historical_avg GT 0) AND (recent_avg GT (historical_avg MULTIPLY 1.2)))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, loan_id=l.id, historical_avg=historical_avg, recent_avg=recent_avg, payment_increase_pct=((recent_avg MINUS historical_avg) DIVIDE historical_avg)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#13-identify-customers-suitable-for-credit-line-decreases","title":"13. Identify customers suitable for credit line decreases","text":"<p>Application: Credit: Risk mitigation</p> Notes <p>Finds credit cards with limits far exceeding usage patterns. Reducing limits can decrease exposure while maintaining customer satisfaction.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_CARD]-&gt;(card:CreditCard)-[:CARD_TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('P180D')\nWITH c, card,\n     MAX(card.credit_limit) AS credit_limit,\n     MAX(t.amount) AS max_transaction,\n     AVG(t.amount) AS avg_transaction\nWHERE max_transaction &lt; credit_limit * 0.3 AND avg_transaction &lt; credit_limit * 0.1\nRETURN c.id, card.id AS card_id, credit_limit, max_transaction, avg_transaction,\n       (credit_limit - max_transaction * 3) AS suggested_new_limit\nORDER BY suggested_new_limit DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_card_id AS card_id\n  ,credit_limit AS credit_limit\n  ,max_transaction AS max_transaction\n  ,avg_transaction AS avg_transaction\n  ,(credit_limit) - ((max_transaction) * (3)) AS suggested_new_limit\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_card_id AS _gsql2rsql_card_id\n    ,MAX(_gsql2rsql_card_credit_limit) AS credit_limit\n    ,MAX(_gsql2rsql_t_amount) AS max_transaction\n    ,AVG(CAST(_gsql2rsql_t_amount AS DOUBLE)) AS avg_transaction\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    ,_gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n    ,_gsql2rsql_card_number AS _gsql2rsql_card_number\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n      ,_left._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n      ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n      ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n        ,_left._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n        ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n        ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_right._gsql2rsql_card_id AS _gsql2rsql_card_id\n          ,_right._gsql2rsql_card_credit_limit AS _gsql2rsql_card_credit_limit\n          ,_right._gsql2rsql_card_number AS _gsql2rsql_card_number\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c_id\n              ,name AS _gsql2rsql_c_name\n              ,status AS _gsql2rsql_c_status\n            FROM\n              catalog.credit.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,card_id AS _gsql2rsql__anon1_card_id\n            FROM\n              catalog.credit.CustomerCard\n          ) AS _right ON\n            _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_card_id\n            ,credit_limit AS _gsql2rsql_card_credit_limit\n            ,number AS _gsql2rsql_card_number\n          FROM\n            catalog.credit.CreditCard\n        ) AS _right ON\n          _right._gsql2rsql_card_id = _left._gsql2rsql__anon1_card_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           card_id AS _gsql2rsql__anon2_card_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.credit.CardTransaction\n      ) AS _right ON\n        _left._gsql2rsql_card_id = _right._gsql2rsql__anon2_card_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.credit.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 180 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_card_id, _gsql2rsql_c_name, _gsql2rsql_c_status, _gsql2rsql_card_credit_limit, _gsql2rsql_card_number\n  HAVING ((max_transaction) &lt; ((credit_limit) * (0.3))) AND ((avg_transaction) &lt; ((credit_limit) * (0.1)))\n) AS _proj\nORDER BY suggested_new_limit DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: card:CreditCard\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:CARD_TRANSACTION]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, card=card, credit_limit=MAX(card.credit_limit), max_transaction=MAX(t.amount), avg_transaction=AVG(t.amount)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P180D')))\n    Having: ((max_transaction LT (credit_limit MULTIPLY 0.3)) AND (avg_transaction LT (credit_limit MULTIPLY 0.1)))\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, card_id=card.id, credit_limit=credit_limit, max_transaction=max_transaction, avg_transaction=avg_transaction, suggested_new_limit=(credit_limit MINUS (max_transaction MULTIPLY 3))\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#14-detect-refinancing-opportunities-via-interest-rate-comparison","title":"14. Detect refinancing opportunities via interest rate comparison","text":"<p>Application: Credit: Refinancing targeting</p> Notes <p>Identifies loans with rates significantly above current market. Proactive refinancing offers can improve retention and customer satisfaction.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:HAS_LOAN]-&gt;(l:Loan)\nWHERE l.status = 'active' AND l.origination_date &lt; TIMESTAMP() - DURATION('P730D')\n  AND l.interest_rate &gt; 7.0\nWITH c, l, l.interest_rate AS current_rate, 5.5 AS market_rate\nWHERE current_rate &gt; market_rate + 1.0\nRETURN c.id, c.name, l.id AS loan_id, l.balance, current_rate, market_rate,\n       (l.balance * (current_rate - market_rate) / 100) AS annual_savings_potential\nORDER BY annual_savings_potential DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,_gsql2rsql_l_id AS loan_id\n  ,_gsql2rsql_l_balance AS balance\n  ,current_rate AS current_rate\n  ,market_rate AS market_rate\n  ,((_gsql2rsql_l_balance) * ((current_rate) - (market_rate))) / (100) AS annual_savings_potential\nFROM (\n  SELECT *\n  FROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_gsql2rsql_l_id AS _gsql2rsql_l_id\n    ,_gsql2rsql_l_interest_rate AS current_rate\n    ,5.5 AS market_rate\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    ,_gsql2rsql_l_amount AS _gsql2rsql_l_amount\n    ,_gsql2rsql_l_balance AS _gsql2rsql_l_balance\n    ,_gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n    ,_gsql2rsql_l_status AS _gsql2rsql_l_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,_right._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n      ,_right._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n      ,_right._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n      ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n      ,_right._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,name AS _gsql2rsql_c_name\n          ,status AS _gsql2rsql_c_status\n        FROM\n          catalog.credit.Customer\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           customer_id AS _gsql2rsql__anon1_customer_id\n          ,loan_id AS _gsql2rsql__anon1_loan_id\n        FROM\n          catalog.credit.CustomerLoan\n      ) AS _right ON\n        _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_l_id\n        ,amount AS _gsql2rsql_l_amount\n        ,balance AS _gsql2rsql_l_balance\n        ,interest_rate AS _gsql2rsql_l_interest_rate\n        ,status AS _gsql2rsql_l_status\n        ,origination_date AS _gsql2rsql_l_origination_date\n      FROM\n        catalog.credit.Loan\n      WHERE (((status) = ('active')) AND ((interest_rate) &gt; (7.0)))\n    ) AS _right ON\n      _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n  ) AS _proj\n  WHERE (_gsql2rsql_l_origination_date) &lt; ((CURRENT_TIMESTAMP()) - (INTERVAL 730 DAY))\n  ) AS _filter\n  WHERE (current_rate) &gt; ((market_rate) + (1.0))\n) AS _proj\nORDER BY annual_savings_potential DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_LOAN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: l:Loan\n    Filter: ((l.status EQ 'active') AND (l.interest_rate GT 7.0))\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: c=c, l=l, current_rate=l.interest_rate, market_rate=5.5\n    Filter: (l.origination_date LT (DATETIME() MINUS DURATION('P730D')))\n    Having: (current_rate GT (market_rate PLUS 1.0))\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=c.id, name=c.name, loan_id=l.id, balance=l.balance, current_rate=current_rate, market_rate=market_rate, annual_savings_potential=((l.balance MULTIPLY (current_rate MINUS market_rate)) DIVIDE 100)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/credit/#15-analyze-co-borrower-relationships-for-joint-credit-assessment","title":"15. Analyze co-borrower relationships for joint credit assessment","text":"<p>Application: Credit: Co-borrower analysis</p> Notes <p>Examines financial strength of co-borrowers for joint loans. Combined liquidity assessment provides more accurate risk picture.</p> OpenCypher Query Cypher<pre><code>MATCH (c1:Customer)-[:CO_BORROWER]-&gt;(l:Loan)&lt;-[:CO_BORROWER]-(c2:Customer)\nWHERE c1.id &lt; c2.id\nMATCH (c1)-[:HAS_ACCOUNT]-&gt;(a1:Account), (c2)-[:HAS_ACCOUNT]-&gt;(a2:Account)\nWITH c1, c2, l,\n     AVG(a1.balance) AS c1_avg_balance,\n     AVG(a2.balance) AS c2_avg_balance\nRETURN c1.id, c2.id, l.id AS loan_id, l.balance,\n       c1_avg_balance, c2_avg_balance,\n       (c1_avg_balance + c2_avg_balance) AS combined_liquidity\nORDER BY combined_liquidity DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c1_id AS id\n  ,_gsql2rsql_c2_id AS id\n  ,_gsql2rsql_l_id AS loan_id\n  ,_gsql2rsql_l_balance AS balance\n  ,c1_avg_balance AS c1_avg_balance\n  ,c2_avg_balance AS c2_avg_balance\n  ,(c1_avg_balance) + (c2_avg_balance) AS combined_liquidity\nFROM (\n  SELECT \n     _gsql2rsql_c1_id AS _gsql2rsql_c1_id\n    ,_gsql2rsql_c2_id AS _gsql2rsql_c2_id\n    ,_gsql2rsql_l_id AS _gsql2rsql_l_id\n    ,AVG(CAST(_gsql2rsql_a1_balance AS DOUBLE)) AS c1_avg_balance\n    ,AVG(CAST(_gsql2rsql_a2_balance AS DOUBLE)) AS c2_avg_balance\n    ,_gsql2rsql_c1_name AS _gsql2rsql_c1_name\n    ,_gsql2rsql_c1_status AS _gsql2rsql_c1_status\n    ,_gsql2rsql_c2_name AS _gsql2rsql_c2_name\n    ,_gsql2rsql_c2_status AS _gsql2rsql_c2_status\n    ,_gsql2rsql_l_amount AS _gsql2rsql_l_amount\n    ,_gsql2rsql_l_balance AS _gsql2rsql_l_balance\n    ,_gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n    ,_gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n    ,_gsql2rsql_l_status AS _gsql2rsql_l_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n      ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n      ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n      ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n      ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n      ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n      ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n      ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n      ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n      ,_left._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n      ,_left._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n      ,_left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n      ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n      ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n      ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_right._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n      ,_right._gsql2rsql_a1_balance AS _gsql2rsql_a1_balance\n      ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_right._gsql2rsql_a2_id AS _gsql2rsql_a2_id\n      ,_right._gsql2rsql_a2_balance AS _gsql2rsql_a2_balance\n    FROM (\n      SELECT *\n      FROM (\n        SELECT\n           _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n          ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n          ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n          ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n          ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n          ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n          ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n          ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n          ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n          ,_left._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n          ,_left._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n          ,_right._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n          ,_right._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n          ,_right._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n        FROM (\n          SELECT\n             _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n            ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n            ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n            ,_left._gsql2rsql_l_id AS _gsql2rsql_l_id\n            ,_left._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n            ,_left._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n            ,_left._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n            ,_left._gsql2rsql_l_status AS _gsql2rsql_l_status\n            ,_left._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n            ,_right._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n            ,_right._gsql2rsql__anon2_loan_id AS _gsql2rsql__anon2_loan_id\n          FROM (\n            SELECT\n               _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n              ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n              ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n              ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_left._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n              ,_right._gsql2rsql_l_id AS _gsql2rsql_l_id\n              ,_right._gsql2rsql_l_amount AS _gsql2rsql_l_amount\n              ,_right._gsql2rsql_l_balance AS _gsql2rsql_l_balance\n              ,_right._gsql2rsql_l_interest_rate AS _gsql2rsql_l_interest_rate\n              ,_right._gsql2rsql_l_status AS _gsql2rsql_l_status\n              ,_right._gsql2rsql_l_origination_date AS _gsql2rsql_l_origination_date\n            FROM (\n              SELECT\n                 _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n                ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n                ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n                ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n                ,_right._gsql2rsql__anon1_loan_id AS _gsql2rsql__anon1_loan_id\n              FROM (\n                SELECT\n                   id AS _gsql2rsql_c1_id\n                  ,name AS _gsql2rsql_c1_name\n                  ,status AS _gsql2rsql_c1_status\n                FROM\n                  catalog.credit.Customer\n              ) AS _left\n              INNER JOIN (\n                SELECT\n                   customer_id AS _gsql2rsql__anon1_customer_id\n                  ,loan_id AS _gsql2rsql__anon1_loan_id\n                FROM\n                  catalog.credit.CoBorrower\n              ) AS _right ON\n                _left._gsql2rsql_c1_id = _right._gsql2rsql__anon1_customer_id\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 id AS _gsql2rsql_l_id\n                ,amount AS _gsql2rsql_l_amount\n                ,balance AS _gsql2rsql_l_balance\n                ,interest_rate AS _gsql2rsql_l_interest_rate\n                ,status AS _gsql2rsql_l_status\n                ,origination_date AS _gsql2rsql_l_origination_date\n              FROM\n                catalog.credit.Loan\n            ) AS _right ON\n              _right._gsql2rsql_l_id = _left._gsql2rsql__anon1_loan_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon2_customer_id\n              ,loan_id AS _gsql2rsql__anon2_loan_id\n            FROM\n              catalog.credit.CoBorrower\n          ) AS _right ON\n            _left._gsql2rsql_l_id = _right._gsql2rsql__anon2_loan_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_c2_id\n            ,name AS _gsql2rsql_c2_name\n            ,status AS _gsql2rsql_c2_status\n          FROM\n            catalog.credit.Customer\n        ) AS _right ON\n          _right._gsql2rsql_c2_id = _left._gsql2rsql__anon2_customer_id\n      ) AS _filter\n      WHERE (_gsql2rsql_c1_id) &lt; (_gsql2rsql_c2_id)\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n        ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n        ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n        ,_left._gsql2rsql_a1_balance AS _gsql2rsql_a1_balance\n        ,_left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n        ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n        ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n        ,_left._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n        ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql_a2_id AS _gsql2rsql_a2_id\n        ,_right._gsql2rsql_a2_balance AS _gsql2rsql_a2_balance\n      FROM (\n        SELECT\n           _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n          ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n          ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n          ,_left._gsql2rsql_a1_balance AS _gsql2rsql_a1_balance\n          ,_left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n          ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n          ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n          ,_right._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n          ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n            ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n            ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n            ,_left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n            ,_left._gsql2rsql_a1_balance AS _gsql2rsql_a1_balance\n            ,_right._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n            ,_right._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n            ,_right._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n          FROM (\n            SELECT\n               _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n              ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n              ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n              ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n              ,_right._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n              ,_right._gsql2rsql_a1_balance AS _gsql2rsql_a1_balance\n            FROM (\n              SELECT\n                 _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n                ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n                ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n                ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n                ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n              FROM (\n                SELECT\n                   id AS _gsql2rsql_c1_id\n                  ,name AS _gsql2rsql_c1_name\n                  ,status AS _gsql2rsql_c1_status\n                FROM\n                  catalog.credit.Customer\n              ) AS _left\n              INNER JOIN (\n                SELECT\n                   customer_id AS _gsql2rsql__anon1_customer_id\n                  ,account_id AS _gsql2rsql__anon1_account_id\n                FROM\n                  catalog.credit.CustomerAccount\n              ) AS _right ON\n                _left._gsql2rsql_c1_id = _right._gsql2rsql__anon1_customer_id\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 id AS _gsql2rsql_a1_id\n                ,balance AS _gsql2rsql_a1_balance\n              FROM\n                catalog.credit.Account\n            ) AS _right ON\n              _right._gsql2rsql_a1_id = _left._gsql2rsql__anon1_account_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               id AS _gsql2rsql_c2_id\n              ,name AS _gsql2rsql_c2_name\n              ,status AS _gsql2rsql_c2_status\n            FROM\n              catalog.credit.Customer\n          ) AS _right ON\n            TRUE\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             customer_id AS _gsql2rsql__anon2_customer_id\n            ,account_id AS _gsql2rsql__anon2_account_id\n          FROM\n            catalog.credit.CustomerAccount\n        ) AS _right ON\n          _left._gsql2rsql_c2_id = _right._gsql2rsql__anon2_customer_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_a2_id\n          ,balance AS _gsql2rsql_a2_balance\n        FROM\n          catalog.credit.Account\n      ) AS _right ON\n        _right._gsql2rsql_a2_id = _left._gsql2rsql__anon2_account_id\n    ) AS _right ON\n      _left._gsql2rsql_c1_id = _right._gsql2rsql_c1_id\n      AND _left._gsql2rsql_c2_id = _right._gsql2rsql_c2_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_c1_id, _gsql2rsql_c2_id, _gsql2rsql_l_id, _gsql2rsql_c1_name, _gsql2rsql_c1_status, _gsql2rsql_c2_name, _gsql2rsql_c2_status, _gsql2rsql_l_amount, _gsql2rsql_l_balance, _gsql2rsql_l_interest_rate, _gsql2rsql_l_origination_date, _gsql2rsql_l_status\n) AS _proj\nORDER BY combined_liquidity DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c1:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:CO_BORROWER]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: l:Loan\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:CO_BORROWER]&lt;-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: c2:Customer\n*\nOpId=11 Op=DataSourceOperator; InOpIds=; OutOpIds=17;\n  DataSourceOperator(id=11)\n    DataSource: c1:Customer\n*\nOpId=12 Op=DataSourceOperator; InOpIds=; OutOpIds=17;\n  DataSourceOperator(id=12)\n    DataSource: [_anon1:HAS_ACCOUNT]-&gt;\n*\nOpId=13 Op=DataSourceOperator; InOpIds=; OutOpIds=18;\n  DataSourceOperator(id=13)\n    DataSource: a1:Account\n*\nOpId=14 Op=DataSourceOperator; InOpIds=; OutOpIds=19;\n  DataSourceOperator(id=14)\n    DataSource: c2:Customer\n*\nOpId=15 Op=DataSourceOperator; InOpIds=; OutOpIds=20;\n  DataSourceOperator(id=15)\n    DataSource: [_anon2:HAS_ACCOUNT]-&gt;\n*\nOpId=16 Op=DataSourceOperator; InOpIds=; OutOpIds=21;\n  DataSourceOperator(id=16)\n    DataSource: a2:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=_anon1 Type=SOURCE\n*\nOpId=17 Op=JoinOperator; InOpIds=11,12; OutOpIds=18;\n  JoinOperator(id=17)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon1 Type=SINK\n*\nOpId=18 Op=JoinOperator; InOpIds=17,13; OutOpIds=19;\n  JoinOperator(id=18)\n    JoinType: INNER\n    Joins: JoinPair: Node=a1 RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=l RelOrNode=_anon2 Type=SINK\n*\nOpId=19 Op=JoinOperator; InOpIds=18,14; OutOpIds=20;\n  JoinOperator(id=19)\n    JoinType: INNER\n    Joins: \n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=10;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=c2 RelOrNode=_anon2 Type=SOURCE\n*\nOpId=20 Op=JoinOperator; InOpIds=19,15; OutOpIds=21;\n  JoinOperator(id=20)\n    JoinType: INNER\n    Joins: JoinPair: Node=c2 RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=SelectionOperator; InOpIds=9; OutOpIds=22;\n  SelectionOperator(id=10)\n    Filter: (c1.id LT c2.id)\n*\nOpId=21 Op=JoinOperator; InOpIds=20,16; OutOpIds=22;\n  JoinOperator(id=21)\n    JoinType: INNER\n    Joins: JoinPair: Node=a2 RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=22 Op=JoinOperator; InOpIds=10,21; OutOpIds=23;\n  JoinOperator(id=22)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=c1 Type=NODE_ID, JoinPair: Node=c2 RelOrNode=c2 Type=NODE_ID\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=23 Op=ProjectionOperator; InOpIds=22; OutOpIds=24;\n  ProjectionOperator(id=23)\n    Projections: c1=c1, c2=c2, l=l, c1_avg_balance=AVG(a1.balance), c2_avg_balance=AVG(a2.balance)\n*\n----------------------------------------------------------------------\nLevel 8:\n----------------------------------------------------------------------\nOpId=24 Op=ProjectionOperator; InOpIds=23; OutOpIds=;\n  ProjectionOperator(id=24)\n    Projections: id=c1.id, id=c2.id, loan_id=l.id, balance=l.balance, c1_avg_balance=c1_avg_balance, c2_avg_balance=c2_avg_balance, combined_liquidity=(c1_avg_balance PLUS c2_avg_balance)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/","title":"Features Queries","text":"<p>This page contains transpiled examples for features queries queries.</p> <p>Each example shows the original OpenCypher query and its corresponding Databricks SQL translation.</p>"},{"location":"examples/features/#1-simple-node-lookup-retrieve-all-nodes-of-a-type","title":"1. Simple node lookup - retrieve all nodes of a type","text":"<p>Application: Features: Basic MATCH</p> Notes <p>The simplest query pattern - retrieves all nodes with a label.</p> <p>WHY USEFUL: Foundation of all graph queries. Start here to explore data.</p> <p>DATABRICKS COMPLEXITY: O(n) - single table scan COST: Very low. Maps to: SELECT name, age FROM Person Optimizations: Partition pruning if table is partitioned.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN p.name, p.age\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_p_age AS age\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,age AS _gsql2rsql_p_age\n  FROM\n    catalog.demo.Person\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: name=p.name, age=p.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#2-property-filter-with-where-clause","title":"2. Property filter with WHERE clause","text":"<p>Application: Features: WHERE filtering</p> Notes <p>Filters nodes by property values using boolean conditions.</p> <p>WHY USEFUL: Essential for narrowing results. Supports =, &lt;&gt;, &lt;, &gt;, &lt;=, &gt;=, AND, OR, NOT.</p> <p>DATABRICKS COMPLEXITY: O(n) without index, O(log n) with Delta index COST: Low. WHERE pushdown to storage layer in Delta Lake. TIP: Create Z-ORDER on frequently filtered columns.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nWHERE p.age &gt; 30 AND p.active = true\nRETURN p.name, p.age\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_p_age AS age\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,age AS _gsql2rsql_p_age\n    ,active AS _gsql2rsql_p_active\n  FROM\n    catalog.demo.Person\n  WHERE (((age) &gt; (30)) AND ((active) = (TRUE)))\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=3;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: ((p.age GT 30) AND (p.active EQ true))\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=3 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=3)\n    Projections: name=p.name, age=p.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#3-property-projection-with-aliases","title":"3. Property projection with aliases","text":"<p>Application: Features: SELECT aliases</p> Notes <p>Projects specific properties with custom column names.</p> <p>WHY USEFUL: Control output schema, rename for clarity, reduce data transfer.</p> <p>DATABRICKS COMPLEXITY: O(n) - projection happens after scan COST: Very low. Column pruning reduces I/O. Note: Only requested columns are read from Delta Lake.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN p.name AS personName, p.age AS personAge, p.salary AS income\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS personName\n  ,_gsql2rsql_p_age AS personAge\n  ,_gsql2rsql_p_salary AS income\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,age AS _gsql2rsql_p_age\n    ,salary AS _gsql2rsql_p_salary\n  FROM\n    catalog.demo.Person\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: personName=p.name, personAge=p.age, income=p.salary\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#4-pagination-with-order-by-skip-and-limit","title":"4. Pagination with ORDER BY, SKIP and LIMIT","text":"<p>Application: Features: Pagination</p> Notes <p>Orders results and returns a specific page of data.</p> <p>WHY USEFUL: Implement pagination in APIs, get top-N results.</p> <p>DATABRICKS COMPLEXITY: O(n log n) for sorting COST: Medium. Full sort before SKIP/LIMIT. WARNING: SKIP without ORDER BY gives non-deterministic results. TIP: For large offsets, consider keyset pagination instead.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN p.name, p.age\nORDER BY p.age DESC\nSKIP 10 LIMIT 5\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_p_age AS age\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,age AS _gsql2rsql_p_age\n  FROM\n    catalog.demo.Person\n) AS _proj\nORDER BY _gsql2rsql_p_age DESC\nLIMIT 5\nOFFSET 10\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: name=p.name, age=p.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#5-count-aggregation-without-grouping","title":"5. COUNT aggregation without grouping","text":"<p>Application: Features: COUNT</p> Notes <p>Counts all nodes matching the pattern.</p> <p>WHY USEFUL: Get cardinality metrics, validate data.</p> <p>DATABRICKS COMPLEXITY: O(n) - single pass COST: Very low. COUNT(*) is highly optimized in Delta Lake. Returns single row. NULL values are counted.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN COUNT(p) AS totalPeople\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   COUNT(_gsql2rsql_p_id) AS totalPeople\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n  FROM\n    catalog.demo.Person\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: totalPeople=COUNT(p)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#6-group-by-with-multiple-aggregations","title":"6. GROUP BY with multiple aggregations","text":"<p>Application: Features: GROUP BY</p> Notes <p>Groups by non-aggregated columns, computes multiple metrics per group.</p> <p>WHY USEFUL: Analytics dashboards, summary reports, KPIs.</p> <p>DATABRICKS COMPLEXITY: O(n) with hash aggregation COST: Medium. Memory for hash table proportional to group count. Cypher implicit GROUP BY: all non-aggregated RETURN columns become keys.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City)\nRETURN c.name AS city,\n       COUNT(p) AS population,\n       AVG(p.age) AS avgAge,\n       MIN(p.salary) AS minSalary,\n       MAX(p.salary) AS maxSalary\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_name AS city\n  ,COUNT(_gsql2rsql_p_id) AS population\n  ,AVG(CAST(_gsql2rsql_p_age AS DOUBLE)) AS avgAge\n  ,MIN(_gsql2rsql_p_salary) AS minSalary\n  ,MAX(_gsql2rsql_p_salary) AS maxSalary\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql_p_salary AS _gsql2rsql_p_salary\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_left._gsql2rsql_p_salary AS _gsql2rsql_p_salary\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,age AS _gsql2rsql_p_age\n        ,salary AS _gsql2rsql_p_salary\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,city_id AS _gsql2rsql__anon1_city_id\n      FROM\n        catalog.demo.LivesIn\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,name AS _gsql2rsql_c_name\n    FROM\n      catalog.demo.City\n  ) AS _right ON\n    _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_city_id\n) AS _proj\nGROUP BY _gsql2rsql_c_name\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: c:City\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: city=c.name, population=COUNT(p), avgAge=AVG(p.age), minSalary=MIN(p.salary), maxSalary=MAX(p.salary)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#7-aggregation-with-order-by-on-aggregated-column","title":"7. Aggregation with ORDER BY on aggregated column","text":"<p>Application: Features: ORDER BY aggregates</p> Notes <p>Orders grouped results by aggregated values.</p> <p>WHY USEFUL: Find top cities, worst performers, outliers.</p> <p>DATABRICKS COMPLEXITY: O(n) aggregate + O(g log g) sort where g = groups COST: Medium. Sort happens after aggregation. TIP: LIMIT reduces sort cost significantly.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City)\nRETURN c.name AS city, COUNT(p) AS population\nORDER BY population DESC\nLIMIT 10\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_name AS city\n  ,COUNT(_gsql2rsql_p_id) AS population\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,city_id AS _gsql2rsql__anon1_city_id\n      FROM\n        catalog.demo.LivesIn\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,name AS _gsql2rsql_c_name\n    FROM\n      catalog.demo.City\n  ) AS _right ON\n    _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_city_id\n) AS _proj\nGROUP BY _gsql2rsql_c_name\nORDER BY population DESC\nLIMIT 10\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: c:City\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: city=c.name, population=COUNT(p)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#8-having-style-filter-using-withwhere","title":"8. HAVING-style filter using WITH...WHERE","text":"<p>Application: Features: HAVING filter</p> Notes <p>Filters aggregated results (SQL HAVING equivalent).</p> <p>WHY USEFUL: Filter groups by computed values. Find \"cities with &gt; 1000 people\".</p> <p>DATABRICKS COMPLEXITY: O(n) aggregate + O(g) filter COST: Low. Filter applied after aggregation, before final output. Pattern: WITH creates intermediate result, WHERE filters it.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City)\nWITH c.name AS city, COUNT(p) AS population\nWHERE population &gt; 1000\nRETURN city, population\nORDER BY population DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   city AS city\n  ,population AS population\nFROM (\n  SELECT \n     _gsql2rsql_c_name AS city\n    ,COUNT(_gsql2rsql_p_id) AS population\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n      ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_p_id\n        FROM\n          catalog.demo.Person\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           person_id AS _gsql2rsql__anon1_person_id\n          ,city_id AS _gsql2rsql__anon1_city_id\n        FROM\n          catalog.demo.LivesIn\n      ) AS _right ON\n        _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_c_id\n        ,name AS _gsql2rsql_c_name\n      FROM\n        catalog.demo.City\n    ) AS _right ON\n      _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_city_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_c_name\n  HAVING (population) &gt; (1000)\n) AS _proj\nORDER BY population DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: c:City\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=7;\n  ProjectionOperator(id=6)\n    Projections: city=c.name, population=COUNT(p)\n    Having: (population GT 1000)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=6; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: city=city, population=population\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#9-collect-aggregation-into-arrays","title":"9. COLLECT aggregation into arrays","text":"<p>Application: Features: COLLECT_LIST</p> Notes <p>Collects values into an array per group.</p> <p>WHY USEFUL: Denormalize data, create nested structures for JSON APIs.</p> <p>DATABRICKS COMPLEXITY: O(n) - single pass COST: Medium-High. Memory for array construction. Maps to COLLECT_LIST() in Databricks SQL. WARNING: Large arrays can cause OOM. Consider LIMIT inside COLLECT.</p> OpenCypher Query Cypher<pre><code>MATCH (c:City)&lt;-[:LIVES_IN]-(p:Person)\nRETURN c.name AS city, COLLECT(p.name) AS residents\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_name AS city\n  ,COLLECT_LIST(_gsql2rsql_p_name) AS residents\nFROM (\n  SELECT\n     _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    ,_right._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_right._gsql2rsql_p_name AS _gsql2rsql_p_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_c_id\n        ,name AS _gsql2rsql_c_name\n      FROM\n        catalog.demo.City\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,city_id AS _gsql2rsql__anon1_city_id\n      FROM\n        catalog.demo.LivesIn\n    ) AS _right ON\n      _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_city_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_p_id\n      ,name AS _gsql2rsql_p_name\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_p_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\nGROUP BY _gsql2rsql_c_name\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: c:City\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]&lt;-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: city=c.name, residents=COLLECT(p.name)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#10-directed-relationship-traversal","title":"10. Directed relationship traversal","text":"<p>Application: Features: Directed edges</p> Notes <p>Matches directed relationships from source to target.</p> <p>WHY USEFUL: Traverse graph edges in specific direction.</p> <p>DATABRICKS COMPLEXITY: O(n * m) worst case, O(n + e) with proper joins COST: Medium. Translates to INNER JOIN. TIP: Ensure foreign keys have indexes/Z-ORDER.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)\nRETURN p.name AS actor, m.title AS movie\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS actor\n  ,_gsql2rsql_m_title AS movie\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n    ,_right._gsql2rsql_m_title AS _gsql2rsql_m_title\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,movie_id AS _gsql2rsql__anon1_movie_id\n      FROM\n        catalog.demo.ActedIn\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_m_id\n      ,title AS _gsql2rsql_m_title\n    FROM\n      catalog.demo.Movie\n  ) AS _right ON\n    _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_movie_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:ACTED_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: m:Movie\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: actor=p.name, movie=m.title\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#11-relationship-with-property-filter","title":"11. Relationship with property filter","text":"<p>Application: Features: Edge properties</p> Notes <p>Filters relationships by their properties.</p> <p>WHY USEFUL: Find \"strong\" relationships, recent connections.</p> <p>DATABRICKS COMPLEXITY: O(e) where e = edges COST: Medium. Filter on edge table reduces join size. Edge properties stored in edge table as columns.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[r:KNOWS]-&gt;(f:Person)\nWHERE r.since &gt; 2020 AND r.strength &gt; 0.8\nRETURN p.name AS person, f.name AS friend, r.since, r.strength\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS person\n  ,_gsql2rsql_f_name AS friend\n  ,_gsql2rsql_r_since AS since\n  ,_gsql2rsql_r_strength AS strength\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_r_person_id AS _gsql2rsql_r_person_id\n    ,_left._gsql2rsql_r_friend_id AS _gsql2rsql_r_friend_id\n    ,_left._gsql2rsql_r_since AS _gsql2rsql_r_since\n    ,_left._gsql2rsql_r_strength AS _gsql2rsql_r_strength\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql_r_person_id AS _gsql2rsql_r_person_id\n      ,_right._gsql2rsql_r_friend_id AS _gsql2rsql_r_friend_id\n      ,_right._gsql2rsql_r_since AS _gsql2rsql_r_since\n      ,_right._gsql2rsql_r_strength AS _gsql2rsql_r_strength\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql_r_person_id\n        ,friend_id AS _gsql2rsql_r_friend_id\n        ,since AS _gsql2rsql_r_since\n        ,strength AS _gsql2rsql_r_strength\n      FROM\n        catalog.demo.Knows\n      WHERE (((since) &gt; (2020)) AND ((strength) &gt; (0.8)))\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql_r_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql_r_friend_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [r:KNOWS]-&gt;\n    Filter: ((r.since GT 2020) AND (r.strength GT 0.8))\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=r Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: person=p.name, friend=f.name, since=r.since, strength=r.strength\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#12-undirected-relationship-both-directions","title":"12. Undirected relationship (both directions)","text":"<p>Application: Features: Undirected edges</p> Notes <p>Matches relationships in both directions.</p> <p>WHY USEFUL: Social networks where direction doesn't matter.</p> <p>DATABRICKS COMPLEXITY: O(2e) - UNION of both directions COST: Higher. Translates to UNION of forward and reverse joins. May produce duplicates - use DISTINCT if needed.</p> <p>OPTIMIZATION: Predicate pushdown moves WHERE p.name = 'Alice' into the Person table subquery BEFORE the join, dramatically reducing rows.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice'\nRETURN DISTINCT f.name AS friend\n</code></pre> Generated SQL SQL<pre><code>SELECT DISTINCT \n   _gsql2rsql_f_name AS friend\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n      WHERE ((name) = ('Alice'))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.name EQ 'Alice')\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: friend=f.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#13-undirected-with-source-filter-pushdown","title":"13. Undirected with source filter pushdown","text":"<p>Application: Features: Predicate Pushdown</p> Notes <p>Compound source-only filter is pushed into the Person subquery.</p> <p>OPTIMIZATION APPLIED:   BEFORE: Full Person scan \u2192 Full KNOWS scan \u2192 Full Person scan \u2192 Filter   AFTER:  Filtered Person (name='Alice' AND age&gt;25) \u2192 KNOWS \u2192 Person</p> <p>WHY IT MATTERS: If Person table has 1M rows but only 1 Alice over 25, we process 1 row instead of 1M in the initial joins.</p> <p>SQL Pattern (optimized):   FROM (SELECT ... FROM Person WHERE name='Alice' AND age&gt;25) AS p   JOIN Knows ON (p.id = source_id OR p.id = target_id)   JOIN Person AS f ON ...</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice' AND p.age &gt; 25\nRETURN f.name AS friend, f.age AS friendAge\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_f_name AS friend\n  ,_gsql2rsql_f_age AS friendAge\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,age AS _gsql2rsql_p_age\n      FROM\n        catalog.demo.Person\n      WHERE (((name) = ('Alice')) AND ((age) &gt; (25)))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: ((p.name EQ 'Alice') AND (p.age GT 25))\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: friend=f.name, friendAge=f.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#14-undirected-with-target-filter-not-pushed","title":"14. Undirected with target filter (not pushed)","text":"<p>Application: Features: Filter Semantics</p> Notes <p>Target node filter cannot be pushed to source - stays after join.</p> <p>WHY NOT PUSHED: The filter references 'f' (target), which is only known after traversing the relationship. The filter must remain after the join to correctly filter matching targets.</p> <p>SQL Pattern:   FROM Person AS p   JOIN Knows ON ...   JOIN Person AS f ON ...   WHERE f.age &gt; 30  -- Applied after all joins</p> <p>COST: Higher than source pushdown - full initial scans required.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE f.age &gt; 30\nRETURN p.name AS person, f.name AS olderFriend\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS person\n  ,_gsql2rsql_f_name AS olderFriend\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n    WHERE ((age) &gt; (30))\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n    Filter: (f.age GT 30)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: person=p.name, olderFriend=f.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#15-undirected-with-mixed-filters-partial-pushdown","title":"15. Undirected with mixed filters (partial pushdown)","text":"<p>Application: Features: Filter Splitting</p> Notes <p>Source filters are pushed, target filter remains after join.</p> <p>FILTER ANALYSIS:   p.name = 'Alice'  \u2192 PUSHED (references only 'p')   p.active = true   \u2192 PUSHED (references only 'p')   f.age &gt; 30        \u2192 NOT PUSHED (references 'f')</p> <p>SQL Pattern (optimized):   FROM (SELECT ... FROM Person WHERE name='Alice' AND active=true) AS p   JOIN Knows ON ...   JOIN Person AS f ON ...   WHERE f.age &gt; 30  -- Target filter stays here</p> <p>BENEFIT: Source node filtering happens early, reducing join size. Target filtering still required but on smaller intermediate result.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice' AND f.age &gt; 30 AND p.active = true\nRETURN f.name AS friend, f.age AS friendAge\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_f_name AS friend\n  ,_gsql2rsql_f_age AS friendAge\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_active AS _gsql2rsql_p_active\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_active AS _gsql2rsql_p_active\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,active AS _gsql2rsql_p_active\n      FROM\n        catalog.demo.Person\n      WHERE (((name) = ('Alice')) AND ((active) = (TRUE)))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n    WHERE ((age) &gt; (30))\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: ((p.name EQ 'Alice') AND (p.active EQ true))\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n    Filter: (f.age GT 30)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: friend=f.name, friendAge=f.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#16-undirected-multi-hop-with-predicate-pushdown","title":"16. Undirected multi-hop with predicate pushdown","text":"<p>Application: Features: Complex Traversal</p> Notes <p>Multi-hop undirected traversal with source filter pushdown.</p> <p>PATTERN: Alice's friends' friends (2-hop undirected)</p> <p>OPTIMIZATION: Filter p.name='Alice' is pushed into first Person scan. Each hop doubles potential paths, so early filtering is critical.</p> <p>SQL Pattern:   FROM (SELECT ... FROM Person WHERE name='Alice') AS p   JOIN Knows k1 ON (p.id = k1.person_id OR p.id = k1.friend_id)   JOIN Person m ON (m.id = k1.person_id OR m.id = k1.friend_id)   JOIN Knows k2 ON (m.id = k2.person_id OR m.id = k2.friend_id)   JOIN Person f ON (f.id = k2.person_id OR f.id = k2.friend_id)</p> <p>DATABRICKS COMPLEXITY: O(k^2) where k = avg degree COST: High, but pushdown prevents O(n * k^2) explosion.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(m:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice'\nRETURN DISTINCT f.name AS friendOfFriend\n</code></pre> Generated SQL SQL<pre><code>SELECT DISTINCT \n   _gsql2rsql_f_name AS friendOfFriend\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_left._gsql2rsql_m_id AS _gsql2rsql_m_id\n    ,_left._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n    ,_left._gsql2rsql__anon2_friend_id AS _gsql2rsql__anon2_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n      ,_left._gsql2rsql_m_id AS _gsql2rsql_m_id\n      ,_right._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n      ,_right._gsql2rsql__anon2_friend_id AS _gsql2rsql__anon2_friend_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n        ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n        ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n          ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n          ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n          ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_p_id\n            ,name AS _gsql2rsql_p_name\n          FROM\n            catalog.demo.Person\n          WHERE ((name) = ('Alice'))\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             person_id AS _gsql2rsql__anon1_person_id\n            ,friend_id AS _gsql2rsql__anon1_friend_id\n            ,since AS _gsql2rsql__anon1_since\n            ,strength AS _gsql2rsql__anon1_strength\n          FROM\n            catalog.demo.Knows\n          UNION ALL\n          SELECT\n             friend_id AS _gsql2rsql__anon1_person_id\n            ,person_id AS _gsql2rsql__anon1_friend_id\n            ,since AS _gsql2rsql__anon1_since\n            ,strength AS _gsql2rsql__anon1_strength\n          FROM\n            catalog.demo.Knows\n        ) AS _right ON\n          _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_m_id\n        FROM\n          catalog.demo.Person\n      ) AS _right ON\n        _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_person_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon2_person_id\n        ,friend_id AS _gsql2rsql__anon2_friend_id\n        ,since AS _gsql2rsql__anon2_since\n        ,strength AS _gsql2rsql__anon2_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon2_person_id\n        ,person_id AS _gsql2rsql__anon2_friend_id\n        ,since AS _gsql2rsql__anon2_since\n        ,strength AS _gsql2rsql__anon2_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_m_id = _right._gsql2rsql__anon2_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon2_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.name EQ 'Alice')\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: m:Person\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:KNOWS]-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon2 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon2 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: friendOfFriend=f.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#17-undirected-relationship-with-aggregation","title":"17. Undirected relationship with aggregation","text":"<p>Application: Features: Aggregation + Pushdown</p> Notes <p>Aggregation over undirected relationships with source filter.</p> <p>USE CASE: \"High earners and their social network metrics\"</p> <p>OPTIMIZATION: p.salary &gt; 100000 pushed to Person scan. Only high earners participate in the aggregation joins.</p> <p>SQL Pattern:   SELECT p.name, COUNT(f.id), AVG(f.age)   FROM (SELECT ... FROM Person WHERE salary &gt; 100000) AS p   JOIN Knows ON ...   JOIN Person AS f ON ...   GROUP BY p.id, p.name</p> <p>COST: Filter before aggregation = fewer GROUP BY operations.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[r:KNOWS]-(f:Person)\nWHERE p.salary &gt; 100000\nRETURN p.name AS highEarner,\n       COUNT(f) AS friendCount,\n       AVG(f.age) AS avgFriendAge\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS highEarner\n  ,COUNT(_gsql2rsql_f_id) AS friendCount\n  ,AVG(CAST(_gsql2rsql_f_age AS DOUBLE)) AS avgFriendAge\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_salary AS _gsql2rsql_p_salary\n    ,_left._gsql2rsql_r_person_id AS _gsql2rsql_r_person_id\n    ,_left._gsql2rsql_r_friend_id AS _gsql2rsql_r_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_salary AS _gsql2rsql_p_salary\n      ,_right._gsql2rsql_r_person_id AS _gsql2rsql_r_person_id\n      ,_right._gsql2rsql_r_friend_id AS _gsql2rsql_r_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,salary AS _gsql2rsql_p_salary\n      FROM\n        catalog.demo.Person\n      WHERE ((salary) &gt; (100000))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql_r_person_id\n        ,friend_id AS _gsql2rsql_r_friend_id\n        ,since AS _gsql2rsql_r_since\n        ,strength AS _gsql2rsql_r_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql_r_person_id\n        ,person_id AS _gsql2rsql_r_friend_id\n        ,since AS _gsql2rsql_r_since\n        ,strength AS _gsql2rsql_r_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql_r_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql_r_person_id\n) AS _proj\nGROUP BY _gsql2rsql_p_name\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.salary GT 100000)\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [r:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=r Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=r Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: highEarner=p.name, friendCount=COUNT(f), avgFriendAge=AVG(f.age)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#18-optional-match-left-join-semantics","title":"18. OPTIONAL MATCH (left join semantics)","text":"<p>Application: Features: OPTIONAL MATCH</p> Notes <p>Returns all people, with movies if they exist (NULL otherwise).</p> <p>WHY USEFUL: Include all entities even without relationships.</p> <p>DATABRICKS COMPLEXITY: O(n + e) - LEFT JOIN COST: Medium. LEFT JOIN preserves all left-side rows. CRITICAL: Uses LEFT JOIN, not INNER JOIN. NULL values appear where no relationship exists.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nOPTIONAL MATCH (p)-[:ACTED_IN]-&gt;(m:Movie)\nRETURN p.name, m.title\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_m_title AS title\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n    ,_right._gsql2rsql_m_title AS _gsql2rsql_m_title\n  FROM (\n    SELECT\n       id AS _gsql2rsql_p_id\n      ,name AS _gsql2rsql_p_name\n    FROM\n      catalog.demo.Person\n  ) AS _left\n  LEFT JOIN (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n      ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n      ,_right._gsql2rsql_m_title AS _gsql2rsql_m_title\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n        ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_p_id\n          ,name AS _gsql2rsql_p_name\n        FROM\n          catalog.demo.Person\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           person_id AS _gsql2rsql__anon1_person_id\n          ,movie_id AS _gsql2rsql__anon1_movie_id\n        FROM\n          catalog.demo.ActedIn\n      ) AS _right ON\n        _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_m_id\n        ,title AS _gsql2rsql_m_title\n      FROM\n        catalog.demo.Movie\n    ) AS _right ON\n      _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_movie_id\n  ) AS _right ON\n    _left._gsql2rsql_p_id = _right._gsql2rsql_p_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=2)\n    DataSource: p:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: [_anon1:ACTED_IN]-&gt;\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=4)\n    DataSource: m:Movie\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=2,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=5,4; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=1,6; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: LEFT\n    Joins: JoinPair: Node=p RelOrNode=p Type=NODE_ID\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: name=p.name, title=m.title\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#19-variable-length-path-traversal-1-to-3-hops","title":"19. Variable-length path traversal (1 to 3 hops)","text":"<p>Application: Features: Recursive paths</p> Notes <p>Finds all people reachable within 1-3 hops.</p> <p>WHY USEFUL: Friend-of-friend queries, network analysis, influence propagation.</p> <p>DATABRICKS COMPLEXITY: O(k^d) where k=avg degree, d=max depth COST: HIGH. Uses WITH RECURSIVE CTE. Includes cycle detection to prevent infinite loops.</p> <p>SQL Pattern: WITH RECURSIVE paths AS (   -- base case   UNION ALL   -- recursive case with depth &lt; max_depth )</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS*1..3]-&gt;(f:Person)\nWHERE p.name = 'Alice'\nRETURN DISTINCT f.name AS reachable\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n    JOIN catalog.demo.Person src ON src.id = e.person_id\n    WHERE (src.name) = ('Alice')\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 3\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT DISTINCT \n   _gsql2rsql_f_name AS reachable\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_f_id\n    ,sink.name AS _gsql2rsql_f_name\n    ,sink.age AS _gsql2rsql_f_age\n    ,sink.nickname AS _gsql2rsql_f_nickname\n    ,sink.salary AS _gsql2rsql_f_salary\n    ,sink.active AS _gsql2rsql_f_active\n    ,source.id AS _gsql2rsql_p_id\n    ,source.name AS _gsql2rsql_p_name\n    ,source.age AS _gsql2rsql_p_age\n    ,source.nickname AS _gsql2rsql_p_nickname\n    ,source.salary AS _gsql2rsql_p_salary\n    ,source.active AS _gsql2rsql_p_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 1 AND p.depth &lt;= 3\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*1..3)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: reachable=f.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#20-variable-length-path-with-zero-length-includes-self","title":"20. Variable-length path with zero-length (includes self)","text":"<p>Application: Features: Zero-length paths</p> Notes <p>Includes the starting node (depth=0) plus 1-2 hop neighbors.</p> <p>WHY USEFUL: Include self in results, optional relationship matching.</p> <p>DATABRICKS COMPLEXITY: O(1 + k + k^2) - identity + 1-hop + 2-hop COST: HIGH. Recursive CTE with special depth=0 base case. Depth 0 = no joins, just the starting node.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS*0..2]-&gt;(f:Person)\nWHERE p.name = 'Alice'\nRETURN DISTINCT f.name AS reachable\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: Zero-length paths (depth = 0)\n    SELECT\n      n.id AS start_node,\n      n.id AS end_node,\n      0 AS depth,\n      ARRAY(n.id) AS path,\n      ARRAY() AS visited\n    FROM catalog.demo.Person n\n    WHERE (n.name) = ('Alice')\n\n    UNION ALL\n\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n    JOIN catalog.demo.Person src ON src.id = e.person_id\n    WHERE (src.name) = ('Alice')\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 2\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT DISTINCT \n   _gsql2rsql_f_name AS reachable\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_f_id\n    ,sink.name AS _gsql2rsql_f_name\n    ,sink.age AS _gsql2rsql_f_age\n    ,sink.nickname AS _gsql2rsql_f_nickname\n    ,sink.salary AS _gsql2rsql_f_salary\n    ,sink.active AS _gsql2rsql_f_active\n    ,source.id AS _gsql2rsql_p_id\n    ,source.name AS _gsql2rsql_p_name\n    ,source.age AS _gsql2rsql_p_age\n    ,source.nickname AS _gsql2rsql_p_nickname\n    ,source.salary AS _gsql2rsql_p_salary\n    ,source.active AS _gsql2rsql_p_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 0 AND p.depth &lt;= 2\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*0..2)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: reachable=f.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#21-case-expression-for-conditional-values","title":"21. CASE expression for conditional values","text":"<p>Application: Features: CASE WHEN</p> Notes <p>Evaluates conditions sequentially, returns first match.</p> <p>WHY USEFUL: Categorize data, compute derived fields, business logic.</p> <p>DATABRICKS COMPLEXITY: O(n) - evaluated per row COST: Very low. Direct translation to SQL CASE. First matching WHEN wins. ELSE is optional (defaults to NULL). Can be used in WHERE, ORDER BY, GROUP BY.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN p.name,\n       CASE\n         WHEN p.age &lt; 18 THEN 'minor'\n         WHEN p.age &lt; 65 THEN 'adult'\n         ELSE 'senior'\n       END AS ageGroup\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,CASE WHEN (_gsql2rsql_p_age) &lt; (18) THEN 'minor' WHEN (_gsql2rsql_p_age) &lt; (65) THEN 'adult' ELSE 'senior' END AS ageGroup\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,age AS _gsql2rsql_p_age\n  FROM\n    catalog.demo.Person\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: name=p.name, ageGroup=CASE WHEN (p.age LT 18) THEN 'minor' WHEN (p.age LT 65) THEN 'adult' ELSE 'senior' END\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#22-coalesce-for-null-safe-default-values","title":"22. COALESCE for null-safe default values","text":"<p>Application: Features: COALESCE</p> Notes <p>Returns first non-NULL value from the argument list.</p> <p>WHY USEFUL: Handle missing data, provide defaults.</p> <p>DATABRICKS COMPLEXITY: O(n) - evaluated per row COST: Very low. Native Databricks function. Left-to-right evaluation with short-circuit. COALESCE(a, b, c) = first non-NULL of a, b, c.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)\nRETURN COALESCE(p.nickname, p.name) AS displayName,\n       COALESCE(p.salary, 0) AS salary\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   COALESCE(_gsql2rsql_p_nickname, _gsql2rsql_p_name) AS displayName\n  ,COALESCE(_gsql2rsql_p_salary, 0) AS salary\nFROM (\n  SELECT\n     id AS _gsql2rsql_p_id\n    ,name AS _gsql2rsql_p_name\n    ,nickname AS _gsql2rsql_p_nickname\n    ,salary AS _gsql2rsql_p_salary\n  FROM\n    catalog.demo.Person\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=ProjectionOperator; InOpIds=1; OutOpIds=;\n  ProjectionOperator(id=2)\n    Projections: displayName=COALESCE(p.nickname, p.name), salary=COALESCE(p.salary, 0)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#23-distinct-for-deduplication","title":"23. DISTINCT for deduplication","text":"<p>Application: Features: DISTINCT</p> Notes <p>Removes duplicate rows from results.</p> <p>WHY USEFUL: Get unique values, eliminate duplicates from traversals.</p> <p>DATABRICKS COMPLEXITY: O(n log n) or O(n) with hash COST: Medium. Requires sorting or hashing. NULL is treated as a distinct value. Compares ALL returned columns for uniqueness.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)\nRETURN DISTINCT m.genre\n</code></pre> Generated SQL SQL<pre><code>SELECT DISTINCT \n   _gsql2rsql_m_genre AS genre\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n    ,_right._gsql2rsql_m_genre AS _gsql2rsql_m_genre\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,movie_id AS _gsql2rsql__anon1_movie_id\n      FROM\n        catalog.demo.ActedIn\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_m_id\n      ,genre AS _gsql2rsql_m_genre\n    FROM\n      catalog.demo.Movie\n  ) AS _right ON\n    _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_movie_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:ACTED_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: m:Movie\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: genre=m.genre\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#24-union-to-combine-query-results","title":"24. UNION to combine query results","text":"<p>Application: Features: UNION</p> Notes <p>Combines results from two queries, removes duplicates.</p> <p>WHY USEFUL: Merge different query paths, find \"actors OR directors\".</p> <p>DATABRICKS COMPLEXITY: O(n + m + (n+m) log(n+m)) for dedup COST: High. UNION requires deduplication. Both queries must have same column count and compatible types. Use UNION ALL if duplicates are OK (faster).</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)\nRETURN p.name AS name\nUNION\nMATCH (d:Person)-[:DIRECTED]-&gt;(m:Movie)\nRETURN d.name AS name\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,movie_id AS _gsql2rsql__anon1_movie_id\n      FROM\n        catalog.demo.ActedIn\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_m_id\n    FROM\n      catalog.demo.Movie\n  ) AS _right ON\n    _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_movie_id\n) AS _proj\nUNION\nSELECT \n   _gsql2rsql_d_name AS name\nFROM (\n  SELECT\n     _left._gsql2rsql_d_id AS _gsql2rsql_d_id\n    ,_left._gsql2rsql_d_name AS _gsql2rsql_d_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n  FROM (\n    SELECT\n       _left._gsql2rsql_d_id AS _gsql2rsql_d_id\n      ,_left._gsql2rsql_d_name AS _gsql2rsql_d_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_movie_id AS _gsql2rsql__anon1_movie_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_d_id\n        ,name AS _gsql2rsql_d_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,movie_id AS _gsql2rsql__anon1_movie_id\n      FROM\n        catalog.demo.Directed\n    ) AS _right ON\n      _left._gsql2rsql_d_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_m_id\n    FROM\n      catalog.demo.Movie\n  ) AS _right ON\n    _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_movie_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:ACTED_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: m:Movie\n*\nOpId=7 Op=DataSourceOperator; InOpIds=; OutOpIds=10;\n  DataSourceOperator(id=7)\n    DataSource: d:Person\n*\nOpId=8 Op=DataSourceOperator; InOpIds=; OutOpIds=10;\n  DataSourceOperator(id=8)\n    DataSource: [_anon1:DIRECTED]-&gt;\n*\nOpId=9 Op=DataSourceOperator; InOpIds=; OutOpIds=11;\n  DataSourceOperator(id=9)\n    DataSource: m:Movie\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\nOpId=10 Op=JoinOperator; InOpIds=7,8; OutOpIds=11;\n  JoinOperator(id=10)\n    JoinType: INNER\n    Joins: JoinPair: Node=d RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\nOpId=11 Op=JoinOperator; InOpIds=10,9; OutOpIds=12;\n  JoinOperator(id=11)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=13;\n  ProjectionOperator(id=6)\n    Projections: name=p.name\n*\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=13;\n  ProjectionOperator(id=12)\n    Projections: name=d.name\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=13 Op=SetOperator; InOpIds=6,12; OutOpIds=;\n  SetOperator(id=13)\n    SetOp: UNION\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#25-multi-hop-path-with-intermediate-filtering","title":"25. Multi-hop path with intermediate filtering","text":"<p>Application: Features: Chained patterns</p> Notes <p>Matches multiple relationship patterns from the same node.</p> <p>WHY USEFUL: Complex entity queries with multiple constraints.</p> <p>DATABRICKS COMPLEXITY: O(n * j1 * j2) worst case COST: Medium-High. Multiple JOINs. Comma-separated patterns share the same variable scope. Filter pushdown optimizes join order.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City),\n      (p)-[:WORKS_AT]-&gt;(co:Company)\nWHERE c.country = 'USA' AND co.industry = 'Tech'\nRETURN p.name, c.name AS city, co.name AS company\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_c_name AS city\n  ,_gsql2rsql_co_name AS company\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n    ,_left._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_left._gsql2rsql_c_country AS _gsql2rsql_c_country\n    ,_left._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n    ,_left._gsql2rsql__anon2_company_id AS _gsql2rsql__anon2_company_id\n    ,_right._gsql2rsql_co_id AS _gsql2rsql_co_id\n    ,_right._gsql2rsql_co_name AS _gsql2rsql_co_name\n    ,_right._gsql2rsql_co_industry AS _gsql2rsql_co_industry\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n      ,_left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_left._gsql2rsql_c_country AS _gsql2rsql_c_country\n      ,_right._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n      ,_right._gsql2rsql__anon2_company_id AS _gsql2rsql__anon2_company_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n        ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n        ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_right._gsql2rsql_c_country AS _gsql2rsql_c_country\n      FROM (\n        SELECT\n           _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n          ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n          ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n          ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_p_id\n            ,name AS _gsql2rsql_p_name\n          FROM\n            catalog.demo.Person\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             person_id AS _gsql2rsql__anon1_person_id\n            ,city_id AS _gsql2rsql__anon1_city_id\n          FROM\n            catalog.demo.LivesIn\n        ) AS _right ON\n          _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,name AS _gsql2rsql_c_name\n          ,country AS _gsql2rsql_c_country\n        FROM\n          catalog.demo.City\n        WHERE ((country) = ('USA'))\n      ) AS _right ON\n        _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_city_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon2_person_id\n        ,company_id AS _gsql2rsql__anon2_company_id\n      FROM\n        catalog.demo.WorksAt\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon2_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_co_id\n      ,name AS _gsql2rsql_co_name\n      ,industry AS _gsql2rsql_co_industry\n    FROM\n      catalog.demo.Company\n    WHERE ((industry) = ('Tech'))\n  ) AS _right ON\n    _right._gsql2rsql_co_id = _left._gsql2rsql__anon2_company_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: c:City\n    Filter: (c.country EQ 'USA')\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:WORKS_AT]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: co:Company\n    Filter: (co.industry EQ 'Tech')\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=co RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: name=p.name, city=c.name, company=co.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#26-chained-with-for-multi-stage-computation","title":"26. Chained WITH for multi-stage computation","text":"<p>Application: Features: WITH chaining</p> Notes <p>Chains multiple WITH clauses for staged computation.</p> <p>WHY USEFUL: Break complex queries into steps, compute derived values.</p> <p>DATABRICKS COMPLEXITY: O(n) per stage COST: Medium. Each WITH creates a logical stage. Variables from previous WITH are available in next stage. Useful for aggregation \u2192 filtering \u2192 transformation pipelines.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City)\nWITH c, COUNT(p) AS pop\nWHERE pop &gt; 100\nWITH c.name AS city, pop, pop * 1.0 / 1000 AS popK\nRETURN city, popK\nORDER BY popK DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   city AS city\n  ,popK AS popK\nFROM (\n  SELECT \n     _gsql2rsql_c_name AS city\n    ,pop AS pop\n    ,((pop) * (1.0)) / (1000) AS popK\n  FROM (\n    SELECT \n       _gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,COUNT(_gsql2rsql_p_id) AS pop\n      ,_gsql2rsql_c_country AS _gsql2rsql_c_country\n      ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_gsql2rsql_c_population AS _gsql2rsql_c_population\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_left._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n        ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_right._gsql2rsql_c_population AS _gsql2rsql_c_population\n        ,_right._gsql2rsql_c_country AS _gsql2rsql_c_country\n      FROM (\n        SELECT\n           _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n          ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n          ,_right._gsql2rsql__anon1_city_id AS _gsql2rsql__anon1_city_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_p_id\n          FROM\n            catalog.demo.Person\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             person_id AS _gsql2rsql__anon1_person_id\n            ,city_id AS _gsql2rsql__anon1_city_id\n          FROM\n            catalog.demo.LivesIn\n        ) AS _right ON\n          _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,name AS _gsql2rsql_c_name\n          ,population AS _gsql2rsql_c_population\n          ,country AS _gsql2rsql_c_country\n        FROM\n          catalog.demo.City\n      ) AS _right ON\n        _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_city_id\n    ) AS _proj\n    GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_country, _gsql2rsql_c_name, _gsql2rsql_c_population\n    HAVING (pop) &gt; (100)\n  ) AS _proj\n) AS _proj\nORDER BY popK DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:LIVES_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: c:City\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=7;\n  ProjectionOperator(id=6)\n    Projections: c=c, pop=COUNT(p)\n    Having: (pop GT 100)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=6; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: city=c.name, pop=pop, popK=((pop MULTIPLY 1.0) DIVIDE 1000)\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: city=city, popK=popK\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#27-simplest-sink-filter-pushdown","title":"27. Simplest sink filter pushdown","text":"<p>Application: Features: Sink Filter Pushdown</p> Notes <p>Minimal example: filter on sink node b is pushed into recursive join.</p> <p>SQL: WHERE p.depth &gt;= 1 AND p.depth &lt;= 2 AND (sink.age) &gt; (30)</p> OpenCypher Query Cypher<pre><code>MATCH (a:Person)-[:KNOWS*1..2]-&gt;(b:Person)\nWHERE b.age &gt; 30\nRETURN a.name, b.name\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 2\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT \n   _gsql2rsql_a_name AS name\n  ,_gsql2rsql_b_name AS name\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.name AS _gsql2rsql_b_name\n    ,sink.age AS _gsql2rsql_b_age\n    ,sink.nickname AS _gsql2rsql_b_nickname\n    ,sink.salary AS _gsql2rsql_b_salary\n    ,sink.active AS _gsql2rsql_b_active\n    ,source.id AS _gsql2rsql_a_id\n    ,source.name AS _gsql2rsql_a_name\n    ,source.age AS _gsql2rsql_a_age\n    ,source.nickname AS _gsql2rsql_a_nickname\n    ,source.salary AS _gsql2rsql_a_salary\n    ,source.active AS _gsql2rsql_a_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 1 AND p.depth &lt;= 2 AND (sink.age) &gt; (30)\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*1..2)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: name=a.name, name=b.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#28-variable-length-path-with-sink-filter-pushdown","title":"28. Variable-length path with sink filter pushdown","text":"<p>Application: Features: Recursive Sink Filter Pushdown</p> Notes <p>Filter on sink node (b.age &gt; 50) is pushed into the recursive join.</p> <p>OPTIMIZATION APPLIED:   BEFORE: CTE \u2192 JOIN sink \u2192 JOIN source \u2192 depth filter \u2192 OUTER sink filter   AFTER:  CTE \u2192 JOIN sink \u2192 JOIN source \u2192 WHERE depth AND sink.age &gt; 50</p> <p>WHY IT MATTERS: Instead of filtering 1000 paths after all joins complete, we filter during the join and only keep paths ending at older people.</p> <p>SQL Pattern (optimized):   FROM paths_1 p   JOIN Person sink ON sink.id = p.end_node   JOIN Person source ON source.id = p.start_node   WHERE p.depth &gt;= 2 AND p.depth &lt;= 4 AND (sink.age) &gt; (50)</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Person)-[:KNOWS*2..4]-&gt;(b:Person)\nWHERE b.age &gt; 50\nRETURN a.id, b.id, LENGTH(path) AS chain_length\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 4\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\n  ,(SIZE(_gsql2rsql_path_id) - 1) AS chain_length\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.name AS _gsql2rsql_b_name\n    ,sink.age AS _gsql2rsql_b_age\n    ,sink.nickname AS _gsql2rsql_b_nickname\n    ,sink.salary AS _gsql2rsql_b_salary\n    ,sink.active AS _gsql2rsql_b_active\n    ,source.id AS _gsql2rsql_a_id\n    ,source.name AS _gsql2rsql_a_name\n    ,source.age AS _gsql2rsql_a_age\n    ,source.nickname AS _gsql2rsql_a_nickname\n    ,source.salary AS _gsql2rsql_a_salary\n    ,source.active AS _gsql2rsql_a_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path AS _gsql2rsql_path_id\n    ,p.path_edges AS _gsql2rsql_path_edges\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 2 AND p.depth &lt;= 4 AND (sink.age) &gt; (50)\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*2..4, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: id=a.id, id=b.id, chain_length=LENGTH(path)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#29-variable-length-with-source-and-sink-filter-pushdown","title":"29. Variable-length with source AND sink filter pushdown","text":"<p>Application: Features: Dual Filter Pushdown</p> Notes <p>Both source and sink filters are optimized:   - Source filter (a.age &gt; 30) \u2192 pushed into CTE base case   - Sink filter (b.age &gt; 50) \u2192 pushed into recursive join</p> <p>BENEFIT: Maximum optimization for path queries between filtered nodes. We only explore paths starting from people over 30 (source filter) and only keep paths ending at people over 50 (sink filter).</p> <p>SQL Pattern:   Base case: ... JOIN Person src ON ... WHERE (src.age) &gt; (30)   Join: ... WHERE depth_bounds AND (sink.age) &gt; (50)</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Person)-[:KNOWS*2..4]-&gt;(b:Person)\nWHERE a.age &gt; 30 AND b.age &gt; 50\nRETURN a.id, b.id\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n    JOIN catalog.demo.Person src ON src.id = e.person_id\n    WHERE (src.age) &gt; (30)\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 4\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.name AS _gsql2rsql_b_name\n    ,sink.age AS _gsql2rsql_b_age\n    ,sink.nickname AS _gsql2rsql_b_nickname\n    ,sink.salary AS _gsql2rsql_b_salary\n    ,sink.active AS _gsql2rsql_b_active\n    ,source.id AS _gsql2rsql_a_id\n    ,source.name AS _gsql2rsql_a_name\n    ,source.age AS _gsql2rsql_a_age\n    ,source.nickname AS _gsql2rsql_a_nickname\n    ,source.salary AS _gsql2rsql_a_salary\n    ,source.active AS _gsql2rsql_a_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path AS _gsql2rsql_path_id\n    ,p.path_edges AS _gsql2rsql_path_edges\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 2 AND p.depth &lt;= 4 AND (sink.age) &gt; (50)\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*2..4, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: id=a.id, id=b.id\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#30-variable-length-with-compound-sink-filter","title":"30. Variable-length with compound sink filter","text":"<p>Application: Features: Compound Sink Filter Pushdown</p> Notes <p>Compound sink filter (AND of two conditions) is pushed together.</p> <p>OPTIMIZATION: Both conditions are applied in the recursive join WHERE:   WHERE p.depth &gt;= 1 AND ((sink.age) &gt; (40) AND sink.active = true)</p> <p>USE CASE: Find chains of connections ending at active people over 40.</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Person)-[:KNOWS*1..3]-&gt;(b:Person)\nWHERE b.age &gt; 40 AND b.active = true\nRETURN a.id, b.id, [n IN nodes(path) | n.id] AS path_nodes\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 3\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\n  ,_gsql2rsql_path_id AS path_nodes\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.name AS _gsql2rsql_b_name\n    ,sink.age AS _gsql2rsql_b_age\n    ,sink.nickname AS _gsql2rsql_b_nickname\n    ,sink.salary AS _gsql2rsql_b_salary\n    ,sink.active AS _gsql2rsql_b_active\n    ,source.id AS _gsql2rsql_a_id\n    ,source.name AS _gsql2rsql_a_name\n    ,source.age AS _gsql2rsql_a_age\n    ,source.nickname AS _gsql2rsql_a_nickname\n    ,source.salary AS _gsql2rsql_a_salary\n    ,source.active AS _gsql2rsql_a_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path AS _gsql2rsql_path_id\n    ,p.path_edges AS _gsql2rsql_path_edges\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 1 AND p.depth &lt;= 3 AND ((sink.age) &gt; (40)) AND ((sink.active) = (TRUE))\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*1..3, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: id=a.id, id=b.id, path_nodes=[n IN NODES(path) | n.id]\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#31-variable-length-with-sink-filter-and-edge-predicate","title":"31. Variable-length with sink filter and edge predicate","text":"<p>Application: Features: Combined Optimizations</p> Notes <p>Combines multiple optimizations:   1. Edge predicate (since &gt; 2010) pushed into CTE base and recursive cases   2. Sink filter (age &gt; 60) pushed into recursive join</p> <p>USE CASE: Find chains of recent connections ending at seniors. Only explores paths where EVERY connection was made after 2010.</p> <p>SQL Pattern:   Base case: WHERE (e.since) &gt; (2010)   Recursive: WHERE depth &lt; 5 AND (e.since) &gt; (2010)   Join: WHERE depth_bounds AND (sink.age) &gt; (60)</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Person)-[:KNOWS*2..5]-&gt;(b:Person)\nWHERE b.age &gt; 60\n  AND ALL(k IN relationships(path) WHERE k.since &gt; 2010)\nRETURN a.id, b.id, LENGTH(path) AS hops\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n    WHERE (e.since) &gt; (2010)\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('person_id', e.person_id, 'friend_id', e.friend_id, 'since', e.since, 'strength', e.strength)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 5\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n      AND (e.since) &gt; (2010)\n  )\nSELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\n  ,(SIZE(_gsql2rsql_path_id) - 1) AS hops\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.name AS _gsql2rsql_b_name\n    ,sink.age AS _gsql2rsql_b_age\n    ,sink.nickname AS _gsql2rsql_b_nickname\n    ,sink.salary AS _gsql2rsql_b_salary\n    ,sink.active AS _gsql2rsql_b_active\n    ,source.id AS _gsql2rsql_a_id\n    ,source.name AS _gsql2rsql_a_name\n    ,source.age AS _gsql2rsql_a_age\n    ,source.nickname AS _gsql2rsql_a_nickname\n    ,source.salary AS _gsql2rsql_a_salary\n    ,source.active AS _gsql2rsql_a_active\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path AS _gsql2rsql_path_id\n    ,p.path_edges AS _gsql2rsql_path_edges\n  FROM paths_1 p\n  JOIN catalog.demo.Person sink\n    ON sink.id = p.end_node\n  JOIN catalog.demo.Person source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 2 AND p.depth &lt;= 5 AND (sink.age) &gt; (60)\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*2..5, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: id=a.id, id=b.id, hops=LENGTH(path)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#32-undirected-both-filters-pushdown-split-and-to-both-sources","title":"32. Undirected both filters pushdown - split AND to both sources","text":"<p>Application: Features: Conjunction Splitting Pushdown</p> Notes <p>BEFORE optimization (suboptimal):   Selection(p.name='Alice' AND f.age&gt;30) sits AFTER the join   \u2192 Joins ALL Person rows, then filters.</p> <p>AFTER optimization (with conjunction splitting):   - p.name = 'Alice' \u2192 pushed to DataSource(p)   - f.age &gt; 30 \u2192 pushed to DataSource(f)   \u2192 Both filters applied BEFORE the join!</p> <p>SQL Pattern (optimized):   FROM (SELECT ... FROM Person WHERE name = 'Alice') AS _left   JOIN ...   JOIN (SELECT ... FROM Person WHERE age &gt; 30) AS _right</p> <p>PERFORMANCE: Dramatically reduces join cardinality.</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice' AND f.age &gt; 30\nRETURN p.name, f.name, f.age\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_f_name AS name\n  ,_gsql2rsql_f_age AS age\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n      WHERE ((name) = ('Alice'))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n    WHERE ((age) &gt; (30))\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.name EQ 'Alice')\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n    Filter: (f.age GT 30)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: name=p.name, name=f.name, age=f.age\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#33-undirected-partial-pushdown-one-pushed-one-cross-variable","title":"33. Undirected partial pushdown - one pushed, one cross-variable","text":"<p>Application: Features: Partial Conjunction Pushdown</p> Notes <p>The optimizer splits the AND conjunction:   - p.age &gt; 25 \u2192 PUSHED to DataSource(p) (single-variable)   - p.name = f.name \u2192 KEPT in Selection (cross-variable, cannot push!)</p> <p>BENEFIT: Even partial pushdown reduces join input size. The cross-variable predicate (p.name = f.name) must be evaluated after the join because it compares values from both sides.</p> <p>SQL Pattern:   FROM (SELECT ... FROM Person WHERE age &gt; 25) AS _left  -- PUSHED   JOIN ...   WHERE p.name = f.name  -- KEPT (cross-variable)</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.age &gt; 25 AND p.name = f.name\nRETURN p.name, f.name\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_f_name AS name\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,age AS _gsql2rsql_p_age\n      FROM\n        catalog.demo.Person\n      WHERE ((age) &gt; (25))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\nWHERE (_gsql2rsql_p_name) = (_gsql2rsql_f_name)\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.age GT 25)\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: name=p.name, name=f.name\n    Filter: (p.name EQ f.name)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#34-undirected-multiple-same-variable-predicates-combined","title":"34. Undirected multiple same-variable predicates combined","text":"<p>Application: Features: Predicate Combination</p> Notes <p>Multiple predicates for the same variable are combined with AND:   - p: (name='Bob' AND age&gt;18 AND active=true) \u2192 pushed to DataSource(p)   - f: (salary&gt;50000) \u2192 pushed to DataSource(f)</p> <p>OPTIMIZATION: All predicates pushed, SelectionOperator removed entirely!</p> <p>SQL Pattern:   FROM (SELECT ... FROM Person WHERE name='Bob' AND age&gt;18 AND active=true) AS _left   JOIN ...   JOIN (SELECT ... FROM Person WHERE salary &gt; 50000) AS _right</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Bob' AND p.age &gt; 18 AND p.active = true AND f.salary &gt; 50000\nRETURN p.id, f.id\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_id AS id\n  ,_gsql2rsql_f_id AS id\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql_p_active AS _gsql2rsql_p_active\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_salary AS _gsql2rsql_f_salary\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_left._gsql2rsql_p_active AS _gsql2rsql_p_active\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n        ,age AS _gsql2rsql_p_age\n        ,active AS _gsql2rsql_p_active\n      FROM\n        catalog.demo.Person\n      WHERE ((((name) = ('Bob')) AND ((age) &gt; (18))) AND ((active) = (TRUE)))\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,salary AS _gsql2rsql_f_salary\n    FROM\n      catalog.demo.Person\n    WHERE ((salary) &gt; (50000))\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (((p.name EQ 'Bob') AND (p.age GT 18)) AND (p.active EQ true))\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n    Filter: (f.salary GT 50000)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: id=p.id, id=f.id\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#35-undirected-with-or-predicate-cannot-split","title":"35. Undirected with OR predicate - cannot split","text":"<p>Application: Features: OR Predicate Handling</p> Notes <p>OR predicates CANNOT be split! This is algebraically unsafe:   \u03c3_{p(A) \u2228 q(B)}(A \u22c8 B) \u2262 \u03c3_{p(A)}(A) \u22c8 \u03c3_{q(B)}(B)</p> <p>If we pushed, we'd miss rows where:   - p.name != 'Alice' but f.age &gt; 30</p> <p>RESULT: Entire predicate stays in Selection (no pushdown).</p> <p>TODO: Future optimization could rewrite as UNION:   (MATCH ... WHERE p.name='Alice')   UNION   (MATCH ... WHERE f.age &gt; 30)</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)\nWHERE p.name = 'Alice' OR f.age &gt; 30\nRETURN p.name, f.name\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_f_name AS name\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_right._gsql2rsql_f_age AS _gsql2rsql_f_age\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    FROM (\n      SELECT\n         id AS _gsql2rsql_p_id\n        ,name AS _gsql2rsql_p_name\n      FROM\n        catalog.demo.Person\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,friend_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n      UNION ALL\n      SELECT\n         friend_id AS _gsql2rsql__anon1_person_id\n        ,person_id AS _gsql2rsql__anon1_friend_id\n        ,since AS _gsql2rsql__anon1_since\n        ,strength AS _gsql2rsql__anon1_strength\n      FROM\n        catalog.demo.Knows\n    ) AS _right ON\n      _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_f_id\n      ,name AS _gsql2rsql_f_name\n      ,age AS _gsql2rsql_f_age\n    FROM\n      catalog.demo.Person\n  ) AS _right ON\n    _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n) AS _proj\nWHERE ((_gsql2rsql_p_name) = ('Alice')) OR ((_gsql2rsql_f_age) &gt; (30))\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: name=p.name, name=f.name\n    Filter: ((p.name EQ 'Alice') OR (f.age GT 30))\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#36-undirected-three-way-mixed-filters","title":"36. Undirected three-way mixed filters","text":"<p>Application: Features: Complex Conjunction Splitting</p> Notes <p>Three-way join with filters on all three entities:   - p.age &gt; 25 \u2192 pushed to DataSource(p)   - f.salary &gt; 50000 \u2192 pushed to DataSource(f)   - c.industry = 'Tech' \u2192 pushed to DataSource\u00a9</p> <p>BENEFIT: All three table scans are filtered before any joins! The SelectionOperator is completely removed.</p> <p>SQL Pattern:   FROM (SELECT ... FROM Person WHERE age &gt; 25) AS p   JOIN ...   JOIN (SELECT ... FROM Person WHERE salary &gt; 50000) AS f   JOIN ...   JOIN (SELECT ... FROM Company WHERE industry = 'Tech') AS c</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS]-(f:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE p.age &gt; 25 AND f.salary &gt; 50000 AND c.industry = 'Tech'\nRETURN p.name, f.name, c.name\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_name AS name\n  ,_gsql2rsql_f_name AS name\n  ,_gsql2rsql_c_name AS name\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n    ,_left._gsql2rsql_f_id AS _gsql2rsql_f_id\n    ,_left._gsql2rsql_f_name AS _gsql2rsql_f_name\n    ,_left._gsql2rsql_f_salary AS _gsql2rsql_f_salary\n    ,_left._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n    ,_left._gsql2rsql__anon2_company_id AS _gsql2rsql__anon2_company_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_right._gsql2rsql_c_industry AS _gsql2rsql_c_industry\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n      ,_left._gsql2rsql_f_id AS _gsql2rsql_f_id\n      ,_left._gsql2rsql_f_name AS _gsql2rsql_f_name\n      ,_left._gsql2rsql_f_salary AS _gsql2rsql_f_salary\n      ,_right._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n      ,_right._gsql2rsql__anon2_company_id AS _gsql2rsql__anon2_company_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n        ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n        ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_left._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n        ,_right._gsql2rsql_f_id AS _gsql2rsql_f_id\n        ,_right._gsql2rsql_f_name AS _gsql2rsql_f_name\n        ,_right._gsql2rsql_f_salary AS _gsql2rsql_f_salary\n      FROM (\n        SELECT\n           _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n          ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n          ,_left._gsql2rsql_p_age AS _gsql2rsql_p_age\n          ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n          ,_right._gsql2rsql__anon1_friend_id AS _gsql2rsql__anon1_friend_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_p_id\n            ,name AS _gsql2rsql_p_name\n            ,age AS _gsql2rsql_p_age\n          FROM\n            catalog.demo.Person\n          WHERE ((age) &gt; (25))\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             person_id AS _gsql2rsql__anon1_person_id\n            ,friend_id AS _gsql2rsql__anon1_friend_id\n            ,since AS _gsql2rsql__anon1_since\n            ,strength AS _gsql2rsql__anon1_strength\n          FROM\n            catalog.demo.Knows\n          UNION ALL\n          SELECT\n             friend_id AS _gsql2rsql__anon1_person_id\n            ,person_id AS _gsql2rsql__anon1_friend_id\n            ,since AS _gsql2rsql__anon1_since\n            ,strength AS _gsql2rsql__anon1_strength\n          FROM\n            catalog.demo.Knows\n        ) AS _right ON\n          _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_person_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_f_id\n          ,name AS _gsql2rsql_f_name\n          ,salary AS _gsql2rsql_f_salary\n        FROM\n          catalog.demo.Person\n        WHERE ((salary) &gt; (50000))\n      ) AS _right ON\n        _right._gsql2rsql_f_id = _left._gsql2rsql__anon1_person_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon2_person_id\n        ,company_id AS _gsql2rsql__anon2_company_id\n      FROM\n        catalog.demo.WorksAt\n    ) AS _right ON\n      _left._gsql2rsql_f_id = _right._gsql2rsql__anon2_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,name AS _gsql2rsql_c_name\n      ,industry AS _gsql2rsql_c_industry\n    FROM\n      catalog.demo.Company\n    WHERE ((industry) = ('Tech'))\n  ) AS _right ON\n    _right._gsql2rsql_c_id = _left._gsql2rsql__anon2_company_id\n) AS _proj\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n    Filter: (p.age GT 25)\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:KNOWS]-\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: f:Person\n    Filter: (f.salary GT 50000)\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:WORKS_AT]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: c:Company\n    Filter: (c.industry EQ 'Tech')\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon1 Type=EITHER\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=f RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: name=p.name, name=f.name, name=c.name\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/features/#37-variable-length-paths-with-multi-hop-traversal-and-aggregation","title":"37. Variable-length paths with multi-hop traversal and aggregation","text":"<p>Application: Features: Complex graph traversal with aggregation</p> Notes <p>This query demonstrates a powerful combination of features:   1. Variable-length path: KNOWS*1..3 (1 to 3 hops)   2. Pattern continuation after varlen path   3. Filter on final relationship target   4. Aggregation with COUNT(DISTINCT)   5. Ordering by aggregated column</p> <p>REAL-WORLD USE: Find people with the most connections to tech workers. Used in professional networking, talent acquisition, and social graph analysis.</p> <p>OPTIMIZATION: Filter on c.industry pushed down before joins.</p> <p>COMPLEXITY: O(n^3) for 3-hop traversal, but filtered early. Result deduplication via DISTINCT crucial for accurate counts.</p> <p>SQL Pattern:   WITH RECURSIVE path AS (...)  -- Variable-length expansion   SELECT p.name, COUNT(DISTINCT friend_id) AS tech_connections   FROM path   JOIN WorksAt ON ...   JOIN (SELECT ... FROM Company WHERE industry = 'Technology') AS c   GROUP BY p.name   ORDER BY tech_connections DESC   LIMIT 10</p> OpenCypher Query Cypher<pre><code>MATCH (p:Person)-[:KNOWS*1..3]-(friend:Person)-[:WORKS_AT]-&gt;(c:Company)\nWHERE c.industry = 'Technology'\nRETURN p.name, COUNT(DISTINCT friend) AS tech_connections\nORDER BY tech_connections DESC\nLIMIT 10\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.person_id AS start_node,\n      e.friend_id AS end_node,\n      1 AS depth,\n      ARRAY(e.person_id, e.friend_id) AS path,\n      ARRAY(e.person_id) AS visited\n    FROM catalog.demo.Knows e\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.friend_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.friend_id)) AS path,\n      CONCAT(p.visited, ARRAY(e.person_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.demo.Knows e\n      ON p.end_node = e.person_id\n    WHERE p.depth &lt; 3\n      AND NOT ARRAY_CONTAINS(p.visited, e.friend_id)\n  )\nSELECT \n   _gsql2rsql_p_name AS name\n  ,COUNT(DISTINCT _gsql2rsql_friend_id) AS tech_connections\nFROM (\n  SELECT\n     _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n    ,_left._gsql2rsql_friend_id AS _gsql2rsql_friend_id\n    ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n    ,_left._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,_right._gsql2rsql_c_industry AS _gsql2rsql_c_industry\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_name AS _gsql2rsql_p_name\n      ,_left._gsql2rsql_friend_id AS _gsql2rsql_friend_id\n      ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_right._gsql2rsql__anon1_company_id AS _gsql2rsql__anon1_company_id\n    FROM (\n      SELECT\n         sink.id AS _gsql2rsql_friend_id\n        ,sink.name AS _gsql2rsql_friend_name\n        ,sink.age AS _gsql2rsql_friend_age\n        ,sink.nickname AS _gsql2rsql_friend_nickname\n        ,sink.salary AS _gsql2rsql_friend_salary\n        ,sink.active AS _gsql2rsql_friend_active\n        ,source.id AS _gsql2rsql_p_id\n        ,source.name AS _gsql2rsql_p_name\n        ,source.age AS _gsql2rsql_p_age\n        ,source.nickname AS _gsql2rsql_p_nickname\n        ,source.salary AS _gsql2rsql_p_salary\n        ,source.active AS _gsql2rsql_p_active\n        ,p.start_node\n        ,p.end_node\n        ,p.depth\n        ,p.path\n      FROM paths_1 p\n      JOIN catalog.demo.Person sink\n        ON sink.id = p.end_node\n      JOIN catalog.demo.Person source\n        ON source.id = p.start_node\n      WHERE p.depth &gt;= 1 AND p.depth &lt;= 3\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         person_id AS _gsql2rsql__anon1_person_id\n        ,company_id AS _gsql2rsql__anon1_company_id\n      FROM\n        catalog.demo.WorksAt\n    ) AS _right ON\n      _left._gsql2rsql_friend_id = _right._gsql2rsql__anon1_person_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_c_id\n      ,industry AS _gsql2rsql_c_industry\n    FROM\n      catalog.demo.Company\n  ) AS _right ON\n    _right._gsql2rsql_c_id = _left._gsql2rsql__anon1_company_id\n) AS _proj\nWHERE (_gsql2rsql_c_industry) = ('Technology')\nGROUP BY _gsql2rsql_p_name\nORDER BY tech_connections DESC\nLIMIT 10\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: p:Person\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: friend:Person\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=5)\n    DataSource: [_anon1:WORKS_AT]-&gt;\n*\nOpId=7 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=7)\n    DataSource: c:Company\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(KNOWS*1..3)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=6;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=friend RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=4,5; OutOpIds=8;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=friend RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=6,7; OutOpIds=10;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=ProjectionOperator; InOpIds=8; OutOpIds=;\n  ProjectionOperator(id=10)\n    Projections: name=p.name, tech_connections=COUNT(DISTINCT friend)\n    Filter: (c.industry EQ 'Technology')\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/","title":"Fraud Queries","text":"<p>This page contains transpiled examples for fraud queries queries.</p> <p>Each example shows the original OpenCypher query and its corresponding Databricks SQL translation.</p>"},{"location":"examples/fraud/#1-detect-co-shopper-fraud-rings-via-shared-transaction-paths","title":"1. Detect co-shopper fraud rings via shared transaction paths","text":"<p>Application: Fraud: Co-shopper detection</p> Notes <p>Finds pairs of accounts that share transactions at the same merchant. High shared transaction counts may indicate coordinated fraud or account sharing.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:TRANSACTION]-&gt;(m:Merchant)&lt;-[:TRANSACTION]-(b:Account)\nWHERE a.id &lt;&gt; b.id\nRETURN a.id, b.id, m.name, COUNT(*) AS shared_transactions\nORDER BY shared_transactions DESC\nLIMIT 10\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\n  ,_gsql2rsql_m_name AS name\n  ,COUNT(*) AS shared_transactions\nFROM (\n  SELECT\n     _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n    ,_left._gsql2rsql__anon1_merchant_id AS _gsql2rsql__anon1_merchant_id\n    ,_left._gsql2rsql_m_id AS _gsql2rsql_m_id\n    ,_left._gsql2rsql_m_name AS _gsql2rsql_m_name\n    ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n    ,_left._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n    ,_right._gsql2rsql_b_id AS _gsql2rsql_b_id\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_merchant_id AS _gsql2rsql__anon1_merchant_id\n      ,_left._gsql2rsql_m_id AS _gsql2rsql_m_id\n      ,_left._gsql2rsql_m_name AS _gsql2rsql_m_name\n      ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_right._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql__anon1_merchant_id AS _gsql2rsql__anon1_merchant_id\n        ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n        ,_right._gsql2rsql_m_name AS _gsql2rsql_m_name\n      FROM (\n        SELECT\n           _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_right._gsql2rsql__anon1_merchant_id AS _gsql2rsql__anon1_merchant_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_a_id\n          FROM\n            catalog.fraud.Account\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             account_id AS _gsql2rsql__anon1_account_id\n            ,merchant_id AS _gsql2rsql__anon1_merchant_id\n          FROM\n            catalog.fraud.AccountTransaction\n        ) AS _right ON\n          _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_m_id\n          ,name AS _gsql2rsql_m_name\n        FROM\n          catalog.fraud.Merchant\n      ) AS _right ON\n        _right._gsql2rsql_m_id = _left._gsql2rsql__anon1_merchant_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         account_id AS _gsql2rsql__anon2_account_id\n        ,merchant_id AS _gsql2rsql__anon2_merchant_id\n      FROM\n        catalog.fraud.AccountTransaction\n    ) AS _right ON\n      _left._gsql2rsql_m_id = _right._gsql2rsql__anon2_merchant_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_b_id\n    FROM\n      catalog.fraud.Account\n  ) AS _right ON\n    _right._gsql2rsql_b_id = _left._gsql2rsql__anon2_account_id\n) AS _proj\nWHERE (_gsql2rsql_a_id) != (_gsql2rsql_b_id)\nGROUP BY _gsql2rsql_a_id, _gsql2rsql_b_id, _gsql2rsql_m_name\nORDER BY shared_transactions DESC\nLIMIT 10\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: m:Merchant\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TRANSACTION]&lt;-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: b:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=;\n  ProjectionOperator(id=11)\n    Projections: id=a.id, id=b.id, name=m.name, shared_transactions=COUNT(*)\n    Filter: (a.id NEQ b.id)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#2-identify-camouflage-patterns-with-hidden-relationship-chains","title":"2. Identify camouflage patterns with hidden relationship chains","text":"<p>Application: Fraud: Camouflage detection</p> Notes <p>Detects indirect transfer chains between high-risk accounts. Fraudsters often use intermediary accounts to obscure direct connections.</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Account)-[:TRANSFER*2..4]-&gt;(b:Account)\nWHERE a.risk_score &gt; 70 AND b.risk_score &gt; 70\nRETURN a.id, b.id, LENGTH(path) AS chain_length,\n       [node IN nodes(path) | node.id] AS path_nodes\nORDER BY chain_length DESC\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_account_id AS start_node,\n      e.target_account_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_account_id, e.target_account_id) AS path,\n      ARRAY(NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      ARRAY(e.source_account_id) AS visited\n    FROM catalog.fraud.Transfer e\n    JOIN catalog.fraud.Account src ON src.id = e.source_account_id\n    WHERE (src.risk_score) &gt; (70)\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_account_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_account_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.source_account_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.fraud.Transfer e\n      ON p.end_node = e.source_account_id\n    WHERE p.depth &lt; 4\n      AND NOT ARRAY_CONTAINS(p.visited, e.target_account_id)\n  )\nSELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_b_id AS id\n  ,(SIZE(_gsql2rsql_path_id) - 1) AS chain_length\n  ,_gsql2rsql_path_id AS path_nodes\nFROM (\n  SELECT\n     sink.id AS _gsql2rsql_b_id\n    ,sink.holder_name AS _gsql2rsql_b_holder_name\n    ,sink.risk_score AS _gsql2rsql_b_risk_score\n    ,sink.status AS _gsql2rsql_b_status\n    ,sink.default_date AS _gsql2rsql_b_default_date\n    ,sink.home_country AS _gsql2rsql_b_home_country\n    ,sink.kyc_status AS _gsql2rsql_b_kyc_status\n    ,sink.days_since_creation AS _gsql2rsql_b_days_since_creation\n    ,source.id AS _gsql2rsql_a_id\n    ,source.holder_name AS _gsql2rsql_a_holder_name\n    ,source.risk_score AS _gsql2rsql_a_risk_score\n    ,source.status AS _gsql2rsql_a_status\n    ,source.default_date AS _gsql2rsql_a_default_date\n    ,source.home_country AS _gsql2rsql_a_home_country\n    ,source.kyc_status AS _gsql2rsql_a_kyc_status\n    ,source.days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,p.start_node\n    ,p.end_node\n    ,p.depth\n    ,p.path AS _gsql2rsql_path_id\n    ,p.path_edges AS _gsql2rsql_path_edges\n  FROM paths_1 p\n  JOIN catalog.fraud.Account sink\n    ON sink.id = p.end_node\n  JOIN catalog.fraud.Account source\n    ON source.id = p.start_node\n  WHERE p.depth &gt;= 2 AND p.depth &lt;= 4 AND (sink.risk_score) &gt; (70)\n) AS _proj\nORDER BY chain_length DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: b:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(TRANSFER*2..4, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=b RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=;\n  ProjectionOperator(id=5)\n    Projections: id=a.id, id=b.id, chain_length=LENGTH(path), path_nodes=[node IN NODES(path) | node.id]\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#3-find-high-risk-pos-machines-with-suspicious-transaction-patterns","title":"3. Find high-risk POS machines with suspicious transaction patterns","text":"<p>Application: Fraud: High-risk device monitoring</p> Notes <p>Monitors POS machines already flagged as high-risk for ongoing suspicious activity. Filters by vertex metadata (risk_status, flagged) to focus on known problem devices. Total volume and transaction counts help prioritize investigation.</p> OpenCypher Query Cypher<pre><code>MATCH (p:POS)-[:PROCESSED]-&gt;(t:Transaction)\nWHERE p.risk_status = 'high_risk' OR p.flagged = true\nWITH p,\n     COUNT(t) AS total_transactions,\n     SUM(t.amount) AS total_volume,\n     AVG(t.amount) AS avg_amount,\n     STDDEV(t.amount) AS stddev_amount\nWHERE total_transactions &gt; 50\nRETURN p.id, p.location, p.risk_status,\n       total_transactions, total_volume, avg_amount, stddev_amount\nORDER BY total_volume DESC\nLIMIT 20\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_p_id AS id\n  ,_gsql2rsql_p_location AS location\n  ,_gsql2rsql_p_risk_status AS risk_status\n  ,total_transactions AS total_transactions\n  ,total_volume AS total_volume\n  ,avg_amount AS avg_amount\n  ,stddev_amount AS stddev_amount\nFROM (\n  SELECT \n     _gsql2rsql_p_id AS _gsql2rsql_p_id\n    ,COUNT(_gsql2rsql_t_id) AS total_transactions\n    ,SUM(_gsql2rsql_t_amount) AS total_volume\n    ,AVG(CAST(_gsql2rsql_t_amount AS DOUBLE)) AS avg_amount\n    ,STDDEV(_gsql2rsql_t_amount) AS stddev_amount\n    ,_gsql2rsql_p_flagged AS _gsql2rsql_p_flagged\n    ,_gsql2rsql_p_location AS _gsql2rsql_p_location\n    ,_gsql2rsql_p_risk_status AS _gsql2rsql_p_risk_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n      ,_left._gsql2rsql_p_location AS _gsql2rsql_p_location\n      ,_left._gsql2rsql_p_risk_status AS _gsql2rsql_p_risk_status\n      ,_left._gsql2rsql_p_flagged AS _gsql2rsql_p_flagged\n      ,_left._gsql2rsql__anon1_pos_id AS _gsql2rsql__anon1_pos_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n    FROM (\n      SELECT\n         _left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql_p_location AS _gsql2rsql_p_location\n        ,_left._gsql2rsql_p_risk_status AS _gsql2rsql_p_risk_status\n        ,_left._gsql2rsql_p_flagged AS _gsql2rsql_p_flagged\n        ,_right._gsql2rsql__anon1_pos_id AS _gsql2rsql__anon1_pos_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_p_id\n          ,location AS _gsql2rsql_p_location\n          ,risk_status AS _gsql2rsql_p_risk_status\n          ,flagged AS _gsql2rsql_p_flagged\n        FROM\n          catalog.fraud.POS\n        WHERE (((risk_status) = ('high_risk')) OR ((flagged) = (TRUE)))\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           pos_id AS _gsql2rsql__anon1_pos_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.POSTransaction\n      ) AS _right ON\n        _left._gsql2rsql_p_id = _right._gsql2rsql__anon1_pos_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n      FROM\n        catalog.fraud.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_p_id, _gsql2rsql_p_flagged, _gsql2rsql_p_location, _gsql2rsql_p_risk_status\n  HAVING (total_transactions) &gt; (50)\n) AS _proj\nORDER BY total_volume DESC\nLIMIT 20\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: p:POS\n    Filter: ((p.risk_status EQ 'high_risk') OR (p.flagged EQ true))\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:PROCESSED]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: p=p, total_transactions=COUNT(t), total_volume=SUM(t.amount), avg_amount=AVG(t.amount), stddev_amount=STDEV(t.amount)\n    Having: (total_transactions GT 50)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=p.id, location=p.location, risk_status=p.risk_status, total_transactions=total_transactions, total_volume=total_volume, avg_amount=avg_amount, stddev_amount=stddev_amount\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#4-detect-synthetic-identity-networks-via-shared-attributes","title":"4. Detect synthetic identity networks via shared attributes","text":"<p>Application: Fraud: Synthetic identity detection</p> Notes <p>Finds addresses associated with multiple recently created accounts. May indicate synthetic identities or identity fabrication rings. Uses WITH...WHERE pattern for HAVING-like filtering on aggregated columns.</p> OpenCypher Query Cypher<pre><code>MATCH (p1:Person)-[:HAS_ADDRESS]-&gt;(addr:Address)&lt;-[:HAS_ADDRESS]-(p2:Person)\nWHERE p1.id &lt;&gt; p2.id AND p1.creation_date &gt; DATE('2023-01-01')\nWITH addr.street AS street, addr.city AS city, COUNT(DISTINCT p1.id) AS person_count\nWHERE person_count &gt; 5\nRETURN street, city, person_count\nORDER BY person_count DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   street AS street\n  ,city AS city\n  ,person_count AS person_count\nFROM (\n  SELECT \n     _gsql2rsql_addr_street AS street\n    ,_gsql2rsql_addr_city AS city\n    ,COUNT(DISTINCT _gsql2rsql_p1_id) AS person_count\n  FROM (\n    SELECT\n       _left._gsql2rsql_p1_id AS _gsql2rsql_p1_id\n      ,_left._gsql2rsql_p1_creation_date AS _gsql2rsql_p1_creation_date\n      ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n      ,_left._gsql2rsql__anon1_address_id AS _gsql2rsql__anon1_address_id\n      ,_left._gsql2rsql_addr_id AS _gsql2rsql_addr_id\n      ,_left._gsql2rsql_addr_street AS _gsql2rsql_addr_street\n      ,_left._gsql2rsql_addr_city AS _gsql2rsql_addr_city\n      ,_left._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n      ,_left._gsql2rsql__anon2_address_id AS _gsql2rsql__anon2_address_id\n      ,_right._gsql2rsql_p2_id AS _gsql2rsql_p2_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_p1_id AS _gsql2rsql_p1_id\n        ,_left._gsql2rsql_p1_creation_date AS _gsql2rsql_p1_creation_date\n        ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n        ,_left._gsql2rsql__anon1_address_id AS _gsql2rsql__anon1_address_id\n        ,_left._gsql2rsql_addr_id AS _gsql2rsql_addr_id\n        ,_left._gsql2rsql_addr_street AS _gsql2rsql_addr_street\n        ,_left._gsql2rsql_addr_city AS _gsql2rsql_addr_city\n        ,_right._gsql2rsql__anon2_person_id AS _gsql2rsql__anon2_person_id\n        ,_right._gsql2rsql__anon2_address_id AS _gsql2rsql__anon2_address_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_p1_id AS _gsql2rsql_p1_id\n          ,_left._gsql2rsql_p1_creation_date AS _gsql2rsql_p1_creation_date\n          ,_left._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n          ,_left._gsql2rsql__anon1_address_id AS _gsql2rsql__anon1_address_id\n          ,_right._gsql2rsql_addr_id AS _gsql2rsql_addr_id\n          ,_right._gsql2rsql_addr_street AS _gsql2rsql_addr_street\n          ,_right._gsql2rsql_addr_city AS _gsql2rsql_addr_city\n        FROM (\n          SELECT\n             _left._gsql2rsql_p1_id AS _gsql2rsql_p1_id\n            ,_left._gsql2rsql_p1_creation_date AS _gsql2rsql_p1_creation_date\n            ,_right._gsql2rsql__anon1_person_id AS _gsql2rsql__anon1_person_id\n            ,_right._gsql2rsql__anon1_address_id AS _gsql2rsql__anon1_address_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_p1_id\n              ,creation_date AS _gsql2rsql_p1_creation_date\n            FROM\n              catalog.fraud.Person\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               person_id AS _gsql2rsql__anon1_person_id\n              ,address_id AS _gsql2rsql__anon1_address_id\n            FROM\n              catalog.fraud.PersonAddress\n          ) AS _right ON\n            _left._gsql2rsql_p1_id = _right._gsql2rsql__anon1_person_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_addr_id\n            ,street AS _gsql2rsql_addr_street\n            ,city AS _gsql2rsql_addr_city\n          FROM\n            catalog.fraud.Address\n        ) AS _right ON\n          _right._gsql2rsql_addr_id = _left._gsql2rsql__anon1_address_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           person_id AS _gsql2rsql__anon2_person_id\n          ,address_id AS _gsql2rsql__anon2_address_id\n        FROM\n          catalog.fraud.PersonAddress\n      ) AS _right ON\n        _left._gsql2rsql_addr_id = _right._gsql2rsql__anon2_address_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_p2_id\n      FROM\n        catalog.fraud.Person\n    ) AS _right ON\n      _right._gsql2rsql_p2_id = _left._gsql2rsql__anon2_person_id\n  ) AS _proj\n  WHERE ((_gsql2rsql_p1_id) != (_gsql2rsql_p2_id)) AND ((_gsql2rsql_p1_creation_date) &gt; (TO_DATE('2023-01-01')))\n  GROUP BY _gsql2rsql_addr_street, _gsql2rsql_addr_city\n  HAVING (person_count) &gt; (5)\n) AS _proj\nORDER BY person_count DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: p1:Person\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_ADDRESS]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: addr:Address\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:HAS_ADDRESS]&lt;-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: p2:Person\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=p1 RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=addr RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=addr RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=p2 RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: street=addr.street, city=addr.city, person_count=COUNT(DISTINCT p1.id)\n    Filter: ((p1.id NEQ p2.id) AND (p1.creation_date GT DATE('2023-01-01')))\n    Having: (person_count GT 5)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: street=street, city=city, person_count=person_count\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#5-identify-card-testing-patterns-with-small-probe-transactions","title":"5. Identify card testing patterns with small probe transactions","text":"<p>Application: Fraud: Card testing</p> Notes <p>Detects cards with many small-value transactions in a short time. Common pattern for fraudsters testing stolen card validity.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Card)-[:USED_IN]-&gt;(t:Transaction)\nWHERE t.amount &lt; 1.00 AND t.timestamp &gt; TIMESTAMP() - DURATION('P1D')\nWITH c, COUNT(t) AS small_tx_count, COLLECT(t.merchant_id) AS merchants\nWHERE small_tx_count &gt; 10\nRETURN c.number, small_tx_count, SIZE(merchants) AS merchant_count\nORDER BY small_tx_count DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_number AS number\n  ,small_tx_count AS small_tx_count\n  ,SIZE(merchants) AS merchant_count\nFROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,COUNT(_gsql2rsql_t_id) AS small_tx_count\n    ,COLLECT_LIST(_gsql2rsql_t_merchant_id) AS merchants\n    ,_gsql2rsql_c_number AS _gsql2rsql_c_number\n  FROM (\n    SELECT\n       _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_left._gsql2rsql_c_number AS _gsql2rsql_c_number\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n      ,_right._gsql2rsql_t_merchant_id AS _gsql2rsql_t_merchant_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_number AS _gsql2rsql_c_number\n        ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_c_id\n          ,number AS _gsql2rsql_c_number\n        FROM\n          catalog.fraud.Card\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           card_id AS _gsql2rsql__anon1_card_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.CardTransaction\n      ) AS _right ON\n        _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_card_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n        ,merchant_id AS _gsql2rsql_t_merchant_id\n      FROM\n        catalog.fraud.Transaction\n      WHERE ((amount) &lt; (1.0))\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 1 DAY))\n  GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_number\n  HAVING (small_tx_count) &gt; (10)\n) AS _proj\nORDER BY small_tx_count DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: c:Card\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:USED_IN]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n    Filter: (t.amount LT 1.0)\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: c=c, small_tx_count=COUNT(t), merchants=COLLECT(t.merchant_id)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('P1D')))\n    Having: (small_tx_count GT 10)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: number=c.number, small_tx_count=small_tx_count, merchant_count=SIZE(merchants)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#6-find-collusion-networks-via-coordinated-transaction-timing","title":"6. Find collusion networks via coordinated transaction timing","text":"<p>Application: Fraud: Collusion detection</p> Notes <p>Identifies accounts making synchronized transactions at the same merchant. May indicate coordinated fraud or collusion rings. Uses WITH...WHERE pattern for HAVING-like filtering on aggregated columns.</p> OpenCypher Query Cypher<pre><code>MATCH (a1:Account)-[:HAS_TRANSACTION]-&gt;(t1:Transaction),\n      (a2:Account)-[:HAS_TRANSACTION]-&gt;(t2:Transaction)\nWHERE a1.id &lt; a2.id\n  AND ABS(t1.timestamp - t2.timestamp) &lt; DURATION('PT5M')\n  AND t1.merchant_id = t2.merchant_id\nWITH a1.id AS a1_id, a2.id AS a2_id, t1.merchant_id AS merchant_id, COUNT(*) AS coordinated_count\nWHERE coordinated_count &gt; 5\nRETURN a1_id, a2_id, merchant_id, coordinated_count\nORDER BY coordinated_count DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   a1_id AS a1_id\n  ,a2_id AS a2_id\n  ,merchant_id AS merchant_id\n  ,coordinated_count AS coordinated_count\nFROM (\n  SELECT \n     _gsql2rsql_a1_id AS a1_id\n    ,_gsql2rsql_a2_id AS a2_id\n    ,_gsql2rsql_t1_merchant_id AS merchant_id\n    ,COUNT(*) AS coordinated_count\n  FROM (\n    SELECT\n       _left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_left._gsql2rsql_t1_id AS _gsql2rsql_t1_id\n      ,_left._gsql2rsql_t1_timestamp AS _gsql2rsql_t1_timestamp\n      ,_left._gsql2rsql_t1_merchant_id AS _gsql2rsql_t1_merchant_id\n      ,_left._gsql2rsql_a2_id AS _gsql2rsql_a2_id\n      ,_left._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_right._gsql2rsql_t2_id AS _gsql2rsql_t2_id\n      ,_right._gsql2rsql_t2_timestamp AS _gsql2rsql_t2_timestamp\n      ,_right._gsql2rsql_t2_merchant_id AS _gsql2rsql_t2_merchant_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n        ,_left._gsql2rsql_t1_id AS _gsql2rsql_t1_id\n        ,_left._gsql2rsql_t1_timestamp AS _gsql2rsql_t1_timestamp\n        ,_left._gsql2rsql_t1_merchant_id AS _gsql2rsql_t1_merchant_id\n        ,_left._gsql2rsql_a2_id AS _gsql2rsql_a2_id\n        ,_right._gsql2rsql__anon2_account_id AS _gsql2rsql__anon2_account_id\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n          ,_left._gsql2rsql_t1_id AS _gsql2rsql_t1_id\n          ,_left._gsql2rsql_t1_timestamp AS _gsql2rsql_t1_timestamp\n          ,_left._gsql2rsql_t1_merchant_id AS _gsql2rsql_t1_merchant_id\n          ,_right._gsql2rsql_a2_id AS _gsql2rsql_a2_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n            ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n            ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n            ,_right._gsql2rsql_t1_id AS _gsql2rsql_t1_id\n            ,_right._gsql2rsql_t1_timestamp AS _gsql2rsql_t1_timestamp\n            ,_right._gsql2rsql_t1_merchant_id AS _gsql2rsql_t1_merchant_id\n          FROM (\n            SELECT\n               _left._gsql2rsql_a1_id AS _gsql2rsql_a1_id\n              ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n              ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n            FROM (\n              SELECT\n                 id AS _gsql2rsql_a1_id\n              FROM\n                catalog.fraud.Account\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 account_id AS _gsql2rsql__anon1_account_id\n                ,transaction_id AS _gsql2rsql__anon1_transaction_id\n              FROM\n                catalog.fraud.AccountTx\n            ) AS _right ON\n              _left._gsql2rsql_a1_id = _right._gsql2rsql__anon1_account_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               id AS _gsql2rsql_t1_id\n              ,timestamp AS _gsql2rsql_t1_timestamp\n              ,merchant_id AS _gsql2rsql_t1_merchant_id\n            FROM\n              catalog.fraud.Transaction\n          ) AS _right ON\n            _right._gsql2rsql_t1_id = _left._gsql2rsql__anon1_transaction_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_a2_id\n          FROM\n            catalog.fraud.Account\n        ) AS _right ON\n          TRUE\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon2_account_id\n          ,transaction_id AS _gsql2rsql__anon2_transaction_id\n        FROM\n          catalog.fraud.AccountTx\n      ) AS _right ON\n        _left._gsql2rsql_a2_id = _right._gsql2rsql__anon2_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t2_id\n        ,timestamp AS _gsql2rsql_t2_timestamp\n        ,merchant_id AS _gsql2rsql_t2_merchant_id\n      FROM\n        catalog.fraud.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t2_id = _left._gsql2rsql__anon2_transaction_id\n  ) AS _proj\n  WHERE (((_gsql2rsql_a1_id) &lt; (_gsql2rsql_a2_id)) AND ((ABS((UNIX_TIMESTAMP(_gsql2rsql_t1_timestamp) - UNIX_TIMESTAMP(_gsql2rsql_t2_timestamp)))) &lt; (300))) AND ((_gsql2rsql_t1_merchant_id) = (_gsql2rsql_t2_merchant_id))\n  GROUP BY _gsql2rsql_a1_id, _gsql2rsql_a2_id, _gsql2rsql_t1_merchant_id\n  HAVING (coordinated_count) &gt; (5)\n) AS _proj\nORDER BY coordinated_count DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=1)\n    DataSource: a1:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=3)\n    DataSource: t1:Transaction\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=4)\n    DataSource: a2:Account\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=10;\n  DataSourceOperator(id=5)\n    DataSource: [_anon2:HAS_TRANSACTION]-&gt;\n*\nOpId=6 Op=DataSourceOperator; InOpIds=; OutOpIds=11;\n  DataSourceOperator(id=6)\n    DataSource: t2:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=1,2; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=a1 RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,3; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=t1 RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,4; OutOpIds=10;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: \n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=10 Op=JoinOperator; InOpIds=9,5; OutOpIds=11;\n  JoinOperator(id=10)\n    JoinType: INNER\n    Joins: JoinPair: Node=a2 RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=JoinOperator; InOpIds=10,6; OutOpIds=13;\n  JoinOperator(id=11)\n    JoinType: INNER\n    Joins: JoinPair: Node=t2 RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=13 Op=ProjectionOperator; InOpIds=11; OutOpIds=14;\n  ProjectionOperator(id=13)\n    Projections: a1_id=a1.id, a2_id=a2.id, merchant_id=t1.merchant_id, coordinated_count=COUNT(*)\n    Filter: (((a1.id LT a2.id) AND (ABS((t1.timestamp MINUS t2.timestamp)) LT DURATION('PT5M'))) AND (t1.merchant_id EQ t2.merchant_id))\n    Having: (coordinated_count GT 5)\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=14 Op=ProjectionOperator; InOpIds=13; OutOpIds=;\n  ProjectionOperator(id=14)\n    Projections: a1_id=a1_id, a2_id=a2_id, merchant_id=merchant_id, coordinated_count=coordinated_count\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#7-trace-money-mule-networks-with-rapid-transfer-chains","title":"7. Trace money mule networks with rapid transfer chains","text":"<p>Application: Fraud: Money mule detection</p> Notes <p>Finds multi-hop transfer chains moving large amounts quickly. Classic pattern for money laundering via mule accounts.</p> OpenCypher Query Cypher<pre><code>MATCH path = (source:Account)-[:TRANSFER*3..6]-&gt;(sink:Account)\nWHERE ALL(rel IN relationships(path) WHERE rel.timestamp &gt; TIMESTAMP() - DURATION('P7D'))\n  AND ALL(rel IN relationships(path) WHERE rel.amount &gt; 1000)\nWITH source, sink, path,\n     REDUCE(total = 0, rel IN relationships(path) | total + rel.amount) AS total_amount\nRETURN source.id, sink.id, LENGTH(path) AS hops, total_amount\nORDER BY total_amount DESC\nLIMIT 15\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_account_id AS start_node,\n      e.target_account_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_account_id, e.target_account_id) AS path,\n      ARRAY(NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      ARRAY(e.source_account_id) AS visited\n    FROM catalog.fraud.Transfer e\n    WHERE ((e.timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 7 DAY))) AND ((e.amount) &gt; (1000))\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_account_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_account_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.source_account_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.fraud.Transfer e\n      ON p.end_node = e.source_account_id\n    WHERE p.depth &lt; 6\n      AND NOT ARRAY_CONTAINS(p.visited, e.target_account_id)\n      AND ((e.timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 7 DAY))) AND ((e.amount) &gt; (1000))\n  )\nSELECT \n   _gsql2rsql_source_id AS id\n  ,_gsql2rsql_sink_id AS id\n  ,(SIZE(_gsql2rsql_path_id) - 1) AS hops\n  ,total_amount AS total_amount\nFROM (\n  SELECT \n     _gsql2rsql_source_id AS _gsql2rsql_source_id\n    ,_gsql2rsql_sink_id AS _gsql2rsql_sink_id\n    ,_gsql2rsql_path_id AS _gsql2rsql_path_id\n    ,AGGREGATE(_gsql2rsql_path_edges, CAST(0 AS DOUBLE), (total, rel) -&gt; (total) + (rel.amount)) AS total_amount\n    ,_gsql2rsql_sink_days_since_creation AS _gsql2rsql_sink_days_since_creation\n    ,_gsql2rsql_sink_default_date AS _gsql2rsql_sink_default_date\n    ,_gsql2rsql_sink_holder_name AS _gsql2rsql_sink_holder_name\n    ,_gsql2rsql_sink_home_country AS _gsql2rsql_sink_home_country\n    ,_gsql2rsql_sink_kyc_status AS _gsql2rsql_sink_kyc_status\n    ,_gsql2rsql_sink_risk_score AS _gsql2rsql_sink_risk_score\n    ,_gsql2rsql_sink_status AS _gsql2rsql_sink_status\n    ,_gsql2rsql_source_days_since_creation AS _gsql2rsql_source_days_since_creation\n    ,_gsql2rsql_source_default_date AS _gsql2rsql_source_default_date\n    ,_gsql2rsql_source_holder_name AS _gsql2rsql_source_holder_name\n    ,_gsql2rsql_source_home_country AS _gsql2rsql_source_home_country\n    ,_gsql2rsql_source_kyc_status AS _gsql2rsql_source_kyc_status\n    ,_gsql2rsql_source_risk_score AS _gsql2rsql_source_risk_score\n    ,_gsql2rsql_source_status AS _gsql2rsql_source_status\n  FROM (\n    SELECT\n       sink.id AS _gsql2rsql_sink_id\n      ,sink.holder_name AS _gsql2rsql_sink_holder_name\n      ,sink.risk_score AS _gsql2rsql_sink_risk_score\n      ,sink.status AS _gsql2rsql_sink_status\n      ,sink.default_date AS _gsql2rsql_sink_default_date\n      ,sink.home_country AS _gsql2rsql_sink_home_country\n      ,sink.kyc_status AS _gsql2rsql_sink_kyc_status\n      ,sink.days_since_creation AS _gsql2rsql_sink_days_since_creation\n      ,source.id AS _gsql2rsql_source_id\n      ,source.holder_name AS _gsql2rsql_source_holder_name\n      ,source.risk_score AS _gsql2rsql_source_risk_score\n      ,source.status AS _gsql2rsql_source_status\n      ,source.default_date AS _gsql2rsql_source_default_date\n      ,source.home_country AS _gsql2rsql_source_home_country\n      ,source.kyc_status AS _gsql2rsql_source_kyc_status\n      ,source.days_since_creation AS _gsql2rsql_source_days_since_creation\n      ,p.start_node\n      ,p.end_node\n      ,p.depth\n      ,p.path AS _gsql2rsql_path_id\n      ,p.path_edges AS _gsql2rsql_path_edges\n    FROM paths_1 p\n    JOIN catalog.fraud.Account sink\n      ON sink.id = p.end_node\n    JOIN catalog.fraud.Account source\n      ON source.id = p.start_node\n    WHERE p.depth &gt;= 3 AND p.depth &lt;= 6\n  ) AS _proj\n) AS _proj\nORDER BY total_amount DESC\nLIMIT 15\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: source:Account\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: sink:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(TRANSFER*3..6, path=path)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=sink RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=5 Op=ProjectionOperator; InOpIds=4; OutOpIds=6;\n  ProjectionOperator(id=5)\n    Projections: source=source, sink=sink, path=path, total_amount=REDUCE(total = 0, rel IN RELATIONSHIPS(path) | (total PLUS rel.amount))\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=;\n  ProjectionOperator(id=6)\n    Projections: id=source.id, id=sink.id, hops=LENGTH(path), total_amount=total_amount\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#8-calculate-customer-similarity-via-shared-card-usage-patterns","title":"8. Calculate customer similarity via shared card usage patterns","text":"<p>Application: Fraud: Customer similarity clustering</p> Notes <p>Measures similarity between customers based on shared cards and merchants. High similarity scores may indicate card sharing, family fraud, or organized rings. Combines multiple metrics for more accurate clustering.</p> OpenCypher Query Cypher<pre><code>MATCH (c1:Customer)-[:HAS_CARD]-&gt;(card:Card)&lt;-[:HAS_CARD]-(c2:Customer)\nWHERE c1.id &lt; c2.id\nWITH c1, c2, COUNT(DISTINCT card) AS shared_cards\nWHERE shared_cards &gt; 0\nMATCH (c1)-[:HAS_CARD]-&gt;(card1:Card)-[:USED_AT]-&gt;(m:Merchant)\nMATCH (c2)-[:HAS_CARD]-&gt;(card2:Card)-[:USED_AT]-&gt;(m)\nWITH c1, c2, shared_cards,\n     COUNT(DISTINCT m) AS shared_merchants,\n     shared_cards * 1.0 / (shared_cards + shared_merchants) AS similarity_score\nWHERE similarity_score &gt; 0.3\nRETURN c1.id, c2.id, shared_cards, shared_merchants, similarity_score\nORDER BY similarity_score DESC\nLIMIT 50\n</code></pre> Generated SQL SQL<pre><code>WITH\nagg_boundary_1 AS (\n  SELECT\n    _gsql2rsql_c1_id AS `c1`,\n    _gsql2rsql_c2_id AS `c2`,\n    COUNT(DISTINCT _gsql2rsql_card_id) AS `shared_cards`\n  FROM (\n  SELECT *\n  FROM (\n    SELECT\n       _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n      ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n      ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n      ,_left._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n      ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n      ,_right._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n      ,_right._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n      ,_right._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n    FROM (\n      SELECT\n         _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n        ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n        ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n        ,_right._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n        ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n          ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n          ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_right._gsql2rsql_card_id AS _gsql2rsql_card_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n            ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n            ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n            ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_c1_id\n              ,name AS _gsql2rsql_c1_name\n              ,status AS _gsql2rsql_c1_status\n            FROM\n              catalog.fraud.Customer\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               customer_id AS _gsql2rsql__anon1_customer_id\n              ,card_id AS _gsql2rsql__anon1_card_id\n            FROM\n              catalog.fraud.CustomerCard\n          ) AS _right ON\n            _left._gsql2rsql_c1_id = _right._gsql2rsql__anon1_customer_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_card_id\n          FROM\n            catalog.fraud.Card\n        ) AS _right ON\n          _right._gsql2rsql_card_id = _left._gsql2rsql__anon1_card_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           customer_id AS _gsql2rsql__anon2_customer_id\n          ,card_id AS _gsql2rsql__anon2_card_id\n        FROM\n          catalog.fraud.CustomerCard\n      ) AS _right ON\n        _left._gsql2rsql_card_id = _right._gsql2rsql__anon2_card_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_c2_id\n        ,name AS _gsql2rsql_c2_name\n        ,status AS _gsql2rsql_c2_status\n      FROM\n        catalog.fraud.Customer\n    ) AS _right ON\n      _right._gsql2rsql_c2_id = _left._gsql2rsql__anon2_customer_id\n  ) AS _filter\n  WHERE (_gsql2rsql_c1_id) &lt; (_gsql2rsql_c2_id)\n  ) AS _agg_input\n  GROUP BY _gsql2rsql_c1_id, _gsql2rsql_c2_id\n  HAVING (shared_cards) &gt; (0)\n)\nSELECT \n   _gsql2rsql_c1_id AS id\n  ,_gsql2rsql_c2_id AS id\n  ,shared_cards AS shared_cards\n  ,shared_merchants AS shared_merchants\n  ,similarity_score AS similarity_score\nFROM (\n  SELECT \n     _gsql2rsql_c1_id AS _gsql2rsql_c1_id\n    ,_gsql2rsql_c2_id AS _gsql2rsql_c2_id\n    ,shared_cards AS shared_cards\n    ,COUNT(DISTINCT _gsql2rsql_m_id) AS shared_merchants\n    ,((shared_cards) * (1.0)) / ((shared_cards) + (shared_merchants)) AS similarity_score\n    ,_gsql2rsql_c1_name AS _gsql2rsql_c1_name\n    ,_gsql2rsql_c1_status AS _gsql2rsql_c1_status\n    ,_gsql2rsql_c2_name AS _gsql2rsql_c2_name\n    ,_gsql2rsql_c2_status AS _gsql2rsql_c2_status\n  FROM (\n    SELECT\n       _left.c1 AS c1\n      ,_left.c2 AS c2\n      ,_left.shared_cards AS shared_cards\n      ,_left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n      ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n      ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql_card1_id AS _gsql2rsql_card1_id\n      ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n      ,_left._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n      ,_left._gsql2rsql_m_id AS _gsql2rsql_m_id\n      ,_right._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n      ,_right._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n      ,_right._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n      ,_right._gsql2rsql_card2_id AS _gsql2rsql_card2_id\n    FROM (\n      SELECT\n         _left.`c1` AS `c1`\n        ,_left.`c2` AS `c2`\n        ,_left.`shared_cards` AS `shared_cards`\n        ,_right._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n        ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n        ,_right._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n        ,_right._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n        ,_right._gsql2rsql_card1_id AS _gsql2rsql_card1_id\n        ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n        ,_right._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n      FROM (\n        SELECT\n           `c1`\n          ,`c2`\n          ,`shared_cards`\n        FROM agg_boundary_1\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n          ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n          ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_left._gsql2rsql_card1_id AS _gsql2rsql_card1_id\n          ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n          ,_left._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n          ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n            ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n            ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n            ,_left._gsql2rsql_card1_id AS _gsql2rsql_card1_id\n            ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n            ,_right._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n          FROM (\n            SELECT\n               _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n              ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n              ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n              ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n              ,_right._gsql2rsql_card1_id AS _gsql2rsql_card1_id\n            FROM (\n              SELECT\n                 _left._gsql2rsql_c1_id AS _gsql2rsql_c1_id\n                ,_left._gsql2rsql_c1_name AS _gsql2rsql_c1_name\n                ,_left._gsql2rsql_c1_status AS _gsql2rsql_c1_status\n                ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n                ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n              FROM (\n                SELECT\n                   id AS _gsql2rsql_c1_id\n                  ,name AS _gsql2rsql_c1_name\n                  ,status AS _gsql2rsql_c1_status\n                FROM\n                  catalog.fraud.Customer\n              ) AS _left\n              INNER JOIN (\n                SELECT\n                   customer_id AS _gsql2rsql__anon1_customer_id\n                  ,card_id AS _gsql2rsql__anon1_card_id\n                FROM\n                  catalog.fraud.CustomerCard\n              ) AS _right ON\n                _left._gsql2rsql_c1_id = _right._gsql2rsql__anon1_customer_id\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 id AS _gsql2rsql_card1_id\n              FROM\n                catalog.fraud.Card\n            ) AS _right ON\n              _right._gsql2rsql_card1_id = _left._gsql2rsql__anon1_card_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               card_id AS _gsql2rsql__anon2_card_id\n              ,merchant_id AS _gsql2rsql__anon2_merchant_id\n            FROM\n              catalog.fraud.CardMerchant\n          ) AS _right ON\n            _left._gsql2rsql_card1_id = _right._gsql2rsql__anon2_card_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_m_id\n          FROM\n            catalog.fraud.Merchant\n        ) AS _right ON\n          _right._gsql2rsql_m_id = _left._gsql2rsql__anon2_merchant_id\n      ) AS _right ON\n        _left.`c1` = _right._gsql2rsql_c1_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         _left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n        ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n        ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_left._gsql2rsql_card2_id AS _gsql2rsql_card2_id\n        ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n        ,_left._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n        ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n          ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n          ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_left._gsql2rsql_card2_id AS _gsql2rsql_card2_id\n          ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n          ,_right._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n            ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n            ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n            ,_right._gsql2rsql_card2_id AS _gsql2rsql_card2_id\n          FROM (\n            SELECT\n               _left._gsql2rsql_c2_id AS _gsql2rsql_c2_id\n              ,_left._gsql2rsql_c2_name AS _gsql2rsql_c2_name\n              ,_left._gsql2rsql_c2_status AS _gsql2rsql_c2_status\n              ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n            FROM (\n              SELECT\n                 id AS _gsql2rsql_c2_id\n                ,name AS _gsql2rsql_c2_name\n                ,status AS _gsql2rsql_c2_status\n              FROM\n                catalog.fraud.Customer\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 customer_id AS _gsql2rsql__anon1_customer_id\n                ,card_id AS _gsql2rsql__anon1_card_id\n              FROM\n                catalog.fraud.CustomerCard\n            ) AS _right ON\n              _left._gsql2rsql_c2_id = _right._gsql2rsql__anon1_customer_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               id AS _gsql2rsql_card2_id\n            FROM\n              catalog.fraud.Card\n          ) AS _right ON\n            _right._gsql2rsql_card2_id = _left._gsql2rsql__anon1_card_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             card_id AS _gsql2rsql__anon2_card_id\n            ,merchant_id AS _gsql2rsql__anon2_merchant_id\n          FROM\n            catalog.fraud.CardMerchant\n        ) AS _right ON\n          _left._gsql2rsql_card2_id = _right._gsql2rsql__anon2_card_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_m_id\n        FROM\n          catalog.fraud.Merchant\n      ) AS _right ON\n        _right._gsql2rsql_m_id = _left._gsql2rsql__anon2_merchant_id\n    ) AS _right ON\n      _left._gsql2rsql_m_id = _right._gsql2rsql_m_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_c1_id, _gsql2rsql_c2_id, shared_cards, _gsql2rsql_c1_name, _gsql2rsql_c1_status, _gsql2rsql_c2_name, _gsql2rsql_c2_status\n  HAVING (similarity_score) &gt; (0.3)\n) AS _proj\nORDER BY similarity_score DESC\nLIMIT 50\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c1:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: card:Card\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:HAS_CARD]&lt;-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: c2:Customer\n*\nOpId=12 Op=DataSourceOperator; InOpIds=; OutOpIds=17;\n  DataSourceOperator(id=12)\n    DataSource: c1:Customer\n*\nOpId=13 Op=DataSourceOperator; InOpIds=; OutOpIds=17;\n  DataSourceOperator(id=13)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=14 Op=DataSourceOperator; InOpIds=; OutOpIds=18;\n  DataSourceOperator(id=14)\n    DataSource: card1:Card\n*\nOpId=15 Op=DataSourceOperator; InOpIds=; OutOpIds=19;\n  DataSourceOperator(id=15)\n    DataSource: [_anon2:USED_AT]-&gt;\n*\nOpId=16 Op=DataSourceOperator; InOpIds=; OutOpIds=20;\n  DataSourceOperator(id=16)\n    DataSource: m:Merchant\n*\nOpId=22 Op=DataSourceOperator; InOpIds=; OutOpIds=27;\n  DataSourceOperator(id=22)\n    DataSource: c2:Customer\n*\nOpId=23 Op=DataSourceOperator; InOpIds=; OutOpIds=27;\n  DataSourceOperator(id=23)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=24 Op=DataSourceOperator; InOpIds=; OutOpIds=28;\n  DataSourceOperator(id=24)\n    DataSource: card2:Card\n*\nOpId=25 Op=DataSourceOperator; InOpIds=; OutOpIds=29;\n  DataSourceOperator(id=25)\n    DataSource: [_anon2:USED_AT]-&gt;\n*\nOpId=26 Op=DataSourceOperator; InOpIds=; OutOpIds=30;\n  DataSourceOperator(id=26)\n    DataSource: m:Merchant\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=_anon1 Type=SOURCE\n*\nOpId=17 Op=JoinOperator; InOpIds=12,13; OutOpIds=18;\n  JoinOperator(id=17)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=_anon1 Type=SOURCE\n*\nOpId=27 Op=JoinOperator; InOpIds=22,23; OutOpIds=28;\n  JoinOperator(id=27)\n    JoinType: INNER\n    Joins: JoinPair: Node=c2 RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon1 Type=SINK\n*\nOpId=18 Op=JoinOperator; InOpIds=17,14; OutOpIds=19;\n  JoinOperator(id=18)\n    JoinType: INNER\n    Joins: JoinPair: Node=card1 RelOrNode=_anon1 Type=SINK\n*\nOpId=28 Op=JoinOperator; InOpIds=27,24; OutOpIds=29;\n  JoinOperator(id=28)\n    JoinType: INNER\n    Joins: JoinPair: Node=card2 RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon2 Type=SINK\n*\nOpId=19 Op=JoinOperator; InOpIds=18,15; OutOpIds=20;\n  JoinOperator(id=19)\n    JoinType: INNER\n    Joins: JoinPair: Node=card1 RelOrNode=_anon2 Type=SOURCE\n*\nOpId=29 Op=JoinOperator; InOpIds=28,25; OutOpIds=30;\n  JoinOperator(id=29)\n    JoinType: INNER\n    Joins: JoinPair: Node=card2 RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=10;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=c2 RelOrNode=_anon2 Type=SOURCE\n*\nOpId=20 Op=JoinOperator; InOpIds=19,16; OutOpIds=21;\n  JoinOperator(id=20)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon2 Type=SINK\n*\nOpId=30 Op=JoinOperator; InOpIds=29,26; OutOpIds=31;\n  JoinOperator(id=30)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=SelectionOperator; InOpIds=9; OutOpIds=11;\n  SelectionOperator(id=10)\n    Filter: (c1.id LT c2.id)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=11 Op=AggregationBoundaryOperator; InOpIds=10; OutOpIds=21;\n  AggregationBoundaryOperator(id=11)\n    GroupBy: [c1, c2]\n    Aggregates: [shared_cards]\n    Having: (shared_cards GT 0)\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=21 Op=JoinOperator; InOpIds=11,20; OutOpIds=31;\n  JoinOperator(id=21)\n    JoinType: INNER\n    Joins: JoinPair: Node=c1 RelOrNode=agg_boundary_1 Type=NODE_ID\n*\n----------------------------------------------------------------------\nLevel 8:\n----------------------------------------------------------------------\nOpId=31 Op=JoinOperator; InOpIds=21,30; OutOpIds=32;\n  JoinOperator(id=31)\n    JoinType: INNER\n    Joins: JoinPair: Node=c2 RelOrNode=agg_boundary_1 Type=NODE_ID, JoinPair: Node=m RelOrNode=m Type=NODE_ID\n*\n----------------------------------------------------------------------\nLevel 9:\n----------------------------------------------------------------------\nOpId=32 Op=ProjectionOperator; InOpIds=31; OutOpIds=33;\n  ProjectionOperator(id=32)\n    Projections: c1=c1, c2=c2, shared_cards=shared_cards, shared_merchants=COUNT(DISTINCT m), similarity_score=((shared_cards MULTIPLY 1.0) DIVIDE (shared_cards PLUS shared_merchants))\n    Having: (similarity_score GT 0.3)\n*\n----------------------------------------------------------------------\nLevel 10:\n----------------------------------------------------------------------\nOpId=33 Op=ProjectionOperator; InOpIds=32; OutOpIds=;\n  ProjectionOperator(id=33)\n    Projections: id=c1.id, id=c2.id, shared_cards=shared_cards, shared_merchants=shared_merchants, similarity_score=similarity_score\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#9-find-velocity-abuse-patterns-with-high-frequency-transactions","title":"9. Find velocity abuse patterns with high-frequency transactions","text":"<p>Application: Fraud: Velocity abuse</p> Notes <p>Detects accounts with abnormally high transaction frequency. May indicate automated fraud, account compromise, or velocity attacks.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:HAS_TRANSACTION]-&gt;(t:Transaction)\nWHERE t.timestamp &gt; TIMESTAMP() - DURATION('PT1H')\nWITH a, COUNT(t) AS tx_per_hour, SUM(t.amount) AS total_amount\nWHERE tx_per_hour &gt; 20\nRETURN a.id, a.holder_name, tx_per_hour, total_amount\nORDER BY tx_per_hour DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_a_holder_name AS holder_name\n  ,tx_per_hour AS tx_per_hour\n  ,total_amount AS total_amount\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,COUNT(_gsql2rsql_t_id) AS tx_per_hour\n    ,SUM(_gsql2rsql_t_amount) AS total_amount\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_a_id\n          ,holder_name AS _gsql2rsql_a_holder_name\n          ,risk_score AS _gsql2rsql_a_risk_score\n          ,status AS _gsql2rsql_a_status\n          ,default_date AS _gsql2rsql_a_default_date\n          ,home_country AS _gsql2rsql_a_home_country\n          ,kyc_status AS _gsql2rsql_a_kyc_status\n          ,days_since_creation AS _gsql2rsql_a_days_since_creation\n        FROM\n          catalog.fraud.Account\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon1_account_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.AccountTx\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.fraud.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 1 HOUR))\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status\n  HAVING (tx_per_hour) &gt; (20)\n) AS _proj\nORDER BY tx_per_hour DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: a=a, tx_per_hour=COUNT(t), total_amount=SUM(t.amount)\n    Filter: (t.timestamp GT (DATETIME() MINUS DURATION('PT1H')))\n    Having: (tx_per_hour GT 20)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=a.id, holder_name=a.holder_name, tx_per_hour=tx_per_hour, total_amount=total_amount\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#10-identify-return-fraud-patterns-with-high-return-rates","title":"10. Identify return fraud patterns with high return rates","text":"<p>Application: Fraud: Return fraud</p> Notes <p>Finds customers with suspiciously high return rates. May indicate wardrobing, receipt fraud, or return fraud schemes.</p> OpenCypher Query Cypher<pre><code>MATCH (c:Customer)-[:MADE_PURCHASE]-&gt;(p:Purchase)-[:RETURNED]-&gt;(r:Return)\nWITH c, COUNT(p) AS total_purchases, COUNT(r) AS total_returns\nWHERE total_purchases &gt; 10\nWITH c, total_purchases, total_returns,\n     (total_returns * 1.0 / total_purchases) AS return_rate\nWHERE return_rate &gt; 0.5\nRETURN c.id, c.name, total_purchases, total_returns, return_rate\nORDER BY return_rate DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_c_id AS id\n  ,_gsql2rsql_c_name AS name\n  ,total_purchases AS total_purchases\n  ,total_returns AS total_returns\n  ,return_rate AS return_rate\nFROM (\n  SELECT *\n  FROM (\n  SELECT \n     _gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,total_purchases AS total_purchases\n    ,total_returns AS total_returns\n    ,((total_returns) * (1.0)) / (total_purchases) AS return_rate\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n    ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n  FROM (\n    SELECT \n       _gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,COUNT(_gsql2rsql_p_id) AS total_purchases\n      ,COUNT(_gsql2rsql_r_id) AS total_returns\n      ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n      ,_gsql2rsql_c_status AS _gsql2rsql_c_status\n    FROM (\n      SELECT\n         _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n        ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n        ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_purchase_id AS _gsql2rsql__anon1_purchase_id\n        ,_left._gsql2rsql_p_id AS _gsql2rsql_p_id\n        ,_left._gsql2rsql__anon2_purchase_id AS _gsql2rsql__anon2_purchase_id\n        ,_left._gsql2rsql__anon2_return_id AS _gsql2rsql__anon2_return_id\n        ,_right._gsql2rsql_r_id AS _gsql2rsql_r_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n          ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n          ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n          ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_left._gsql2rsql__anon1_purchase_id AS _gsql2rsql__anon1_purchase_id\n          ,_left._gsql2rsql_p_id AS _gsql2rsql_p_id\n          ,_right._gsql2rsql__anon2_purchase_id AS _gsql2rsql__anon2_purchase_id\n          ,_right._gsql2rsql__anon2_return_id AS _gsql2rsql__anon2_return_id\n        FROM (\n          SELECT\n             _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n            ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n            ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n            ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n            ,_left._gsql2rsql__anon1_purchase_id AS _gsql2rsql__anon1_purchase_id\n            ,_right._gsql2rsql_p_id AS _gsql2rsql_p_id\n          FROM (\n            SELECT\n               _left._gsql2rsql_c_id AS _gsql2rsql_c_id\n              ,_left._gsql2rsql_c_name AS _gsql2rsql_c_name\n              ,_left._gsql2rsql_c_status AS _gsql2rsql_c_status\n              ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n              ,_right._gsql2rsql__anon1_purchase_id AS _gsql2rsql__anon1_purchase_id\n            FROM (\n              SELECT\n                 id AS _gsql2rsql_c_id\n                ,name AS _gsql2rsql_c_name\n                ,status AS _gsql2rsql_c_status\n              FROM\n                catalog.fraud.Customer\n            ) AS _left\n            INNER JOIN (\n              SELECT\n                 customer_id AS _gsql2rsql__anon1_customer_id\n                ,purchase_id AS _gsql2rsql__anon1_purchase_id\n              FROM\n                catalog.fraud.CustomerPurchase\n            ) AS _right ON\n              _left._gsql2rsql_c_id = _right._gsql2rsql__anon1_customer_id\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               id AS _gsql2rsql_p_id\n            FROM\n              catalog.fraud.Purchase\n          ) AS _right ON\n            _right._gsql2rsql_p_id = _left._gsql2rsql__anon1_purchase_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             purchase_id AS _gsql2rsql__anon2_purchase_id\n            ,return_id AS _gsql2rsql__anon2_return_id\n          FROM\n            catalog.fraud.PurchaseReturn\n        ) AS _right ON\n          _left._gsql2rsql_p_id = _right._gsql2rsql__anon2_purchase_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_r_id\n        FROM\n          catalog.fraud.Return\n      ) AS _right ON\n        _right._gsql2rsql_r_id = _left._gsql2rsql__anon2_return_id\n    ) AS _proj\n    GROUP BY _gsql2rsql_c_id, _gsql2rsql_c_name, _gsql2rsql_c_status\n    HAVING (total_purchases) &gt; (10)\n  ) AS _proj\n  ) AS _filter\n  WHERE (return_rate) &gt; (0.5)\n) AS _proj\nORDER BY return_rate DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: c:Customer\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:MADE_PURCHASE]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: p:Purchase\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:RETURNED]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: r:Return\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=p RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=10;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=r RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=10 Op=ProjectionOperator; InOpIds=9; OutOpIds=11;\n  ProjectionOperator(id=10)\n    Projections: c=c, total_purchases=COUNT(p), total_returns=COUNT(r)\n    Having: (total_purchases GT 10)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=10; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: c=c, total_purchases=total_purchases, total_returns=total_returns, return_rate=((total_returns MULTIPLY 1.0) DIVIDE total_purchases)\n    Having: (return_rate GT 0.5)\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=c.id, name=c.name, total_purchases=total_purchases, total_returns=total_returns, return_rate=return_rate\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#11-detect-bust-out-fraud-with-sudden-spending-spikes-before-default","title":"11. Detect bust-out fraud with sudden spending spikes before default","text":"<p>Application: Fraud: Bust-out fraud</p> Notes <p>Identifies accounts with sudden spending increases before defaulting. Classic bust-out fraud pattern where fraudsters max out credit before disappearing.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:HAS_TRANSACTION]-&gt;(t:Transaction)\nWHERE a.status = 'defaulted' AND t.timestamp &gt; a.default_date - DURATION('P30D')\nWITH a,\n     SUM(CASE WHEN t.timestamp &gt; a.default_date - DURATION('P7D') THEN t.amount ELSE 0 END) AS last_week,\n     SUM(CASE WHEN t.timestamp &lt;= a.default_date - DURATION('P7D') THEN t.amount ELSE 0 END) AS prior_weeks\nWHERE prior_weeks &gt; 0 AND (last_week / prior_weeks) &gt; 5.0\nRETURN a.id, last_week, prior_weeks, (last_week / prior_weeks) AS spike_ratio\nORDER BY spike_ratio DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,last_week AS last_week\n  ,prior_weeks AS prior_weeks\n  ,(last_week) / (prior_weeks) AS spike_ratio\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,SUM(CASE WHEN (_gsql2rsql_t_timestamp) &gt; ((_gsql2rsql_a_default_date) - (INTERVAL 7 DAY)) THEN _gsql2rsql_t_amount ELSE 0 END) AS last_week\n    ,SUM(CASE WHEN (_gsql2rsql_t_timestamp) &lt;= ((_gsql2rsql_a_default_date) - (INTERVAL 7 DAY)) THEN _gsql2rsql_t_amount ELSE 0 END) AS prior_weeks\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_a_id\n          ,holder_name AS _gsql2rsql_a_holder_name\n          ,risk_score AS _gsql2rsql_a_risk_score\n          ,status AS _gsql2rsql_a_status\n          ,default_date AS _gsql2rsql_a_default_date\n          ,home_country AS _gsql2rsql_a_home_country\n          ,kyc_status AS _gsql2rsql_a_kyc_status\n          ,days_since_creation AS _gsql2rsql_a_days_since_creation\n        FROM\n          catalog.fraud.Account\n        WHERE ((status) = ('defaulted'))\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon1_account_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.AccountTx\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.fraud.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_t_timestamp) &gt; ((_gsql2rsql_a_default_date) - (INTERVAL 30 DAY))\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status\n  HAVING ((prior_weeks) &gt; (0)) AND (((last_week) / (prior_weeks)) &gt; (5.0))\n) AS _proj\nORDER BY spike_ratio DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n    Filter: (a.status EQ 'defaulted')\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: a=a, last_week=SUM(CASE WHEN (t.timestamp GT (a.default_date MINUS DURATION('P7D'))) THEN t.amount ELSE 0 END), prior_weeks=SUM(CASE WHEN (t.timestamp LEQ (a.default_date MINUS DURATION('P7D'))) THEN t.amount ELSE 0 END)\n    Filter: (t.timestamp GT (a.default_date MINUS DURATION('P30D')))\n    Having: ((prior_weeks GT 0) AND ((last_week DIVIDE prior_weeks) GT 5.0))\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=a.id, last_week=last_week, prior_weeks=prior_weeks, spike_ratio=(last_week DIVIDE prior_weeks)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#12-find-circular-payment-patterns-indicating-money-laundering","title":"12. Find circular payment patterns indicating money laundering","text":"<p>Application: Fraud: Circular payment detection</p> Notes <p>Detects circular money flows where funds return to origin account. Strong indicator of structuring or layering in money laundering.</p> OpenCypher Query Cypher<pre><code>MATCH path = (a:Account)-[:TRANSFER*4..8]-&gt;(a)\nWHERE ALL(rel IN relationships(path) WHERE rel.amount &gt; 500)\n  AND LENGTH(path) &gt;= 4\nWITH path, REDUCE(total = 0, rel IN relationships(path) | total + rel.amount) AS cycle_amount\nRETURN [node IN nodes(path) | node.id] AS cycle_accounts,\n       LENGTH(path) AS cycle_length,\n       cycle_amount\nORDER BY cycle_amount DESC\nLIMIT 10\n</code></pre> Generated SQL SQL<pre><code>WITH RECURSIVE\n  paths_1 AS (\n    -- Base case: direct edges (depth = 1)\n    SELECT\n      e.source_account_id AS start_node,\n      e.target_account_id AS end_node,\n      1 AS depth,\n      ARRAY(e.source_account_id, e.target_account_id) AS path,\n      ARRAY(NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      ARRAY(e.source_account_id) AS visited\n    FROM catalog.fraud.Transfer e\n    WHERE (e.amount) &gt; (500)\n\n    UNION ALL\n\n    -- Recursive case: extend paths\n    SELECT\n      p.start_node,\n      e.target_account_id AS end_node,\n      p.depth + 1 AS depth,\n      CONCAT(p.path, ARRAY(e.target_account_id)) AS path,\n      ARRAY_APPEND(p.path_edges, NAMED_STRUCT('source_account_id', e.source_account_id, 'target_account_id', e.target_account_id, 'amount', e.amount, 'timestamp', e.timestamp)) AS path_edges,\n      CONCAT(p.visited, ARRAY(e.source_account_id)) AS visited\n    FROM paths_1 p\n    JOIN catalog.fraud.Transfer e\n      ON p.end_node = e.source_account_id\n    WHERE p.depth &lt; 8\n      AND NOT ARRAY_CONTAINS(p.visited, e.target_account_id)\n      AND (e.amount) &gt; (500)\n  )\nSELECT \n   _gsql2rsql_path_id AS cycle_accounts\n  ,(SIZE(_gsql2rsql_path_id) - 1) AS cycle_length\n  ,cycle_amount AS cycle_amount\nFROM (\n  SELECT \n     _gsql2rsql_path_id AS _gsql2rsql_path_id\n    ,AGGREGATE(_gsql2rsql_path_edges, CAST(0 AS DOUBLE), (total, rel) -&gt; (total) + (rel.amount)) AS cycle_amount\n  FROM (\n    SELECT\n       sink.id AS _gsql2rsql_a_id\n      ,sink.holder_name AS _gsql2rsql_a_holder_name\n      ,sink.risk_score AS _gsql2rsql_a_risk_score\n      ,sink.status AS _gsql2rsql_a_status\n      ,sink.default_date AS _gsql2rsql_a_default_date\n      ,sink.home_country AS _gsql2rsql_a_home_country\n      ,sink.kyc_status AS _gsql2rsql_a_kyc_status\n      ,sink.days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,p.start_node\n      ,p.end_node\n      ,p.depth\n      ,p.path AS _gsql2rsql_path_id\n      ,p.path_edges AS _gsql2rsql_path_edges\n    FROM paths_1 p\n    JOIN catalog.fraud.Account sink\n      ON sink.id = p.end_node\n    JOIN catalog.fraud.Account source\n      ON source.id = p.start_node\n    WHERE p.depth &gt;= 4 AND p.depth &lt;= 8 AND p.start_node = p.end_node\n  ) AS _proj\n  WHERE ((SIZE(_gsql2rsql_path_id) - 1)) &gt;= (4)\n) AS _proj\nORDER BY cycle_amount DESC\nLIMIT 10\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=2;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=3)\n    DataSource: a:Account\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=2 Op=RecursiveTraversalOperator; InOpIds=1; OutOpIds=4;\n  RecursiveTraversal(TRANSFER*4..8, path=path, circular=True)\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=2,3; OutOpIds=6;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=paths_r Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=4; OutOpIds=7;\n  ProjectionOperator(id=6)\n    Projections: path=path, cycle_amount=REDUCE(total = 0, rel IN RELATIONSHIPS(path) | (total PLUS rel.amount))\n    Filter: (LENGTH(path) GEQ 4)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=6; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: cycle_accounts=[node IN NODES(path) | node.id], cycle_length=LENGTH(path), cycle_amount=cycle_amount\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#13-identify-anomalous-cross-border-transaction-patterns","title":"13. Identify anomalous cross-border transaction patterns","text":"<p>Application: Fraud: Cross-border anomaly</p> Notes <p>Finds accounts with high-value cross-border transaction activity. May indicate trade-based money laundering or sanctions evasion.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:HAS_TRANSACTION]-&gt;(t:Transaction)-[:TO_COUNTRY]-&gt;(c:Country)\nWHERE c.code &lt;&gt; a.home_country AND t.amount &gt; 10000\nWITH a, c, COUNT(t) AS cross_border_count, SUM(t.amount) AS total_amount\nWHERE cross_border_count &gt; 5\nRETURN a.id, c.name AS destination_country, cross_border_count, total_amount\nORDER BY total_amount DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_c_name AS destination_country\n  ,cross_border_count AS cross_border_count\n  ,total_amount AS total_amount\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,_gsql2rsql_c_id AS _gsql2rsql_c_id\n    ,COUNT(_gsql2rsql_t_id) AS cross_border_count\n    ,SUM(_gsql2rsql_t_amount) AS total_amount\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n    ,_gsql2rsql_c_code AS _gsql2rsql_c_code\n    ,_gsql2rsql_c_name AS _gsql2rsql_c_name\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_left._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_left._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_left._gsql2rsql__anon2_country_id AS _gsql2rsql__anon2_country_id\n      ,_right._gsql2rsql_c_id AS _gsql2rsql_c_id\n      ,_right._gsql2rsql_c_code AS _gsql2rsql_c_code\n      ,_right._gsql2rsql_c_name AS _gsql2rsql_c_name\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n        ,_left._gsql2rsql_t_id AS _gsql2rsql_t_id\n        ,_left._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n        ,_right._gsql2rsql__anon2_country_id AS _gsql2rsql__anon2_country_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n          ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n          ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n          ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n          ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n          ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n          ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n          ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n          ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n        FROM (\n          SELECT\n             _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n            ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n            ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n            ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n            ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n            ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n            ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n            ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n            ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_a_id\n              ,holder_name AS _gsql2rsql_a_holder_name\n              ,risk_score AS _gsql2rsql_a_risk_score\n              ,status AS _gsql2rsql_a_status\n              ,default_date AS _gsql2rsql_a_default_date\n              ,home_country AS _gsql2rsql_a_home_country\n              ,kyc_status AS _gsql2rsql_a_kyc_status\n              ,days_since_creation AS _gsql2rsql_a_days_since_creation\n            FROM\n              catalog.fraud.Account\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               account_id AS _gsql2rsql__anon1_account_id\n              ,transaction_id AS _gsql2rsql__anon1_transaction_id\n            FROM\n              catalog.fraud.AccountTx\n          ) AS _right ON\n            _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_t_id\n            ,amount AS _gsql2rsql_t_amount\n          FROM\n            catalog.fraud.Transaction\n          WHERE ((amount) &gt; (10000))\n        ) AS _right ON\n          _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           transaction_id AS _gsql2rsql__anon2_transaction_id\n          ,country_id AS _gsql2rsql__anon2_country_id\n        FROM\n          catalog.fraud.TransactionCountry\n      ) AS _right ON\n        _left._gsql2rsql_t_id = _right._gsql2rsql__anon2_transaction_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_c_id\n        ,code AS _gsql2rsql_c_code\n        ,name AS _gsql2rsql_c_name\n      FROM\n        catalog.fraud.Country\n    ) AS _right ON\n      _right._gsql2rsql_c_id = _left._gsql2rsql__anon2_country_id\n  ) AS _proj\n  WHERE (_gsql2rsql_c_code) != (_gsql2rsql_a_home_country)\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_c_id, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status, _gsql2rsql_c_code, _gsql2rsql_c_name\n  HAVING (cross_border_count) &gt; (5)\n) AS _proj\nORDER BY total_amount DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n    Filter: (t.amount GT 10000)\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:TO_COUNTRY]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: c:Country\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=c RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: a=a, c=c, cross_border_count=COUNT(t), total_amount=SUM(t.amount)\n    Filter: (c.code NEQ a.home_country)\n    Having: (cross_border_count GT 5)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=a.id, destination_country=c.name, cross_border_count=cross_border_count, total_amount=total_amount\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#14-detect-account-takeover-via-sudden-behavioral-changes","title":"14. Detect account takeover via sudden behavioral changes","text":"<p>Application: Fraud: Account takeover</p> Notes <p>Identifies accounts with dramatic changes in transaction patterns. Sudden increases may indicate account takeover by fraudsters.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:HAS_TRANSACTION]-&gt;(t:Transaction)\nWITH a,\n     AVG(CASE WHEN t.timestamp &lt; TIMESTAMP() - DURATION('P30D') THEN t.amount END) AS avg_30d_ago,\n     AVG(CASE WHEN t.timestamp &gt;= TIMESTAMP() - DURATION('P7D') THEN t.amount END) AS avg_recent\nWHERE avg_30d_ago IS NOT NULL AND avg_recent &gt; avg_30d_ago * 3\nRETURN a.id, avg_30d_ago, avg_recent, (avg_recent / avg_30d_ago) AS behavior_change_ratio\nORDER BY behavior_change_ratio DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,avg_30d_ago AS avg_30d_ago\n  ,avg_recent AS avg_recent\n  ,(avg_recent) / (avg_30d_ago) AS behavior_change_ratio\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,AVG(CAST(CASE WHEN (_gsql2rsql_t_timestamp) &lt; ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY)) THEN _gsql2rsql_t_amount END AS DOUBLE)) AS avg_30d_ago\n    ,AVG(CAST(CASE WHEN (_gsql2rsql_t_timestamp) &gt;= ((CURRENT_TIMESTAMP()) - (INTERVAL 7 DAY)) THEN _gsql2rsql_t_amount END AS DOUBLE)) AS avg_recent\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_t_timestamp AS _gsql2rsql_t_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_a_id\n          ,holder_name AS _gsql2rsql_a_holder_name\n          ,risk_score AS _gsql2rsql_a_risk_score\n          ,status AS _gsql2rsql_a_status\n          ,default_date AS _gsql2rsql_a_default_date\n          ,home_country AS _gsql2rsql_a_home_country\n          ,kyc_status AS _gsql2rsql_a_kyc_status\n          ,days_since_creation AS _gsql2rsql_a_days_since_creation\n        FROM\n          catalog.fraud.Account\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon1_account_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.AccountTx\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_t_id\n        ,amount AS _gsql2rsql_t_amount\n        ,timestamp AS _gsql2rsql_t_timestamp\n      FROM\n        catalog.fraud.Transaction\n    ) AS _right ON\n      _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status\n  HAVING ((avg_30d_ago) IS NOT NULL) AND ((avg_recent) &gt; ((avg_30d_ago) * (3)))\n) AS _proj\nORDER BY behavior_change_ratio DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=6;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=6 Op=ProjectionOperator; InOpIds=5; OutOpIds=7;\n  ProjectionOperator(id=6)\n    Projections: a=a, avg_30d_ago=AVG(CASE WHEN (t.timestamp LT (DATETIME() MINUS DURATION('P30D'))) THEN t.amount END), avg_recent=AVG(CASE WHEN (t.timestamp GEQ (DATETIME() MINUS DURATION('P7D'))) THEN t.amount END)\n    Having: (IS_NOT_NULL(avg_30d_ago) AND (avg_recent GT (avg_30d_ago MULTIPLY 3)))\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=6; OutOpIds=;\n  ProjectionOperator(id=7)\n    Projections: id=a.id, avg_30d_ago=avg_30d_ago, avg_recent=avg_recent, behavior_change_ratio=(avg_recent DIVIDE avg_30d_ago)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#15-find-smurfing-patterns-with-structured-deposits-below-reporting-thresholds","title":"15. Find smurfing patterns with structured deposits below reporting thresholds","text":"<p>Application: Fraud: Structuring/Smurfing</p> Notes <p>Detects multiple deposits just under regulatory reporting thresholds. Classic smurfing/structuring pattern to avoid currency transaction reports.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:DEPOSIT]-&gt;(d:Transaction)\nWHERE d.amount &gt; 9000 AND d.amount &lt; 10000\n  AND d.timestamp &gt; TIMESTAMP() - DURATION('P30D')\nWITH a, COUNT(d) AS deposit_count, SUM(d.amount) AS total_deposits\nWHERE deposit_count &gt; 5\nRETURN a.id, deposit_count, total_deposits, (total_deposits / deposit_count) AS avg_deposit\nORDER BY deposit_count DESC\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,deposit_count AS deposit_count\n  ,total_deposits AS total_deposits\n  ,(total_deposits) / (deposit_count) AS avg_deposit\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,COUNT(_gsql2rsql_d_id) AS deposit_count\n    ,SUM(_gsql2rsql_d_amount) AS total_deposits\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_d_id AS _gsql2rsql_d_id\n      ,_right._gsql2rsql_d_amount AS _gsql2rsql_d_amount\n      ,_right._gsql2rsql_d_timestamp AS _gsql2rsql_d_timestamp\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      FROM (\n        SELECT\n           id AS _gsql2rsql_a_id\n          ,holder_name AS _gsql2rsql_a_holder_name\n          ,risk_score AS _gsql2rsql_a_risk_score\n          ,status AS _gsql2rsql_a_status\n          ,default_date AS _gsql2rsql_a_default_date\n          ,home_country AS _gsql2rsql_a_home_country\n          ,kyc_status AS _gsql2rsql_a_kyc_status\n          ,days_since_creation AS _gsql2rsql_a_days_since_creation\n        FROM\n          catalog.fraud.Account\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           account_id AS _gsql2rsql__anon1_account_id\n          ,transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM\n          catalog.fraud.AccountDeposit\n      ) AS _right ON\n        _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_d_id\n        ,amount AS _gsql2rsql_d_amount\n        ,timestamp AS _gsql2rsql_d_timestamp\n      FROM\n        catalog.fraud.Transaction\n      WHERE (((amount) &gt; (9000)) AND ((amount) &lt; (10000)))\n    ) AS _right ON\n      _right._gsql2rsql_d_id = _left._gsql2rsql__anon1_transaction_id\n  ) AS _proj\n  WHERE (_gsql2rsql_d_timestamp) &gt; ((CURRENT_TIMESTAMP()) - (INTERVAL 30 DAY))\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status\n  HAVING (deposit_count) &gt; (5)\n) AS _proj\nORDER BY deposit_count DESC\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=4;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:DEPOSIT]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=5;\n  DataSourceOperator(id=3)\n    DataSource: d:Transaction\n    Filter: ((d.amount GT 9000) AND (d.amount LT 10000))\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=4 Op=JoinOperator; InOpIds=1,2; OutOpIds=5;\n  JoinOperator(id=4)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=5 Op=JoinOperator; InOpIds=4,3; OutOpIds=7;\n  JoinOperator(id=5)\n    JoinType: INNER\n    Joins: JoinPair: Node=d RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=7 Op=ProjectionOperator; InOpIds=5; OutOpIds=8;\n  ProjectionOperator(id=7)\n    Projections: a=a, deposit_count=COUNT(d), total_deposits=SUM(d.amount)\n    Filter: (d.timestamp GT (DATETIME() MINUS DURATION('P30D')))\n    Having: (deposit_count GT 5)\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=8 Op=ProjectionOperator; InOpIds=7; OutOpIds=;\n  ProjectionOperator(id=8)\n    Projections: id=a.id, deposit_count=deposit_count, total_deposits=total_deposits, avg_deposit=(total_deposits DIVIDE deposit_count)\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#16-analyze-transaction-volumes-by-merchant-category-for-suspicious-accounts","title":"16. Analyze transaction volumes by merchant category for suspicious accounts","text":"<p>Application: Fraud: Category-based volume analysis</p> Notes <p>Filters accounts by metadata (KYC status, account age) to focus on risky profiles. Aggregates transaction volumes by merchant category to identify unusual spending patterns. New accounts with high volumes in specific categories (e.g., electronics, gift cards) are red flags. Combines vertex filtering with edge aggregation for comprehensive risk assessment.</p> OpenCypher Query Cypher<pre><code>MATCH (a:Account)-[:HAS_TRANSACTION]-&gt;(t:Transaction)-[:AT_MERCHANT]-&gt;(m:Merchant)\nWHERE a.kyc_status = 'incomplete' OR a.days_since_creation &lt; 30\nWITH a,\n     m.category AS merchant_category,\n     COUNT(t) AS transaction_count,\n     SUM(t.amount) AS total_volume,\n     AVG(t.amount) AS avg_transaction\nWHERE transaction_count &gt; 10\nRETURN a.id,\n       a.kyc_status,\n       a.days_since_creation,\n       merchant_category,\n       transaction_count,\n       total_volume,\n       avg_transaction\nORDER BY total_volume DESC\nLIMIT 100\n</code></pre> Generated SQL SQL<pre><code>SELECT \n   _gsql2rsql_a_id AS id\n  ,_gsql2rsql_a_kyc_status AS kyc_status\n  ,_gsql2rsql_a_days_since_creation AS days_since_creation\n  ,merchant_category AS merchant_category\n  ,transaction_count AS transaction_count\n  ,total_volume AS total_volume\n  ,avg_transaction AS avg_transaction\nFROM (\n  SELECT \n     _gsql2rsql_a_id AS _gsql2rsql_a_id\n    ,_gsql2rsql_m_category AS merchant_category\n    ,COUNT(_gsql2rsql_t_id) AS transaction_count\n    ,SUM(_gsql2rsql_t_amount) AS total_volume\n    ,AVG(CAST(_gsql2rsql_t_amount AS DOUBLE)) AS avg_transaction\n    ,_gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n    ,_gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n    ,_gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n    ,_gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n    ,_gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n    ,_gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n    ,_gsql2rsql_a_status AS _gsql2rsql_a_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n      ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n      ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n      ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n      ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n      ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n      ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n      ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n      ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n      ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_left._gsql2rsql_t_id AS _gsql2rsql_t_id\n      ,_left._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_left._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n      ,_left._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n      ,_right._gsql2rsql_m_id AS _gsql2rsql_m_id\n      ,_right._gsql2rsql_m_category AS _gsql2rsql_m_category\n    FROM (\n      SELECT\n         _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n        ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n        ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n        ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n        ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n        ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n        ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n        ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n        ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n        ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n        ,_left._gsql2rsql_t_id AS _gsql2rsql_t_id\n        ,_left._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n        ,_right._gsql2rsql__anon2_transaction_id AS _gsql2rsql__anon2_transaction_id\n        ,_right._gsql2rsql__anon2_merchant_id AS _gsql2rsql__anon2_merchant_id\n      FROM (\n        SELECT\n           _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n          ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n          ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n          ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n          ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n          ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n          ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n          ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n          ,_left._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n          ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n          ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n          ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n        FROM (\n          SELECT\n             _left._gsql2rsql_a_id AS _gsql2rsql_a_id\n            ,_left._gsql2rsql_a_holder_name AS _gsql2rsql_a_holder_name\n            ,_left._gsql2rsql_a_risk_score AS _gsql2rsql_a_risk_score\n            ,_left._gsql2rsql_a_status AS _gsql2rsql_a_status\n            ,_left._gsql2rsql_a_default_date AS _gsql2rsql_a_default_date\n            ,_left._gsql2rsql_a_home_country AS _gsql2rsql_a_home_country\n            ,_left._gsql2rsql_a_kyc_status AS _gsql2rsql_a_kyc_status\n            ,_left._gsql2rsql_a_days_since_creation AS _gsql2rsql_a_days_since_creation\n            ,_right._gsql2rsql__anon1_account_id AS _gsql2rsql__anon1_account_id\n            ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n          FROM (\n            SELECT\n               id AS _gsql2rsql_a_id\n              ,holder_name AS _gsql2rsql_a_holder_name\n              ,risk_score AS _gsql2rsql_a_risk_score\n              ,status AS _gsql2rsql_a_status\n              ,default_date AS _gsql2rsql_a_default_date\n              ,home_country AS _gsql2rsql_a_home_country\n              ,kyc_status AS _gsql2rsql_a_kyc_status\n              ,days_since_creation AS _gsql2rsql_a_days_since_creation\n            FROM\n              catalog.fraud.Account\n            WHERE (((kyc_status) = ('incomplete')) OR ((days_since_creation) &lt; (30)))\n          ) AS _left\n          INNER JOIN (\n            SELECT\n               account_id AS _gsql2rsql__anon1_account_id\n              ,transaction_id AS _gsql2rsql__anon1_transaction_id\n            FROM\n              catalog.fraud.AccountTx\n          ) AS _right ON\n            _left._gsql2rsql_a_id = _right._gsql2rsql__anon1_account_id\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             id AS _gsql2rsql_t_id\n            ,amount AS _gsql2rsql_t_amount\n          FROM\n            catalog.fraud.Transaction\n        ) AS _right ON\n          _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           transaction_id AS _gsql2rsql__anon2_transaction_id\n          ,merchant_id AS _gsql2rsql__anon2_merchant_id\n        FROM\n          catalog.fraud.TransactionMerchant\n      ) AS _right ON\n        _left._gsql2rsql_t_id = _right._gsql2rsql__anon2_transaction_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         id AS _gsql2rsql_m_id\n        ,category AS _gsql2rsql_m_category\n      FROM\n        catalog.fraud.Merchant\n    ) AS _right ON\n      _right._gsql2rsql_m_id = _left._gsql2rsql__anon2_merchant_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_a_id, _gsql2rsql_m_category, _gsql2rsql_a_days_since_creation, _gsql2rsql_a_default_date, _gsql2rsql_a_holder_name, _gsql2rsql_a_home_country, _gsql2rsql_a_kyc_status, _gsql2rsql_a_risk_score, _gsql2rsql_a_status\n  HAVING (transaction_count) &gt; (10)\n) AS _proj\nORDER BY total_volume DESC\nLIMIT 100\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: a:Account\n    Filter: ((a.kyc_status EQ 'incomplete') OR (a.days_since_creation LT 30))\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_TRANSACTION]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: t:Transaction\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:AT_MERCHANT]-&gt;\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: m:Merchant\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=a RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=m RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=ProjectionOperator; InOpIds=9; OutOpIds=12;\n  ProjectionOperator(id=11)\n    Projections: a=a, merchant_category=m.category, transaction_count=COUNT(t), total_volume=SUM(t.amount), avg_transaction=AVG(t.amount)\n    Having: (transaction_count GT 10)\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=12 Op=ProjectionOperator; InOpIds=11; OutOpIds=;\n  ProjectionOperator(id=12)\n    Projections: id=a.id, kyc_status=a.kyc_status, days_since_creation=a.days_since_creation, merchant_category=merchant_category, transaction_count=transaction_count, total_volume=total_volume, avg_transaction=avg_transaction\n*\n----------------------------------------------------------------------\n</code></pre>"},{"location":"examples/fraud/#17-detect-shared-card-usage-across-blacklisted-and-verified-customers","title":"17. Detect shared card usage across blacklisted and verified customers","text":"<p>Application: Fraud: Card contamination tracking</p> Notes <p>Identifies cards shared between blacklisted and verified customers (contamination). Uses customer status metadata to filter and categorize relationships. Calculates total transaction metrics to assess card usage impact. Critical for identifying compromised cards or insider fraud networks.</p> OpenCypher Query Cypher<pre><code>MATCH (blacklisted:Customer)-[:HAS_CARD]-&gt;(card:Card)&lt;-[:HAS_CARD]-(verified:Customer)\nWHERE blacklisted.status = 'blacklisted' AND verified.status = 'verified'\nWITH card,\n     COLLECT(DISTINCT blacklisted.id) AS blacklisted_customers,\n     COLLECT(DISTINCT verified.id) AS verified_customers\nMATCH (card)-[:USED_IN]-&gt;(t:Transaction)\nWITH card,\n     blacklisted_customers,\n     verified_customers,\n     COUNT(t) AS total_transactions,\n     SUM(t.amount) AS total_amount\nRETURN card.number,\n       SIZE(blacklisted_customers) AS blacklisted_count,\n       SIZE(verified_customers) AS verified_count,\n       total_transactions,\n       total_amount,\n       blacklisted_customers,\n       verified_customers\nORDER BY total_amount DESC\nLIMIT 25\n</code></pre> Generated SQL SQL<pre><code>WITH\nagg_boundary_1 AS (\n  SELECT\n    _gsql2rsql_card_id AS `card`,\n    COLLECT_LIST(DISTINCT _gsql2rsql_blacklisted_id) AS `blacklisted_customers`,\n    COLLECT_LIST(DISTINCT _gsql2rsql_verified_id) AS `verified_customers`\n  FROM (\n  SELECT\n     _left._gsql2rsql_blacklisted_id AS _gsql2rsql_blacklisted_id\n    ,_left._gsql2rsql_blacklisted_status AS _gsql2rsql_blacklisted_status\n    ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n    ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n    ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n    ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n    ,_left._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n    ,_left._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n    ,_right._gsql2rsql_verified_id AS _gsql2rsql_verified_id\n    ,_right._gsql2rsql_verified_status AS _gsql2rsql_verified_status\n  FROM (\n    SELECT\n       _left._gsql2rsql_blacklisted_id AS _gsql2rsql_blacklisted_id\n      ,_left._gsql2rsql_blacklisted_status AS _gsql2rsql_blacklisted_status\n      ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n      ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_left._gsql2rsql_card_id AS _gsql2rsql_card_id\n      ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n      ,_right._gsql2rsql__anon2_customer_id AS _gsql2rsql__anon2_customer_id\n      ,_right._gsql2rsql__anon2_card_id AS _gsql2rsql__anon2_card_id\n    FROM (\n      SELECT\n         _left._gsql2rsql_blacklisted_id AS _gsql2rsql_blacklisted_id\n        ,_left._gsql2rsql_blacklisted_status AS _gsql2rsql_blacklisted_status\n        ,_left._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_right._gsql2rsql_card_id AS _gsql2rsql_card_id\n        ,_right._gsql2rsql_card_number AS _gsql2rsql_card_number\n      FROM (\n        SELECT\n           _left._gsql2rsql_blacklisted_id AS _gsql2rsql_blacklisted_id\n          ,_left._gsql2rsql_blacklisted_status AS _gsql2rsql_blacklisted_status\n          ,_right._gsql2rsql__anon1_customer_id AS _gsql2rsql__anon1_customer_id\n          ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_blacklisted_id\n            ,status AS _gsql2rsql_blacklisted_status\n          FROM\n            catalog.fraud.Customer\n          WHERE ((status) = ('blacklisted'))\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             customer_id AS _gsql2rsql__anon1_customer_id\n            ,card_id AS _gsql2rsql__anon1_card_id\n          FROM\n            catalog.fraud.CustomerCard\n        ) AS _right ON\n          _left._gsql2rsql_blacklisted_id = _right._gsql2rsql__anon1_customer_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_card_id\n          ,number AS _gsql2rsql_card_number\n        FROM\n          catalog.fraud.Card\n      ) AS _right ON\n        _right._gsql2rsql_card_id = _left._gsql2rsql__anon1_card_id\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         customer_id AS _gsql2rsql__anon2_customer_id\n        ,card_id AS _gsql2rsql__anon2_card_id\n      FROM\n        catalog.fraud.CustomerCard\n    ) AS _right ON\n      _left._gsql2rsql_card_id = _right._gsql2rsql__anon2_card_id\n  ) AS _left\n  INNER JOIN (\n    SELECT\n       id AS _gsql2rsql_verified_id\n      ,status AS _gsql2rsql_verified_status\n    FROM\n      catalog.fraud.Customer\n    WHERE ((status) = ('verified'))\n  ) AS _right ON\n    _right._gsql2rsql_verified_id = _left._gsql2rsql__anon2_customer_id\n  ) AS _agg_input\n  GROUP BY _gsql2rsql_card_id\n)\nSELECT \n   _gsql2rsql_card_number AS number\n  ,SIZE(blacklisted_customers) AS blacklisted_count\n  ,SIZE(verified_customers) AS verified_count\n  ,total_transactions AS total_transactions\n  ,total_amount AS total_amount\n  ,blacklisted_customers AS blacklisted_customers\n  ,verified_customers AS verified_customers\nFROM (\n  SELECT \n     _gsql2rsql_card_id AS _gsql2rsql_card_id\n    ,blacklisted_customers AS blacklisted_customers\n    ,verified_customers AS verified_customers\n    ,COUNT(_gsql2rsql_t_id) AS total_transactions\n    ,SUM(_gsql2rsql_t_amount) AS total_amount\n    ,_gsql2rsql_card_number AS _gsql2rsql_card_number\n  FROM (\n    SELECT\n       _left.`card` AS `card`\n      ,_left.`blacklisted_customers` AS `blacklisted_customers`\n      ,_left.`verified_customers` AS `verified_customers`\n      ,_right._gsql2rsql_card_id AS _gsql2rsql_card_id\n      ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      ,_right._gsql2rsql_card_number AS _gsql2rsql_card_number\n      ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n      ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n      ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n    FROM (\n      SELECT\n         `card`\n        ,`blacklisted_customers`\n        ,`verified_customers`\n      FROM agg_boundary_1\n    ) AS _left\n    INNER JOIN (\n      SELECT\n         _left._gsql2rsql_card_id AS _gsql2rsql_card_id\n        ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n        ,_left._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n        ,_left._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n        ,_right._gsql2rsql_t_id AS _gsql2rsql_t_id\n        ,_right._gsql2rsql_t_amount AS _gsql2rsql_t_amount\n      FROM (\n        SELECT\n           _left._gsql2rsql_card_id AS _gsql2rsql_card_id\n          ,_left._gsql2rsql_card_number AS _gsql2rsql_card_number\n          ,_right._gsql2rsql__anon1_card_id AS _gsql2rsql__anon1_card_id\n          ,_right._gsql2rsql__anon1_transaction_id AS _gsql2rsql__anon1_transaction_id\n        FROM (\n          SELECT\n             id AS _gsql2rsql_card_id\n            ,number AS _gsql2rsql_card_number\n          FROM\n            catalog.fraud.Card\n        ) AS _left\n        INNER JOIN (\n          SELECT\n             card_id AS _gsql2rsql__anon1_card_id\n            ,transaction_id AS _gsql2rsql__anon1_transaction_id\n          FROM\n            catalog.fraud.CardTransaction\n        ) AS _right ON\n          _left._gsql2rsql_card_id = _right._gsql2rsql__anon1_card_id\n      ) AS _left\n      INNER JOIN (\n        SELECT\n           id AS _gsql2rsql_t_id\n          ,amount AS _gsql2rsql_t_amount\n        FROM\n          catalog.fraud.Transaction\n      ) AS _right ON\n        _right._gsql2rsql_t_id = _left._gsql2rsql__anon1_transaction_id\n    ) AS _right ON\n      _left.`card` = _right._gsql2rsql_card_id\n  ) AS _proj\n  GROUP BY _gsql2rsql_card_id, blacklisted_customers, verified_customers, _gsql2rsql_card_number\n) AS _proj\nORDER BY total_amount DESC\nLIMIT 25\n</code></pre> Logical Plan Text Only<pre><code>Level 0:\n----------------------------------------------------------------------\nOpId=1 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=1)\n    DataSource: blacklisted:Customer\n    Filter: (blacklisted.status EQ 'blacklisted')\n*\nOpId=2 Op=DataSourceOperator; InOpIds=; OutOpIds=6;\n  DataSourceOperator(id=2)\n    DataSource: [_anon1:HAS_CARD]-&gt;\n*\nOpId=3 Op=DataSourceOperator; InOpIds=; OutOpIds=7;\n  DataSourceOperator(id=3)\n    DataSource: card:Card\n*\nOpId=4 Op=DataSourceOperator; InOpIds=; OutOpIds=8;\n  DataSourceOperator(id=4)\n    DataSource: [_anon2:HAS_CARD]&lt;-\n*\nOpId=5 Op=DataSourceOperator; InOpIds=; OutOpIds=9;\n  DataSourceOperator(id=5)\n    DataSource: verified:Customer\n    Filter: (verified.status EQ 'verified')\n*\nOpId=12 Op=DataSourceOperator; InOpIds=; OutOpIds=15;\n  DataSourceOperator(id=12)\n    DataSource: card:Card\n*\nOpId=13 Op=DataSourceOperator; InOpIds=; OutOpIds=15;\n  DataSourceOperator(id=13)\n    DataSource: [_anon1:USED_IN]-&gt;\n*\nOpId=14 Op=DataSourceOperator; InOpIds=; OutOpIds=16;\n  DataSourceOperator(id=14)\n    DataSource: t:Transaction\n*\n----------------------------------------------------------------------\nLevel 1:\n----------------------------------------------------------------------\nOpId=6 Op=JoinOperator; InOpIds=1,2; OutOpIds=7;\n  JoinOperator(id=6)\n    JoinType: INNER\n    Joins: JoinPair: Node=blacklisted RelOrNode=_anon1 Type=SOURCE\n*\nOpId=15 Op=JoinOperator; InOpIds=12,13; OutOpIds=16;\n  JoinOperator(id=15)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon1 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 2:\n----------------------------------------------------------------------\nOpId=7 Op=JoinOperator; InOpIds=6,3; OutOpIds=8;\n  JoinOperator(id=7)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon1 Type=SINK\n*\nOpId=16 Op=JoinOperator; InOpIds=15,14; OutOpIds=17;\n  JoinOperator(id=16)\n    JoinType: INNER\n    Joins: JoinPair: Node=t RelOrNode=_anon1 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 3:\n----------------------------------------------------------------------\nOpId=8 Op=JoinOperator; InOpIds=7,4; OutOpIds=9;\n  JoinOperator(id=8)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=_anon2 Type=SINK\n*\n----------------------------------------------------------------------\nLevel 4:\n----------------------------------------------------------------------\nOpId=9 Op=JoinOperator; InOpIds=8,5; OutOpIds=11;\n  JoinOperator(id=9)\n    JoinType: INNER\n    Joins: JoinPair: Node=verified RelOrNode=_anon2 Type=SOURCE\n*\n----------------------------------------------------------------------\nLevel 5:\n----------------------------------------------------------------------\nOpId=11 Op=AggregationBoundaryOperator; InOpIds=9; OutOpIds=17;\n  AggregationBoundaryOperator(id=11)\n    GroupBy: [card]\n    Aggregates: [blacklisted_customers, verified_customers]\n*\n----------------------------------------------------------------------\nLevel 6:\n----------------------------------------------------------------------\nOpId=17 Op=JoinOperator; InOpIds=11,16; OutOpIds=18;\n  JoinOperator(id=17)\n    JoinType: INNER\n    Joins: JoinPair: Node=card RelOrNode=agg_boundary_1 Type=NODE_ID\n*\n----------------------------------------------------------------------\nLevel 7:\n----------------------------------------------------------------------\nOpId=18 Op=ProjectionOperator; InOpIds=17; OutOpIds=19;\n  ProjectionOperator(id=18)\n    Projections: card=card, blacklisted_customers=blacklisted_customers, verified_customers=verified_customers, total_transactions=COUNT(t), total_amount=SUM(t.amount)\n*\n----------------------------------------------------------------------\nLevel 8:\n----------------------------------------------------------------------\nOpId=19 Op=ProjectionOperator; InOpIds=18; OutOpIds=;\n  ProjectionOperator(id=19)\n    Projections: number=card.number, blacklisted_count=SIZE(blacklisted_customers), verified_count=SIZE(verified_customers), total_transactions=total_transactions, total_amount=total_amount, blacklisted_customers=blacklisted_customers, verified_customers=verified_customers\n*\n----------------------------------------------------------------------\n</code></pre>"}]}